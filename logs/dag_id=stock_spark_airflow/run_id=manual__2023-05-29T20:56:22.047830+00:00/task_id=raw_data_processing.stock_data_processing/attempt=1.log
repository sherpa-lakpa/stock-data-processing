[2023-05-29 20:56:24,223] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: stock_spark_airflow.raw_data_processing.stock_data_processing manual__2023-05-29T20:56:22.047830+00:00 [queued]>
[2023-05-29 20:56:24,235] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: stock_spark_airflow.raw_data_processing.stock_data_processing manual__2023-05-29T20:56:22.047830+00:00 [queued]>
[2023-05-29 20:56:24,239] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-05-29 20:56:24,240] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2023-05-29 20:56:24,240] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-05-29 20:56:24,255] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): raw_data_processing.stock_data_processing> on 2023-05-29 20:56:22.047830+00:00
[2023-05-29 20:56:24,262] {standard_task_runner.py:52} INFO - Started process 1536 to run task
[2023-05-29 20:56:24,267] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'stock_spark_***', 'raw_data_processing.stock_data_processing', 'manual__2023-05-29T20:56:22.047830+00:00', '--job-id', '385', '--raw', '--subdir', 'DAGS_FOLDER/stock_spark_ml.py', '--cfg-path', '/tmp/tmpndzqypw3', '--error-file', '/tmp/tmpoufdzhci']
[2023-05-29 20:56:24,270] {standard_task_runner.py:80} INFO - Job 385: Subtask raw_data_processing.stock_data_processing
[2023-05-29 20:56:24,350] {task_command.py:369} INFO - Running <TaskInstance: stock_spark_airflow.raw_data_processing.stock_data_processing manual__2023-05-29T20:56:22.047830+00:00 [running]> on host 5b0f4fa1b7e1
[2023-05-29 20:56:24,405] {taskinstance.py:1571} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=stock_spark_***
AIRFLOW_CTX_TASK_ID=raw_data_processing.stock_data_processing
AIRFLOW_CTX_EXECUTION_DATE=2023-05-29T20:56:22.047830+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=manual__2023-05-29T20:56:22.047830+00:00
[2023-05-29 20:56:24,414] {base.py:68} INFO - Using connection ID 'spark_local' for task execution.
[2023-05-29 20:56:24,415] {spark_submit.py:339} INFO - Spark-Submit cmd: spark-submit --master spark://spark:7077 --name arrow-spark /usr/local/spark/app/raw_data_processing.py stocks
[2023-05-29 20:56:24,491] {spark_submit.py:490} INFO - /home/***/.local/lib/python3.7/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2023-05-29 20:56:26,666] {spark_submit.py:490} INFO - 23/05/29 20:56:26 INFO SparkContext: Running Spark version 3.4.0
[2023-05-29 20:56:26,741] {spark_submit.py:490} INFO - 23/05/29 20:56:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-05-29 20:56:26,827] {spark_submit.py:490} INFO - 23/05/29 20:56:26 INFO ResourceUtils: ==============================================================
[2023-05-29 20:56:26,828] {spark_submit.py:490} INFO - 23/05/29 20:56:26 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-05-29 20:56:26,829] {spark_submit.py:490} INFO - 23/05/29 20:56:26 INFO ResourceUtils: ==============================================================
[2023-05-29 20:56:26,829] {spark_submit.py:490} INFO - 23/05/29 20:56:26 INFO SparkContext: Submitted application: raw_data_processing_stocks
[2023-05-29 20:56:26,860] {spark_submit.py:490} INFO - 23/05/29 20:56:26 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-05-29 20:56:26,871] {spark_submit.py:490} INFO - 23/05/29 20:56:26 INFO ResourceProfile: Limiting resource is cpu
[2023-05-29 20:56:26,873] {spark_submit.py:490} INFO - 23/05/29 20:56:26 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-05-29 20:56:26,935] {spark_submit.py:490} INFO - 23/05/29 20:56:26 INFO SecurityManager: Changing view acls to: default
[2023-05-29 20:56:26,936] {spark_submit.py:490} INFO - 23/05/29 20:56:26 INFO SecurityManager: Changing modify acls to: default
[2023-05-29 20:56:26,936] {spark_submit.py:490} INFO - 23/05/29 20:56:26 INFO SecurityManager: Changing view acls groups to:
[2023-05-29 20:56:26,938] {spark_submit.py:490} INFO - 23/05/29 20:56:26 INFO SecurityManager: Changing modify acls groups to:
[2023-05-29 20:56:26,939] {spark_submit.py:490} INFO - 23/05/29 20:56:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
[2023-05-29 20:56:27,244] {spark_submit.py:490} INFO - 23/05/29 20:56:27 INFO Utils: Successfully started service 'sparkDriver' on port 43589.
[2023-05-29 20:56:27,270] {spark_submit.py:490} INFO - 23/05/29 20:56:27 INFO SparkEnv: Registering MapOutputTracker
[2023-05-29 20:56:27,294] {spark_submit.py:490} INFO - 23/05/29 20:56:27 INFO SparkEnv: Registering BlockManagerMaster
[2023-05-29 20:56:27,308] {spark_submit.py:490} INFO - 23/05/29 20:56:27 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-05-29 20:56:27,309] {spark_submit.py:490} INFO - 23/05/29 20:56:27 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-05-29 20:56:27,311] {spark_submit.py:490} INFO - 23/05/29 20:56:27 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-05-29 20:56:27,332] {spark_submit.py:490} INFO - 23/05/29 20:56:27 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f67a1683-5d2a-40d8-acb5-fd8428e9c402
[2023-05-29 20:56:27,343] {spark_submit.py:490} INFO - 23/05/29 20:56:27 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-05-29 20:56:27,353] {spark_submit.py:490} INFO - 23/05/29 20:56:27 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-05-29 20:56:27,446] {spark_submit.py:490} INFO - 23/05/29 20:56:27 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2023-05-29 20:56:27,488] {spark_submit.py:490} INFO - 23/05/29 20:56:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-05-29 20:56:27,494] {spark_submit.py:490} INFO - 23/05/29 20:56:27 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-05-29 20:56:27,610] {spark_submit.py:490} INFO - 23/05/29 20:56:27 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark:7077...
[2023-05-29 20:56:27,648] {spark_submit.py:490} INFO - 23/05/29 20:56:27 INFO TransportClientFactory: Successfully created connection to spark/172.23.0.3:7077 after 19 ms (0 ms spent in bootstraps)
[2023-05-29 20:56:27,735] {spark_submit.py:490} INFO - 23/05/29 20:56:27 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20230529205627-0005
[2023-05-29 20:56:27,765] {spark_submit.py:490} INFO - 23/05/29 20:56:27 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20230529205627-0005/0 on worker-20230529205018-172.23.0.9-38081 (172.23.0.9:38081) with 1 core(s)
[2023-05-29 20:56:27,769] {spark_submit.py:490} INFO - 23/05/29 20:56:27 INFO StandaloneSchedulerBackend: Granted executor ID app-20230529205627-0005/0 on hostPort 172.23.0.9:38081 with 1 core(s), 1024.0 MiB RAM
[2023-05-29 20:56:27,770] {spark_submit.py:490} INFO - 23/05/29 20:56:27 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20230529205627-0005/1 on worker-20230529205017-172.23.0.4-33813 (172.23.0.4:33813) with 1 core(s)
[2023-05-29 20:56:27,772] {spark_submit.py:490} INFO - 23/05/29 20:56:27 INFO StandaloneSchedulerBackend: Granted executor ID app-20230529205627-0005/1 on hostPort 172.23.0.4:33813 with 1 core(s), 1024.0 MiB RAM
[2023-05-29 20:56:27,776] {spark_submit.py:490} INFO - 23/05/29 20:56:27 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20230529205627-0005/2 on worker-20230529205018-172.23.0.2-37121 (172.23.0.2:37121) with 1 core(s)
[2023-05-29 20:56:27,777] {spark_submit.py:490} INFO - 23/05/29 20:56:27 INFO StandaloneSchedulerBackend: Granted executor ID app-20230529205627-0005/2 on hostPort 172.23.0.2:37121 with 1 core(s), 1024.0 MiB RAM
[2023-05-29 20:56:27,783] {spark_submit.py:490} INFO - 23/05/29 20:56:27 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20230529205627-0005/3 on worker-20230529205017-172.23.0.8-35605 (172.23.0.8:35605) with 1 core(s)
[2023-05-29 20:56:27,784] {spark_submit.py:490} INFO - 23/05/29 20:56:27 INFO StandaloneSchedulerBackend: Granted executor ID app-20230529205627-0005/3 on hostPort 172.23.0.8:35605 with 1 core(s), 1024.0 MiB RAM
[2023-05-29 20:56:27,785] {spark_submit.py:490} INFO - 23/05/29 20:56:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44821.
[2023-05-29 20:56:27,786] {spark_submit.py:490} INFO - 23/05/29 20:56:27 INFO NettyBlockTransferService: Server created on 5b0f4fa1b7e1:44821
[2023-05-29 20:56:27,790] {spark_submit.py:490} INFO - 23/05/29 20:56:27 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-05-29 20:56:27,847] {spark_submit.py:490} INFO - 23/05/29 20:56:27 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230529205627-0005/3 is now RUNNING
[2023-05-29 20:56:27,848] {spark_submit.py:490} INFO - 23/05/29 20:56:27 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 5b0f4fa1b7e1, 44821, None)
[2023-05-29 20:56:27,852] {spark_submit.py:490} INFO - 23/05/29 20:56:27 INFO BlockManagerMasterEndpoint: Registering block manager 5b0f4fa1b7e1:44821 with 434.4 MiB RAM, BlockManagerId(driver, 5b0f4fa1b7e1, 44821, None)
[2023-05-29 20:56:27,855] {spark_submit.py:490} INFO - 23/05/29 20:56:27 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 5b0f4fa1b7e1, 44821, None)
[2023-05-29 20:56:27,857] {spark_submit.py:490} INFO - 23/05/29 20:56:27 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 5b0f4fa1b7e1, 44821, None)
[2023-05-29 20:56:27,865] {spark_submit.py:490} INFO - 23/05/29 20:56:27 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230529205627-0005/2 is now RUNNING
[2023-05-29 20:56:27,880] {spark_submit.py:490} INFO - 23/05/29 20:56:27 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230529205627-0005/0 is now RUNNING
[2023-05-29 20:56:27,884] {spark_submit.py:490} INFO - 23/05/29 20:56:27 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230529205627-0005/1 is now RUNNING
[2023-05-29 20:56:28,472] {spark_submit.py:490} INFO - 23/05/29 20:56:28 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2023-05-29 20:56:28,521] {spark_submit.py:490} INFO - /home/***/.local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.
[2023-05-29 20:56:28,522] {spark_submit.py:490} INFO - warnings.warn("Python 3.7 support is deprecated in Spark 3.4.", FutureWarning)
[2023-05-29 20:56:29,193] {spark_submit.py:490} INFO - 23/05/29 20:56:29 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-05-29 20:56:29,211] {spark_submit.py:490} INFO - 23/05/29 20:56:29 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-05-29 20:56:32,451] {spark_submit.py:490} INFO - 23/05/29 20:56:32 INFO InMemoryFileIndex: It took 225 ms to list leaf files for 1 paths.
[2023-05-29 20:56:32,528] {spark_submit.py:490} INFO - 23/05/29 20:56:32 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.23.0.9:47352) with ID 0,  ResourceProfileId 0
[2023-05-29 20:56:32,624] {spark_submit.py:490} INFO - 23/05/29 20:56:32 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.23.0.2:36250) with ID 2,  ResourceProfileId 0
[2023-05-29 20:56:32,646] {spark_submit.py:490} INFO - 23/05/29 20:56:32 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.23.0.8:56410) with ID 3,  ResourceProfileId 0
[2023-05-29 20:56:32,662] {spark_submit.py:490} INFO - 23/05/29 20:56:32 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.23.0.4:33036) with ID 1,  ResourceProfileId 0
[2023-05-29 20:56:32,736] {spark_submit.py:490} INFO - 23/05/29 20:56:32 INFO BlockManagerMasterEndpoint: Registering block manager 172.23.0.9:41877 with 434.4 MiB RAM, BlockManagerId(0, 172.23.0.9, 41877, None)
[2023-05-29 20:56:32,743] {spark_submit.py:490} INFO - 23/05/29 20:56:32 INFO BlockManagerMasterEndpoint: Registering block manager 172.23.0.2:35575 with 434.4 MiB RAM, BlockManagerId(2, 172.23.0.2, 35575, None)
[2023-05-29 20:56:32,754] {spark_submit.py:490} INFO - 23/05/29 20:56:32 INFO BlockManagerMasterEndpoint: Registering block manager 172.23.0.8:40007 with 434.4 MiB RAM, BlockManagerId(3, 172.23.0.8, 40007, None)
[2023-05-29 20:56:32,817] {spark_submit.py:490} INFO - 23/05/29 20:56:32 INFO BlockManagerMasterEndpoint: Registering block manager 172.23.0.4:44551 with 434.4 MiB RAM, BlockManagerId(1, 172.23.0.4, 44551, None)
[2023-05-29 20:56:32,850] {spark_submit.py:490} INFO - 23/05/29 20:56:32 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2023-05-29 20:56:34,435] {spark_submit.py:490} INFO - 23/05/29 20:56:34 INFO FileSourceStrategy: Pushed Filters:
[2023-05-29 20:56:34,436] {spark_submit.py:490} INFO - 23/05/29 20:56:34 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
[2023-05-29 20:56:35,350] {spark_submit.py:490} INFO - 23/05/29 20:56:35 INFO CodeGenerator: Code generated in 439.471084 ms
[2023-05-29 20:56:35,396] {spark_submit.py:490} INFO - 23/05/29 20:56:35 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.5 KiB, free 434.2 MiB)
[2023-05-29 20:56:35,428] {spark_submit.py:490} INFO - 23/05/29 20:56:35 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 434.2 MiB)
[2023-05-29 20:56:35,430] {spark_submit.py:490} INFO - 23/05/29 20:56:35 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 5b0f4fa1b7e1:44821 (size: 34.3 KiB, free: 434.4 MiB)
[2023-05-29 20:56:35,432] {spark_submit.py:490} INFO - 23/05/29 20:56:35 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
[2023-05-29 20:56:35,438] {spark_submit.py:490} INFO - 23/05/29 20:56:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-29 20:56:35,493] {spark_submit.py:490} INFO - 23/05/29 20:56:35 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2023-05-29 20:56:35,513] {spark_submit.py:490} INFO - 23/05/29 20:56:35 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-29 20:56:35,516] {spark_submit.py:490} INFO - 23/05/29 20:56:35 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
[2023-05-29 20:56:35,517] {spark_submit.py:490} INFO - 23/05/29 20:56:35 INFO DAGScheduler: Parents of final stage: List()
[2023-05-29 20:56:35,518] {spark_submit.py:490} INFO - 23/05/29 20:56:35 INFO DAGScheduler: Missing parents: List()
[2023-05-29 20:56:35,530] {spark_submit.py:490} INFO - 23/05/29 20:56:35 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-29 20:56:35,574] {spark_submit.py:490} INFO - 23/05/29 20:56:35 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 434.2 MiB)
[2023-05-29 20:56:35,576] {spark_submit.py:490} INFO - 23/05/29 20:56:35 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 434.2 MiB)
[2023-05-29 20:56:35,577] {spark_submit.py:490} INFO - 23/05/29 20:56:35 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 5b0f4fa1b7e1:44821 (size: 6.0 KiB, free: 434.4 MiB)
[2023-05-29 20:56:35,578] {spark_submit.py:490} INFO - 23/05/29 20:56:35 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535
[2023-05-29 20:56:35,588] {spark_submit.py:490} INFO - 23/05/29 20:56:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-29 20:56:35,591] {spark_submit.py:490} INFO - 23/05/29 20:56:35 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-05-29 20:56:35,623] {spark_submit.py:490} INFO - 23/05/29 20:56:35 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.23.0.4, executor 1, partition 0, PROCESS_LOCAL, 7935 bytes)
[2023-05-29 20:56:35,742] {spark_submit.py:490} INFO - 23/05/29 20:56:35 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.23.0.4:44551 (size: 6.0 KiB, free: 434.4 MiB)
[2023-05-29 20:56:36,153] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.23.0.4:44551 (size: 34.3 KiB, free: 434.4 MiB)
[2023-05-29 20:56:36,267] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 649 ms on 172.23.0.4 (executor 1) (1/1)
[2023-05-29 20:56:36,268] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-05-29 20:56:36,278] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 0.740 s
[2023-05-29 20:56:36,280] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-29 20:56:36,280] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-05-29 20:56:36,281] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 0.787773 s
[2023-05-29 20:56:36,295] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO CodeGenerator: Code generated in 6.062333 ms
[2023-05-29 20:56:36,323] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO FileSourceStrategy: Pushed Filters:
[2023-05-29 20:56:36,324] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO FileSourceStrategy: Post-Scan Filters:
[2023-05-29 20:56:36,330] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.5 KiB, free 434.0 MiB)
[2023-05-29 20:56:36,337] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.9 MiB)
[2023-05-29 20:56:36,339] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 5b0f4fa1b7e1:44821 (size: 34.3 KiB, free: 434.3 MiB)
[2023-05-29 20:56:36,339] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
[2023-05-29 20:56:36,340] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-29 20:56:36,412] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 5 paths.
[2023-05-29 20:56:36,417] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 5 paths.
[2023-05-29 20:56:36,452] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO FileSourceStrategy: Pushed Filters:
[2023-05-29 20:56:36,452] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#44, None)) > 0)
[2023-05-29 20:56:36,459] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 199.5 KiB, free 433.7 MiB)
[2023-05-29 20:56:36,463] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.7 MiB)
[2023-05-29 20:56:36,464] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 5b0f4fa1b7e1:44821 (size: 34.3 KiB, free: 434.3 MiB)
[2023-05-29 20:56:36,465] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO SparkContext: Created broadcast 3 from load at NativeMethodAccessorImpl.java:0
[2023-05-29 20:56:36,466] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 5929104 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-29 20:56:36,480] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-05-29 20:56:36,484] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO DAGScheduler: Got job 1 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-29 20:56:36,485] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO DAGScheduler: Final stage: ResultStage 1 (load at NativeMethodAccessorImpl.java:0)
[2023-05-29 20:56:36,485] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO DAGScheduler: Parents of final stage: List()
[2023-05-29 20:56:36,486] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO DAGScheduler: Missing parents: List()
[2023-05-29 20:56:36,487] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[13] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-29 20:56:36,494] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 12.1 KiB, free 433.7 MiB)
[2023-05-29 20:56:36,507] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 433.7 MiB)
[2023-05-29 20:56:36,510] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 5b0f4fa1b7e1:44821 (size: 6.0 KiB, free: 434.3 MiB)
[2023-05-29 20:56:36,512] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1535
[2023-05-29 20:56:36,529] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[13] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-29 20:56:36,530] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-05-29 20:56:36,531] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.23.0.9, executor 0, partition 0, PROCESS_LOCAL, 7926 bytes)
[2023-05-29 20:56:36,538] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 5b0f4fa1b7e1:44821 in memory (size: 34.3 KiB, free: 434.3 MiB)
[2023-05-29 20:56:36,557] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 5b0f4fa1b7e1:44821 in memory (size: 6.0 KiB, free: 434.3 MiB)
[2023-05-29 20:56:36,558] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.23.0.4:44551 in memory (size: 6.0 KiB, free: 434.4 MiB)
[2023-05-29 20:56:36,577] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.23.0.4:44551 in memory (size: 34.3 KiB, free: 434.4 MiB)
[2023-05-29 20:56:36,579] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 5b0f4fa1b7e1:44821 in memory (size: 34.3 KiB, free: 434.4 MiB)
[2023-05-29 20:56:36,656] {spark_submit.py:490} INFO - 23/05/29 20:56:36 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.23.0.9:41877 (size: 6.0 KiB, free: 434.4 MiB)
[2023-05-29 20:56:37,016] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.23.0.9:41877 (size: 34.3 KiB, free: 434.4 MiB)
[2023-05-29 20:56:37,120] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 588 ms on 172.23.0.9 (executor 0) (1/1)
[2023-05-29 20:56:37,121] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-05-29 20:56:37,121] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO DAGScheduler: ResultStage 1 (load at NativeMethodAccessorImpl.java:0) finished in 0.632 s
[2023-05-29 20:56:37,122] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-29 20:56:37,122] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-05-29 20:56:37,123] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO DAGScheduler: Job 1 finished: load at NativeMethodAccessorImpl.java:0, took 0.640915 s
[2023-05-29 20:56:37,131] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO FileSourceStrategy: Pushed Filters:
[2023-05-29 20:56:37,132] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO FileSourceStrategy: Post-Scan Filters:
[2023-05-29 20:56:37,135] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 199.5 KiB, free 434.0 MiB)
[2023-05-29 20:56:37,140] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.9 MiB)
[2023-05-29 20:56:37,141] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 5b0f4fa1b7e1:44821 (size: 34.3 KiB, free: 434.3 MiB)
[2023-05-29 20:56:37,141] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO SparkContext: Created broadcast 5 from load at NativeMethodAccessorImpl.java:0
[2023-05-29 20:56:37,142] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 5929104 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-29 20:56:37,231] {spark_submit.py:490} INFO - /usr/local/spark/staging/20230529/raw_data_processing/stocks.parquet
[2023-05-29 20:56:37,318] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO FileSourceStrategy: Pushed Filters:
[2023-05-29 20:56:37,319] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO FileSourceStrategy: Post-Scan Filters:
[2023-05-29 20:56:37,319] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO FileSourceStrategy: Pushed Filters: IsNotNull(Symbol)
[2023-05-29 20:56:37,320] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(Symbol#18)
[2023-05-29 20:56:37,378] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO CodeGenerator: Code generated in 7.121166 ms
[2023-05-29 20:56:37,381] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 199.4 KiB, free 433.7 MiB)
[2023-05-29 20:56:37,385] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.2 KiB, free 433.7 MiB)
[2023-05-29 20:56:37,385] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 5b0f4fa1b7e1:44821 (size: 34.2 KiB, free: 434.3 MiB)
[2023-05-29 20:56:37,386] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO SparkContext: Created broadcast 6 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2023-05-29 20:56:37,392] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-29 20:56:37,413] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2023-05-29 20:56:37,414] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO DAGScheduler: Got job 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
[2023-05-29 20:56:37,415] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO DAGScheduler: Final stage: ResultStage 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2023-05-29 20:56:37,415] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO DAGScheduler: Parents of final stage: List()
[2023-05-29 20:56:37,416] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO DAGScheduler: Missing parents: List()
[2023-05-29 20:56:37,416] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[23] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2023-05-29 20:56:37,417] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 14.4 KiB, free 433.7 MiB)
[2023-05-29 20:56:37,417] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 433.7 MiB)
[2023-05-29 20:56:37,418] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 5b0f4fa1b7e1:44821 (size: 7.4 KiB, free: 434.3 MiB)
[2023-05-29 20:56:37,418] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1535
[2023-05-29 20:56:37,419] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[23] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
[2023-05-29 20:56:37,419] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-05-29 20:56:37,420] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.23.0.2, executor 2, partition 0, PROCESS_LOCAL, 7935 bytes)
[2023-05-29 20:56:37,524] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.23.0.2:35575 (size: 7.4 KiB, free: 434.4 MiB)
[2023-05-29 20:56:37,900] {spark_submit.py:490} INFO - 23/05/29 20:56:37 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.23.0.2:35575 (size: 34.2 KiB, free: 434.4 MiB)
[2023-05-29 20:56:38,099] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 679 ms on 172.23.0.2 (executor 2) (1/1)
[2023-05-29 20:56:38,099] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-05-29 20:56:38,101] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO DAGScheduler: ResultStage 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.685 s
[2023-05-29 20:56:38,101] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-29 20:56:38,102] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-05-29 20:56:38,102] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO DAGScheduler: Job 2 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.687927 s
[2023-05-29 20:56:38,129] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO CodeGenerator: Code generated in 4.142083 ms
[2023-05-29 20:56:38,181] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.23.0.2:35575 in memory (size: 7.4 KiB, free: 434.4 MiB)
[2023-05-29 20:56:38,185] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 5b0f4fa1b7e1:44821 in memory (size: 7.4 KiB, free: 434.3 MiB)
[2023-05-29 20:56:38,199] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 364.4 KiB, free 433.3 MiB)
[2023-05-29 20:56:38,205] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 5b0f4fa1b7e1:44821 (size: 364.4 KiB, free: 433.9 MiB)
[2023-05-29 20:56:38,207] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO SparkContext: Created broadcast 8 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2023-05-29 20:56:38,220] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 5b0f4fa1b7e1:44821 in memory (size: 34.3 KiB, free: 434.0 MiB)
[2023-05-29 20:56:38,252] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO FileSourceStrategy: Pushed Filters:
[2023-05-29 20:56:38,254] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO FileSourceStrategy: Post-Scan Filters:
[2023-05-29 20:56:38,263] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.23.0.9:41877 in memory (size: 34.3 KiB, free: 434.4 MiB)
[2023-05-29 20:56:38,267] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 5b0f4fa1b7e1:44821 in memory (size: 34.3 KiB, free: 434.0 MiB)
[2023-05-29 20:56:38,284] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 5b0f4fa1b7e1:44821 in memory (size: 6.0 KiB, free: 434.0 MiB)
[2023-05-29 20:56:38,285] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.23.0.9:41877 in memory (size: 6.0 KiB, free: 434.4 MiB)
[2023-05-29 20:56:38,309] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-05-29 20:56:38,318] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-29 20:56:38,321] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-29 20:56:38,322] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-05-29 20:56:38,323] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-29 20:56:38,324] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-29 20:56:38,324] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-05-29 20:56:38,394] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO CodeGenerator: Code generated in 31.680458 ms
[2023-05-29 20:56:38,396] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 199.4 KiB, free 433.6 MiB)
[2023-05-29 20:56:38,401] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 34.2 KiB, free 433.6 MiB)
[2023-05-29 20:56:38,402] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 5b0f4fa1b7e1:44821 (size: 34.2 KiB, free: 434.0 MiB)
[2023-05-29 20:56:38,403] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO SparkContext: Created broadcast 9 from parquet at NativeMethodAccessorImpl.java:0
[2023-05-29 20:56:38,403] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 5929104 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-29 20:56:38,414] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2023-05-29 20:56:38,414] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO DAGScheduler: Got job 3 (parquet at NativeMethodAccessorImpl.java:0) with 3 output partitions
[2023-05-29 20:56:38,415] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO DAGScheduler: Final stage: ResultStage 3 (parquet at NativeMethodAccessorImpl.java:0)
[2023-05-29 20:56:38,415] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO DAGScheduler: Parents of final stage: List()
[2023-05-29 20:56:38,416] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO DAGScheduler: Missing parents: List()
[2023-05-29 20:56:38,416] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[27] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-29 20:56:38,434] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 221.9 KiB, free 433.4 MiB)
[2023-05-29 20:56:38,437] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 79.8 KiB, free 433.3 MiB)
[2023-05-29 20:56:38,438] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 5b0f4fa1b7e1:44821 (size: 79.8 KiB, free: 433.9 MiB)
[2023-05-29 20:56:38,438] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1535
[2023-05-29 20:56:38,439] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 3 (MapPartitionsRDD[27] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
[2023-05-29 20:56:38,440] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO TaskSchedulerImpl: Adding task set 3.0 with 3 tasks resource profile 0
[2023-05-29 20:56:38,440] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (172.23.0.8, executor 3, partition 0, PROCESS_LOCAL, 7926 bytes)
[2023-05-29 20:56:38,441] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 4) (172.23.0.9, executor 0, partition 1, PROCESS_LOCAL, 8020 bytes)
[2023-05-29 20:56:38,458] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 5) (172.23.0.2, executor 2, partition 2, PROCESS_LOCAL, 8024 bytes)
[2023-05-29 20:56:38,478] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.23.0.2:35575 (size: 79.8 KiB, free: 434.3 MiB)
[2023-05-29 20:56:38,479] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.23.0.9:41877 (size: 79.8 KiB, free: 434.3 MiB)
[2023-05-29 20:56:38,638] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.23.0.9:41877 (size: 364.4 KiB, free: 434.0 MiB)
[2023-05-29 20:56:38,657] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.23.0.8:40007 (size: 79.8 KiB, free: 434.3 MiB)
[2023-05-29 20:56:38,684] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.23.0.2:35575 (size: 364.4 KiB, free: 433.9 MiB)
[2023-05-29 20:56:38,786] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.23.0.9:41877 (size: 34.2 KiB, free: 433.9 MiB)
[2023-05-29 20:56:38,874] {spark_submit.py:490} INFO - 23/05/29 20:56:38 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.23.0.2:35575 (size: 34.2 KiB, free: 433.9 MiB)
[2023-05-29 20:56:39,687] {spark_submit.py:490} INFO - 23/05/29 20:56:39 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.23.0.8:40007 (size: 364.4 KiB, free: 434.0 MiB)
[2023-05-29 20:56:39,986] {spark_submit.py:490} INFO - 23/05/29 20:56:39 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 5) in 1545 ms on 172.23.0.2 (executor 2) (1/3)
[2023-05-29 20:56:40,143] {spark_submit.py:490} INFO - 23/05/29 20:56:40 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 4) in 1703 ms on 172.23.0.9 (executor 0) (2/3)
[2023-05-29 20:56:40,193] {spark_submit.py:490} INFO - 23/05/29 20:56:40 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.23.0.8:40007 (size: 34.2 KiB, free: 433.9 MiB)
[2023-05-29 20:56:40,698] {spark_submit.py:490} INFO - 23/05/29 20:56:40 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 2258 ms on 172.23.0.8 (executor 3) (3/3)
[2023-05-29 20:56:40,699] {spark_submit.py:490} INFO - 23/05/29 20:56:40 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-05-29 20:56:40,700] {spark_submit.py:490} INFO - 23/05/29 20:56:40 INFO DAGScheduler: ResultStage 3 (parquet at NativeMethodAccessorImpl.java:0) finished in 2.283 s
[2023-05-29 20:56:40,700] {spark_submit.py:490} INFO - 23/05/29 20:56:40 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-29 20:56:40,701] {spark_submit.py:490} INFO - 23/05/29 20:56:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-05-29 20:56:40,702] {spark_submit.py:490} INFO - 23/05/29 20:56:40 INFO DAGScheduler: Job 3 finished: parquet at NativeMethodAccessorImpl.java:0, took 2.287169 s
[2023-05-29 20:56:40,703] {spark_submit.py:490} INFO - 23/05/29 20:56:40 INFO FileFormatWriter: Start to commit write Job 300edc30-3400-4ba3-b9ed-60ac2e4735ac.
[2023-05-29 20:56:40,763] {spark_submit.py:490} INFO - 23/05/29 20:56:40 INFO FileFormatWriter: Write Job 300edc30-3400-4ba3-b9ed-60ac2e4735ac committed. Elapsed time: 59 ms.
[2023-05-29 20:56:40,765] {spark_submit.py:490} INFO - 23/05/29 20:56:40 INFO FileFormatWriter: Finished processing stats for write job 300edc30-3400-4ba3-b9ed-60ac2e4735ac.
[2023-05-29 20:56:40,803] {spark_submit.py:490} INFO - 23/05/29 20:56:40 INFO SparkContext: Invoking stop() from shutdown hook
[2023-05-29 20:56:40,804] {spark_submit.py:490} INFO - 23/05/29 20:56:40 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2023-05-29 20:56:40,810] {spark_submit.py:490} INFO - 23/05/29 20:56:40 INFO SparkUI: Stopped Spark web UI at http://5b0f4fa1b7e1:4041
[2023-05-29 20:56:40,815] {spark_submit.py:490} INFO - 23/05/29 20:56:40 INFO StandaloneSchedulerBackend: Shutting down all executors
[2023-05-29 20:56:40,833] {spark_submit.py:490} INFO - 23/05/29 20:56:40 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
[2023-05-29 20:56:40,941] {spark_submit.py:490} INFO - 23/05/29 20:56:40 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-05-29 20:56:40,985] {spark_submit.py:490} INFO - 23/05/29 20:56:40 INFO MemoryStore: MemoryStore cleared
[2023-05-29 20:56:40,988] {spark_submit.py:490} INFO - 23/05/29 20:56:40 INFO BlockManager: BlockManager stopped
[2023-05-29 20:56:41,004] {spark_submit.py:490} INFO - 23/05/29 20:56:41 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-05-29 20:56:41,012] {spark_submit.py:490} INFO - 23/05/29 20:56:41 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-05-29 20:56:41,058] {spark_submit.py:490} INFO - 23/05/29 20:56:41 INFO SparkContext: Successfully stopped SparkContext
[2023-05-29 20:56:41,059] {spark_submit.py:490} INFO - 23/05/29 20:56:41 INFO ShutdownHookManager: Shutdown hook called
[2023-05-29 20:56:41,059] {spark_submit.py:490} INFO - 23/05/29 20:56:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-97bf3418-f71a-4984-93d8-3fed93746d29
[2023-05-29 20:56:41,074] {spark_submit.py:490} INFO - 23/05/29 20:56:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-97bf3418-f71a-4984-93d8-3fed93746d29/pyspark-f0883175-4e44-45f2-84d4-8977f7ce31c5
[2023-05-29 20:56:41,080] {spark_submit.py:490} INFO - 23/05/29 20:56:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-3aeb6dcc-f23a-4a3c-babe-e087fce4ac2c
[2023-05-29 20:56:41,373] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=stock_spark_***, task_id=raw_data_processing.stock_data_processing, execution_date=20230529T205622, start_date=20230529T205624, end_date=20230529T205641
[2023-05-29 20:56:41,427] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-05-29 20:56:41,472] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
