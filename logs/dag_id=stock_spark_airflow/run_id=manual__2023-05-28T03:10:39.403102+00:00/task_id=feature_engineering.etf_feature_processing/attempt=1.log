[2023-05-28 03:13:08,515] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: stock_spark_airflow.feature_engineering.etf_feature_processing manual__2023-05-28T03:10:39.403102+00:00 [queued]>
[2023-05-28 03:13:08,522] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: stock_spark_airflow.feature_engineering.etf_feature_processing manual__2023-05-28T03:10:39.403102+00:00 [queued]>
[2023-05-28 03:13:08,523] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-05-28 03:13:08,523] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2023-05-28 03:13:08,524] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-05-28 03:13:08,531] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): feature_engineering.etf_feature_processing> on 2023-05-28 03:10:39.403102+00:00
[2023-05-28 03:13:08,536] {standard_task_runner.py:52} INFO - Started process 12693 to run task
[2023-05-28 03:13:08,540] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'stock_spark_***', 'feature_engineering.etf_feature_processing', 'manual__2023-05-28T03:10:39.403102+00:00', '--job-id', '319', '--raw', '--subdir', 'DAGS_FOLDER/stock_spark_ml.py', '--cfg-path', '/tmp/tmpzocgcrqc', '--error-file', '/tmp/tmp76o1dp9j']
[2023-05-28 03:13:08,541] {standard_task_runner.py:80} INFO - Job 319: Subtask feature_engineering.etf_feature_processing
[2023-05-28 03:13:08,597] {task_command.py:369} INFO - Running <TaskInstance: stock_spark_airflow.feature_engineering.etf_feature_processing manual__2023-05-28T03:10:39.403102+00:00 [running]> on host c1224b457a2d
[2023-05-28 03:13:08,653] {taskinstance.py:1571} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=stock_spark_***
AIRFLOW_CTX_TASK_ID=feature_engineering.etf_feature_processing
AIRFLOW_CTX_EXECUTION_DATE=2023-05-28T03:10:39.403102+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=manual__2023-05-28T03:10:39.403102+00:00
[2023-05-28 03:13:08,662] {base.py:68} INFO - Using connection ID 'spark_local' for task execution.
[2023-05-28 03:13:08,664] {spark_submit.py:339} INFO - Spark-Submit cmd: spark-submit --master local[*] --name arrow-spark /usr/local/spark/app/feature_engineering_processing.py etfs
[2023-05-28 03:13:08,749] {spark_submit.py:490} INFO - /home/***/.local/lib/python3.7/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2023-05-28 03:13:10,924] {spark_submit.py:490} INFO - 23/05/28 03:13:10 INFO SparkContext: Running Spark version 3.4.0
[2023-05-28 03:13:10,982] {spark_submit.py:490} INFO - 23/05/28 03:13:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-05-28 03:13:11,074] {spark_submit.py:490} INFO - 23/05/28 03:13:11 INFO ResourceUtils: ==============================================================
[2023-05-28 03:13:11,075] {spark_submit.py:490} INFO - 23/05/28 03:13:11 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-05-28 03:13:11,076] {spark_submit.py:490} INFO - 23/05/28 03:13:11 INFO ResourceUtils: ==============================================================
[2023-05-28 03:13:11,076] {spark_submit.py:490} INFO - 23/05/28 03:13:11 INFO SparkContext: Submitted application: random_reg
[2023-05-28 03:13:11,091] {spark_submit.py:490} INFO - 23/05/28 03:13:11 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-05-28 03:13:11,099] {spark_submit.py:490} INFO - 23/05/28 03:13:11 INFO ResourceProfile: Limiting resource is cpu
[2023-05-28 03:13:11,100] {spark_submit.py:490} INFO - 23/05/28 03:13:11 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-05-28 03:13:11,142] {spark_submit.py:490} INFO - 23/05/28 03:13:11 INFO SecurityManager: Changing view acls to: default
[2023-05-28 03:13:11,144] {spark_submit.py:490} INFO - 23/05/28 03:13:11 INFO SecurityManager: Changing modify acls to: default
[2023-05-28 03:13:11,144] {spark_submit.py:490} INFO - 23/05/28 03:13:11 INFO SecurityManager: Changing view acls groups to:
[2023-05-28 03:13:11,145] {spark_submit.py:490} INFO - 23/05/28 03:13:11 INFO SecurityManager: Changing modify acls groups to:
[2023-05-28 03:13:11,145] {spark_submit.py:490} INFO - 23/05/28 03:13:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
[2023-05-28 03:13:11,371] {spark_submit.py:490} INFO - 23/05/28 03:13:11 INFO Utils: Successfully started service 'sparkDriver' on port 42099.
[2023-05-28 03:13:11,466] {spark_submit.py:490} INFO - 23/05/28 03:13:11 INFO SparkEnv: Registering MapOutputTracker
[2023-05-28 03:13:11,491] {spark_submit.py:490} INFO - 23/05/28 03:13:11 INFO SparkEnv: Registering BlockManagerMaster
[2023-05-28 03:13:11,505] {spark_submit.py:490} INFO - 23/05/28 03:13:11 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-05-28 03:13:11,506] {spark_submit.py:490} INFO - 23/05/28 03:13:11 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-05-28 03:13:11,509] {spark_submit.py:490} INFO - 23/05/28 03:13:11 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-05-28 03:13:11,526] {spark_submit.py:490} INFO - 23/05/28 03:13:11 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7fb538e0-1a3e-4b32-aea4-c4ec104ba80b
[2023-05-28 03:13:11,540] {spark_submit.py:490} INFO - 23/05/28 03:13:11 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-05-28 03:13:11,550] {spark_submit.py:490} INFO - 23/05/28 03:13:11 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-05-28 03:13:11,646] {spark_submit.py:490} INFO - 23/05/28 03:13:11 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2023-05-28 03:13:11,686] {spark_submit.py:490} INFO - 23/05/28 03:13:11 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-05-28 03:13:11,775] {spark_submit.py:490} INFO - 23/05/28 03:13:11 INFO Executor: Starting executor ID driver on host c1224b457a2d
[2023-05-28 03:13:11,779] {spark_submit.py:490} INFO - 23/05/28 03:13:11 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-05-28 03:13:11,791] {spark_submit.py:490} INFO - 23/05/28 03:13:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43525.
[2023-05-28 03:13:11,792] {spark_submit.py:490} INFO - 23/05/28 03:13:11 INFO NettyBlockTransferService: Server created on c1224b457a2d:43525
[2023-05-28 03:13:11,792] {spark_submit.py:490} INFO - 23/05/28 03:13:11 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-05-28 03:13:11,797] {spark_submit.py:490} INFO - 23/05/28 03:13:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, c1224b457a2d, 43525, None)
[2023-05-28 03:13:11,800] {spark_submit.py:490} INFO - 23/05/28 03:13:11 INFO BlockManagerMasterEndpoint: Registering block manager c1224b457a2d:43525 with 434.4 MiB RAM, BlockManagerId(driver, c1224b457a2d, 43525, None)
[2023-05-28 03:13:11,802] {spark_submit.py:490} INFO - 23/05/28 03:13:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, c1224b457a2d, 43525, None)
[2023-05-28 03:13:11,803] {spark_submit.py:490} INFO - 23/05/28 03:13:11 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, c1224b457a2d, 43525, None)
[2023-05-28 03:13:11,971] {spark_submit.py:490} INFO - /home/***/.local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.
[2023-05-28 03:13:11,972] {spark_submit.py:490} INFO - warnings.warn("Python 3.7 support is deprecated in Spark 3.4.", FutureWarning)
[2023-05-28 03:13:12,105] {spark_submit.py:490} INFO - 23/05/28 03:13:12 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-05-28 03:13:12,110] {spark_submit.py:490} INFO - 23/05/28 03:13:12 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-05-28 03:13:12,964] {spark_submit.py:490} INFO - 23/05/28 03:13:12 INFO HadoopFSUtils: Listing leaf files and directories in parallel under 70 paths. The first several paths are: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00000-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00001-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00002-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00003-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00004-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00005-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00006-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00007-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00008-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00009-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet.
[2023-05-28 03:13:13,395] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-05-28 03:13:13,418] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 70 output partitions
[2023-05-28 03:13:13,424] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-05-28 03:13:13,425] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO DAGScheduler: Parents of final stage: List()
[2023-05-28 03:13:13,426] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO DAGScheduler: Missing parents: List()
[2023-05-28 03:13:13,436] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-28 03:13:13,519] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 102.8 KiB, free 434.3 MiB)
[2023-05-28 03:13:13,542] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 434.3 MiB)
[2023-05-28 03:13:13,549] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on c1224b457a2d:43525 (size: 36.9 KiB, free: 434.4 MiB)
[2023-05-28 03:13:13,556] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535
[2023-05-28 03:13:13,609] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO DAGScheduler: Submitting 70 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2023-05-28 03:13:13,611] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO TaskSchedulerImpl: Adding task set 0.0 with 70 tasks resource profile 0
[2023-05-28 03:13:13,675] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (c1224b457a2d, executor driver, partition 0, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:13,682] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (c1224b457a2d, executor driver, partition 1, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:13,687] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (c1224b457a2d, executor driver, partition 2, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:13,689] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (c1224b457a2d, executor driver, partition 3, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:13,737] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-05-28 03:13:13,737] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
[2023-05-28 03:13:13,738] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
[2023-05-28 03:13:13,744] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
[2023-05-28 03:13:13,906] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 2010 bytes result sent to driver
[2023-05-28 03:13:13,908] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 2010 bytes result sent to driver
[2023-05-28 03:13:13,909] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2010 bytes result sent to driver
[2023-05-28 03:13:13,909] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1967 bytes result sent to driver
[2023-05-28 03:13:13,926] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (c1224b457a2d, executor driver, partition 4, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:13,927] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
[2023-05-28 03:13:13,931] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (c1224b457a2d, executor driver, partition 5, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:13,937] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
[2023-05-28 03:13:13,938] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 2010 bytes result sent to driver
[2023-05-28 03:13:13,950] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 1967 bytes result sent to driver
[2023-05-28 03:13:13,959] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (c1224b457a2d, executor driver, partition 6, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:13,960] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
[2023-05-28 03:13:13,961] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (c1224b457a2d, executor driver, partition 7, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:13,962] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (c1224b457a2d, executor driver, partition 8, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:13,962] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (c1224b457a2d, executor driver, partition 9, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:13,962] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)
[2023-05-28 03:13:13,963] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)
[2023-05-28 03:13:13,964] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 282 ms on c1224b457a2d (executor driver) (1/70)
[2023-05-28 03:13:13,984] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 295 ms on c1224b457a2d (executor driver) (2/70)
[2023-05-28 03:13:13,994] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
[2023-05-28 03:13:13,998] {spark_submit.py:490} INFO - 23/05/28 03:13:13 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 316 ms on c1224b457a2d (executor driver) (3/70)
[2023-05-28 03:13:14,007] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 82 ms on c1224b457a2d (executor driver) (4/70)
[2023-05-28 03:13:14,008] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 76 ms on c1224b457a2d (executor driver) (5/70)
[2023-05-28 03:13:14,008] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 363 ms on c1224b457a2d (executor driver) (6/70)
[2023-05-28 03:13:14,022] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 1967 bytes result sent to driver
[2023-05-28 03:13:14,023] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10) (c1224b457a2d, executor driver, partition 10, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,024] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 8.0 in stage 0.0 (TID 8). 2010 bytes result sent to driver
[2023-05-28 03:13:14,033] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 10.0 in stage 0.0 (TID 10)
[2023-05-28 03:13:14,034] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 2010 bytes result sent to driver
[2023-05-28 03:13:14,034] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11) (c1224b457a2d, executor driver, partition 11, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,035] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 1967 bytes result sent to driver
[2023-05-28 03:13:14,035] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12) (c1224b457a2d, executor driver, partition 12, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,036] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 11.0 in stage 0.0 (TID 11)
[2023-05-28 03:13:14,036] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 12.0 in stage 0.0 (TID 12)
[2023-05-28 03:13:14,041] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13) (c1224b457a2d, executor driver, partition 13, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,042] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 81 ms on c1224b457a2d (executor driver) (7/70)
[2023-05-28 03:13:14,045] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 13.0 in stage 0.0 (TID 13)
[2023-05-28 03:13:14,045] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 84 ms on c1224b457a2d (executor driver) (8/70)
[2023-05-28 03:13:14,046] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 84 ms on c1224b457a2d (executor driver) (9/70)
[2023-05-28 03:13:14,046] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 87 ms on c1224b457a2d (executor driver) (10/70)
[2023-05-28 03:13:14,054] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 13.0 in stage 0.0 (TID 13). 1924 bytes result sent to driver
[2023-05-28 03:13:14,056] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14) (c1224b457a2d, executor driver, partition 14, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,057] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 14.0 in stage 0.0 (TID 14)
[2023-05-28 03:13:14,064] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 19 ms on c1224b457a2d (executor driver) (11/70)
[2023-05-28 03:13:14,088] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 11.0 in stage 0.0 (TID 11). 1967 bytes result sent to driver
[2023-05-28 03:13:14,091] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15) (c1224b457a2d, executor driver, partition 15, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,096] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 60 ms on c1224b457a2d (executor driver) (12/70)
[2023-05-28 03:13:14,101] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 12.0 in stage 0.0 (TID 12). 1967 bytes result sent to driver
[2023-05-28 03:13:14,106] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 15.0 in stage 0.0 (TID 15)
[2023-05-28 03:13:14,106] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 16.0 in stage 0.0 (TID 16) (c1224b457a2d, executor driver, partition 16, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,107] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 59 ms on c1224b457a2d (executor driver) (13/70)
[2023-05-28 03:13:14,108] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 10.0 in stage 0.0 (TID 10). 1967 bytes result sent to driver
[2023-05-28 03:13:14,108] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 17.0 in stage 0.0 (TID 17) (c1224b457a2d, executor driver, partition 17, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,113] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 17.0 in stage 0.0 (TID 17)
[2023-05-28 03:13:14,114] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 79 ms on c1224b457a2d (executor driver) (14/70)
[2023-05-28 03:13:14,115] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 14.0 in stage 0.0 (TID 14). 1967 bytes result sent to driver
[2023-05-28 03:13:14,115] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 18.0 in stage 0.0 (TID 18) (c1224b457a2d, executor driver, partition 18, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,115] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 57 ms on c1224b457a2d (executor driver) (15/70)
[2023-05-28 03:13:14,117] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 18.0 in stage 0.0 (TID 18)
[2023-05-28 03:13:14,120] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 15.0 in stage 0.0 (TID 15). 1967 bytes result sent to driver
[2023-05-28 03:13:14,123] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 17.0 in stage 0.0 (TID 17). 2010 bytes result sent to driver
[2023-05-28 03:13:14,128] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 16.0 in stage 0.0 (TID 16)
[2023-05-28 03:13:14,130] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 19.0 in stage 0.0 (TID 19) (c1224b457a2d, executor driver, partition 19, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,132] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 19.0 in stage 0.0 (TID 19)
[2023-05-28 03:13:14,133] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 40 ms on c1224b457a2d (executor driver) (16/70)
[2023-05-28 03:13:14,139] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 20.0 in stage 0.0 (TID 20) (c1224b457a2d, executor driver, partition 20, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,140] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 17.0 in stage 0.0 (TID 17) in 39 ms on c1224b457a2d (executor driver) (17/70)
[2023-05-28 03:13:14,140] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 20.0 in stage 0.0 (TID 20)
[2023-05-28 03:13:14,152] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 18.0 in stage 0.0 (TID 18). 1967 bytes result sent to driver
[2023-05-28 03:13:14,153] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 21.0 in stage 0.0 (TID 21) (c1224b457a2d, executor driver, partition 21, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,153] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 20.0 in stage 0.0 (TID 20). 1924 bytes result sent to driver
[2023-05-28 03:13:14,155] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 16.0 in stage 0.0 (TID 16). 1924 bytes result sent to driver
[2023-05-28 03:13:14,155] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 18.0 in stage 0.0 (TID 18) in 42 ms on c1224b457a2d (executor driver) (18/70)
[2023-05-28 03:13:14,158] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 21.0 in stage 0.0 (TID 21)
[2023-05-28 03:13:14,158] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 19.0 in stage 0.0 (TID 19). 1967 bytes result sent to driver
[2023-05-28 03:13:14,159] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 22.0 in stage 0.0 (TID 22) (c1224b457a2d, executor driver, partition 22, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,160] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 19.0 in stage 0.0 (TID 19) in 30 ms on c1224b457a2d (executor driver) (19/70)
[2023-05-28 03:13:14,161] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 23.0 in stage 0.0 (TID 23) (c1224b457a2d, executor driver, partition 23, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,163] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 24.0 in stage 0.0 (TID 24) (c1224b457a2d, executor driver, partition 24, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,163] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 23.0 in stage 0.0 (TID 23)
[2023-05-28 03:13:14,164] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 20.0 in stage 0.0 (TID 20) in 24 ms on c1224b457a2d (executor driver) (20/70)
[2023-05-28 03:13:14,164] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 16.0 in stage 0.0 (TID 16) in 71 ms on c1224b457a2d (executor driver) (21/70)
[2023-05-28 03:13:14,165] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 22.0 in stage 0.0 (TID 22)
[2023-05-28 03:13:14,167] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 24.0 in stage 0.0 (TID 24)
[2023-05-28 03:13:14,171] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 23.0 in stage 0.0 (TID 23). 1924 bytes result sent to driver
[2023-05-28 03:13:14,200] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 21.0 in stage 0.0 (TID 21). 1967 bytes result sent to driver
[2023-05-28 03:13:14,201] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 24.0 in stage 0.0 (TID 24). 1924 bytes result sent to driver
[2023-05-28 03:13:14,202] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 22.0 in stage 0.0 (TID 22). 2053 bytes result sent to driver
[2023-05-28 03:13:14,203] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 25.0 in stage 0.0 (TID 25) (c1224b457a2d, executor driver, partition 25, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,204] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 24.0 in stage 0.0 (TID 24) in 41 ms on c1224b457a2d (executor driver) (22/70)
[2023-05-28 03:13:14,215] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 26.0 in stage 0.0 (TID 26) (c1224b457a2d, executor driver, partition 26, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,217] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 27.0 in stage 0.0 (TID 27) (c1224b457a2d, executor driver, partition 27, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,218] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 26.0 in stage 0.0 (TID 26)
[2023-05-28 03:13:14,219] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 21.0 in stage 0.0 (TID 21) in 66 ms on c1224b457a2d (executor driver) (23/70)
[2023-05-28 03:13:14,220] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 23.0 in stage 0.0 (TID 23) in 57 ms on c1224b457a2d (executor driver) (24/70)
[2023-05-28 03:13:14,221] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 27.0 in stage 0.0 (TID 27)
[2023-05-28 03:13:14,225] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 22.0 in stage 0.0 (TID 22) in 66 ms on c1224b457a2d (executor driver) (25/70)
[2023-05-28 03:13:14,228] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 28.0 in stage 0.0 (TID 28) (c1224b457a2d, executor driver, partition 28, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,228] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 28.0 in stage 0.0 (TID 28)
[2023-05-28 03:13:14,243] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 26.0 in stage 0.0 (TID 26). 1924 bytes result sent to driver
[2023-05-28 03:13:14,246] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 29.0 in stage 0.0 (TID 29) (c1224b457a2d, executor driver, partition 29, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,246] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 26.0 in stage 0.0 (TID 26) in 31 ms on c1224b457a2d (executor driver) (26/70)
[2023-05-28 03:13:14,247] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 29.0 in stage 0.0 (TID 29)
[2023-05-28 03:13:14,248] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 27.0 in stage 0.0 (TID 27). 1967 bytes result sent to driver
[2023-05-28 03:13:14,248] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 28.0 in stage 0.0 (TID 28). 1924 bytes result sent to driver
[2023-05-28 03:13:14,249] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 30.0 in stage 0.0 (TID 30) (c1224b457a2d, executor driver, partition 30, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,250] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 27.0 in stage 0.0 (TID 27) in 34 ms on c1224b457a2d (executor driver) (27/70)
[2023-05-28 03:13:14,255] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 31.0 in stage 0.0 (TID 31) (c1224b457a2d, executor driver, partition 31, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,255] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 28.0 in stage 0.0 (TID 28) in 29 ms on c1224b457a2d (executor driver) (28/70)
[2023-05-28 03:13:14,260] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 29.0 in stage 0.0 (TID 29). 1924 bytes result sent to driver
[2023-05-28 03:13:14,261] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 31.0 in stage 0.0 (TID 31)
[2023-05-28 03:13:14,263] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 30.0 in stage 0.0 (TID 30)
[2023-05-28 03:13:14,264] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 32.0 in stage 0.0 (TID 32) (c1224b457a2d, executor driver, partition 32, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,265] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 29.0 in stage 0.0 (TID 29) in 18 ms on c1224b457a2d (executor driver) (29/70)
[2023-05-28 03:13:14,265] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 32.0 in stage 0.0 (TID 32)
[2023-05-28 03:13:14,266] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 25.0 in stage 0.0 (TID 25)
[2023-05-28 03:13:14,331] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 32.0 in stage 0.0 (TID 32). 1967 bytes result sent to driver
[2023-05-28 03:13:14,332] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 33.0 in stage 0.0 (TID 33) (c1224b457a2d, executor driver, partition 33, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,333] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 32.0 in stage 0.0 (TID 32) in 70 ms on c1224b457a2d (executor driver) (30/70)
[2023-05-28 03:13:14,339] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 33.0 in stage 0.0 (TID 33)
[2023-05-28 03:13:14,340] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 30.0 in stage 0.0 (TID 30). 1967 bytes result sent to driver
[2023-05-28 03:13:14,342] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 34.0 in stage 0.0 (TID 34) (c1224b457a2d, executor driver, partition 34, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,343] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 30.0 in stage 0.0 (TID 30) in 93 ms on c1224b457a2d (executor driver) (31/70)
[2023-05-28 03:13:14,343] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 34.0 in stage 0.0 (TID 34)
[2023-05-28 03:13:14,344] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 31.0 in stage 0.0 (TID 31). 1967 bytes result sent to driver
[2023-05-28 03:13:14,348] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 35.0 in stage 0.0 (TID 35) (c1224b457a2d, executor driver, partition 35, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,348] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 25.0 in stage 0.0 (TID 25). 1967 bytes result sent to driver
[2023-05-28 03:13:14,349] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 31.0 in stage 0.0 (TID 31) in 92 ms on c1224b457a2d (executor driver) (32/70)
[2023-05-28 03:13:14,349] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 36.0 in stage 0.0 (TID 36) (c1224b457a2d, executor driver, partition 36, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,350] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 25.0 in stage 0.0 (TID 25) in 145 ms on c1224b457a2d (executor driver) (33/70)
[2023-05-28 03:13:14,350] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 36.0 in stage 0.0 (TID 36)
[2023-05-28 03:13:14,356] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 34.0 in stage 0.0 (TID 34). 1924 bytes result sent to driver
[2023-05-28 03:13:14,357] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 37.0 in stage 0.0 (TID 37) (c1224b457a2d, executor driver, partition 37, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,357] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 37.0 in stage 0.0 (TID 37)
[2023-05-28 03:13:14,357] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 34.0 in stage 0.0 (TID 34) in 16 ms on c1224b457a2d (executor driver) (34/70)
[2023-05-28 03:13:14,358] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 36.0 in stage 0.0 (TID 36). 1924 bytes result sent to driver
[2023-05-28 03:13:14,361] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 38.0 in stage 0.0 (TID 38) (c1224b457a2d, executor driver, partition 38, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,362] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 36.0 in stage 0.0 (TID 36) in 14 ms on c1224b457a2d (executor driver) (35/70)
[2023-05-28 03:13:14,362] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 38.0 in stage 0.0 (TID 38)
[2023-05-28 03:13:14,363] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 35.0 in stage 0.0 (TID 35)
[2023-05-28 03:13:14,363] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 37.0 in stage 0.0 (TID 37). 1967 bytes result sent to driver
[2023-05-28 03:13:14,364] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 39.0 in stage 0.0 (TID 39) (c1224b457a2d, executor driver, partition 39, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,365] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 39.0 in stage 0.0 (TID 39)
[2023-05-28 03:13:14,365] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 37.0 in stage 0.0 (TID 37) in 8 ms on c1224b457a2d (executor driver) (36/70)
[2023-05-28 03:13:14,377] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 33.0 in stage 0.0 (TID 33). 1967 bytes result sent to driver
[2023-05-28 03:13:14,379] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 40.0 in stage 0.0 (TID 40) (c1224b457a2d, executor driver, partition 40, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,380] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 33.0 in stage 0.0 (TID 33) in 48 ms on c1224b457a2d (executor driver) (37/70)
[2023-05-28 03:13:14,381] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 40.0 in stage 0.0 (TID 40)
[2023-05-28 03:13:14,400] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 35.0 in stage 0.0 (TID 35). 1924 bytes result sent to driver
[2023-05-28 03:13:14,401] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 41.0 in stage 0.0 (TID 41) (c1224b457a2d, executor driver, partition 41, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,403] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 35.0 in stage 0.0 (TID 35) in 57 ms on c1224b457a2d (executor driver) (38/70)
[2023-05-28 03:13:14,406] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 41.0 in stage 0.0 (TID 41)
[2023-05-28 03:13:14,410] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 40.0 in stage 0.0 (TID 40). 2010 bytes result sent to driver
[2023-05-28 03:13:14,413] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 42.0 in stage 0.0 (TID 42) (c1224b457a2d, executor driver, partition 42, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,413] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 40.0 in stage 0.0 (TID 40) in 34 ms on c1224b457a2d (executor driver) (39/70)
[2023-05-28 03:13:14,414] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 42.0 in stage 0.0 (TID 42)
[2023-05-28 03:13:14,416] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 41.0 in stage 0.0 (TID 41). 1967 bytes result sent to driver
[2023-05-28 03:13:14,419] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 38.0 in stage 0.0 (TID 38). 1967 bytes result sent to driver
[2023-05-28 03:13:14,430] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 39.0 in stage 0.0 (TID 39). 1967 bytes result sent to driver
[2023-05-28 03:13:14,431] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 43.0 in stage 0.0 (TID 43) (c1224b457a2d, executor driver, partition 43, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,431] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 44.0 in stage 0.0 (TID 44) (c1224b457a2d, executor driver, partition 44, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,439] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 41.0 in stage 0.0 (TID 41) in 28 ms on c1224b457a2d (executor driver) (40/70)
[2023-05-28 03:13:14,440] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 38.0 in stage 0.0 (TID 38) in 69 ms on c1224b457a2d (executor driver) (41/70)
[2023-05-28 03:13:14,450] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 44.0 in stage 0.0 (TID 44)
[2023-05-28 03:13:14,453] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 42.0 in stage 0.0 (TID 42). 1924 bytes result sent to driver
[2023-05-28 03:13:14,457] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 45.0 in stage 0.0 (TID 45) (c1224b457a2d, executor driver, partition 45, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,459] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 42.0 in stage 0.0 (TID 42) in 25 ms on c1224b457a2d (executor driver) (42/70)
[2023-05-28 03:13:14,462] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 45.0 in stage 0.0 (TID 45)
[2023-05-28 03:13:14,463] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 45.0 in stage 0.0 (TID 45). 1924 bytes result sent to driver
[2023-05-28 03:13:14,463] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 46.0 in stage 0.0 (TID 46) (c1224b457a2d, executor driver, partition 46, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,465] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 46.0 in stage 0.0 (TID 46)
[2023-05-28 03:13:14,468] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 45.0 in stage 0.0 (TID 45) in 8 ms on c1224b457a2d (executor driver) (43/70)
[2023-05-28 03:13:14,469] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 44.0 in stage 0.0 (TID 44). 1967 bytes result sent to driver
[2023-05-28 03:13:14,469] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 43.0 in stage 0.0 (TID 43)
[2023-05-28 03:13:14,470] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 47.0 in stage 0.0 (TID 47) (c1224b457a2d, executor driver, partition 47, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,470] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 39.0 in stage 0.0 (TID 39) in 90 ms on c1224b457a2d (executor driver) (44/70)
[2023-05-28 03:13:14,471] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 48.0 in stage 0.0 (TID 48) (c1224b457a2d, executor driver, partition 48, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,471] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 44.0 in stage 0.0 (TID 44) in 30 ms on c1224b457a2d (executor driver) (45/70)
[2023-05-28 03:13:14,472] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 46.0 in stage 0.0 (TID 46). 1924 bytes result sent to driver
[2023-05-28 03:13:14,472] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 49.0 in stage 0.0 (TID 49) (c1224b457a2d, executor driver, partition 49, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,472] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 49.0 in stage 0.0 (TID 49)
[2023-05-28 03:13:14,473] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 46.0 in stage 0.0 (TID 46) in 21 ms on c1224b457a2d (executor driver) (46/70)
[2023-05-28 03:13:14,474] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 48.0 in stage 0.0 (TID 48)
[2023-05-28 03:13:14,475] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 47.0 in stage 0.0 (TID 47)
[2023-05-28 03:13:14,500] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 47.0 in stage 0.0 (TID 47). 2010 bytes result sent to driver
[2023-05-28 03:13:14,513] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 50.0 in stage 0.0 (TID 50) (c1224b457a2d, executor driver, partition 50, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,529] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 47.0 in stage 0.0 (TID 47) in 51 ms on c1224b457a2d (executor driver) (47/70)
[2023-05-28 03:13:14,531] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 50.0 in stage 0.0 (TID 50)
[2023-05-28 03:13:14,531] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 43.0 in stage 0.0 (TID 43). 2010 bytes result sent to driver
[2023-05-28 03:13:14,532] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 51.0 in stage 0.0 (TID 51) (c1224b457a2d, executor driver, partition 51, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,532] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 43.0 in stage 0.0 (TID 43) in 87 ms on c1224b457a2d (executor driver) (48/70)
[2023-05-28 03:13:14,533] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 51.0 in stage 0.0 (TID 51)
[2023-05-28 03:13:14,533] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 49.0 in stage 0.0 (TID 49). 2010 bytes result sent to driver
[2023-05-28 03:13:14,537] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 48.0 in stage 0.0 (TID 48). 2010 bytes result sent to driver
[2023-05-28 03:13:14,538] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 52.0 in stage 0.0 (TID 52) (c1224b457a2d, executor driver, partition 52, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,538] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 53.0 in stage 0.0 (TID 53) (c1224b457a2d, executor driver, partition 53, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,539] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 53.0 in stage 0.0 (TID 53)
[2023-05-28 03:13:14,539] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 52.0 in stage 0.0 (TID 52)
[2023-05-28 03:13:14,540] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 53.0 in stage 0.0 (TID 53). 1924 bytes result sent to driver
[2023-05-28 03:13:14,548] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 51.0 in stage 0.0 (TID 51). 1967 bytes result sent to driver
[2023-05-28 03:13:14,552] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 52.0 in stage 0.0 (TID 52). 1924 bytes result sent to driver
[2023-05-28 03:13:14,556] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 54.0 in stage 0.0 (TID 54) (c1224b457a2d, executor driver, partition 54, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,557] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 55.0 in stage 0.0 (TID 55) (c1224b457a2d, executor driver, partition 55, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,557] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 56.0 in stage 0.0 (TID 56) (c1224b457a2d, executor driver, partition 56, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,558] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 48.0 in stage 0.0 (TID 48) in 99 ms on c1224b457a2d (executor driver) (49/70)
[2023-05-28 03:13:14,558] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 56.0 in stage 0.0 (TID 56)
[2023-05-28 03:13:14,558] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 51.0 in stage 0.0 (TID 51) in 49 ms on c1224b457a2d (executor driver) (50/70)
[2023-05-28 03:13:14,559] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 49.0 in stage 0.0 (TID 49) in 93 ms on c1224b457a2d (executor driver) (51/70)
[2023-05-28 03:13:14,560] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 52.0 in stage 0.0 (TID 52) in 27 ms on c1224b457a2d (executor driver) (52/70)
[2023-05-28 03:13:14,560] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 53.0 in stage 0.0 (TID 53) in 27 ms on c1224b457a2d (executor driver) (53/70)
[2023-05-28 03:13:14,563] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 55.0 in stage 0.0 (TID 55)
[2023-05-28 03:13:14,565] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 50.0 in stage 0.0 (TID 50). 1967 bytes result sent to driver
[2023-05-28 03:13:14,565] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 56.0 in stage 0.0 (TID 56). 1967 bytes result sent to driver
[2023-05-28 03:13:14,565] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 57.0 in stage 0.0 (TID 57) (c1224b457a2d, executor driver, partition 57, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,566] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 50.0 in stage 0.0 (TID 50) in 65 ms on c1224b457a2d (executor driver) (54/70)
[2023-05-28 03:13:14,568] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 57.0 in stage 0.0 (TID 57)
[2023-05-28 03:13:14,569] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 54.0 in stage 0.0 (TID 54)
[2023-05-28 03:13:14,571] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 57.0 in stage 0.0 (TID 57). 1924 bytes result sent to driver
[2023-05-28 03:13:14,571] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 58.0 in stage 0.0 (TID 58) (c1224b457a2d, executor driver, partition 58, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,572] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 57.0 in stage 0.0 (TID 57) in 7 ms on c1224b457a2d (executor driver) (55/70)
[2023-05-28 03:13:14,572] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 58.0 in stage 0.0 (TID 58)
[2023-05-28 03:13:14,574] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 59.0 in stage 0.0 (TID 59) (c1224b457a2d, executor driver, partition 59, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,574] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 56.0 in stage 0.0 (TID 56) in 18 ms on c1224b457a2d (executor driver) (56/70)
[2023-05-28 03:13:14,574] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 59.0 in stage 0.0 (TID 59)
[2023-05-28 03:13:14,578] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 59.0 in stage 0.0 (TID 59). 1924 bytes result sent to driver
[2023-05-28 03:13:14,584] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 60.0 in stage 0.0 (TID 60) (c1224b457a2d, executor driver, partition 60, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,584] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 59.0 in stage 0.0 (TID 59) in 10 ms on c1224b457a2d (executor driver) (57/70)
[2023-05-28 03:13:14,585] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 60.0 in stage 0.0 (TID 60)
[2023-05-28 03:13:14,590] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 60.0 in stage 0.0 (TID 60). 1924 bytes result sent to driver
[2023-05-28 03:13:14,592] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 61.0 in stage 0.0 (TID 61) (c1224b457a2d, executor driver, partition 61, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,592] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 60.0 in stage 0.0 (TID 60) in 9 ms on c1224b457a2d (executor driver) (58/70)
[2023-05-28 03:13:14,592] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 61.0 in stage 0.0 (TID 61)
[2023-05-28 03:13:14,596] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 61.0 in stage 0.0 (TID 61). 1924 bytes result sent to driver
[2023-05-28 03:13:14,597] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 62.0 in stage 0.0 (TID 62) (c1224b457a2d, executor driver, partition 62, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,598] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 61.0 in stage 0.0 (TID 61) in 6 ms on c1224b457a2d (executor driver) (59/70)
[2023-05-28 03:13:14,599] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 54.0 in stage 0.0 (TID 54). 1967 bytes result sent to driver
[2023-05-28 03:13:14,599] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 63.0 in stage 0.0 (TID 63) (c1224b457a2d, executor driver, partition 63, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,599] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 54.0 in stage 0.0 (TID 54) in 44 ms on c1224b457a2d (executor driver) (60/70)
[2023-05-28 03:13:14,600] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 63.0 in stage 0.0 (TID 63)
[2023-05-28 03:13:14,605] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 63.0 in stage 0.0 (TID 63). 1924 bytes result sent to driver
[2023-05-28 03:13:14,605] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 64.0 in stage 0.0 (TID 64) (c1224b457a2d, executor driver, partition 64, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,606] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 63.0 in stage 0.0 (TID 63) in 6 ms on c1224b457a2d (executor driver) (61/70)
[2023-05-28 03:13:14,608] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 58.0 in stage 0.0 (TID 58). 1924 bytes result sent to driver
[2023-05-28 03:13:14,611] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 64.0 in stage 0.0 (TID 64)
[2023-05-28 03:13:14,611] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 55.0 in stage 0.0 (TID 55). 1924 bytes result sent to driver
[2023-05-28 03:13:14,612] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 62.0 in stage 0.0 (TID 62)
[2023-05-28 03:13:14,613] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 65.0 in stage 0.0 (TID 65) (c1224b457a2d, executor driver, partition 65, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,614] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 55.0 in stage 0.0 (TID 55) in 58 ms on c1224b457a2d (executor driver) (62/70)
[2023-05-28 03:13:14,617] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 64.0 in stage 0.0 (TID 64). 1924 bytes result sent to driver
[2023-05-28 03:13:14,623] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 65.0 in stage 0.0 (TID 65)
[2023-05-28 03:13:14,624] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 62.0 in stage 0.0 (TID 62). 1924 bytes result sent to driver
[2023-05-28 03:13:14,624] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 66.0 in stage 0.0 (TID 66) (c1224b457a2d, executor driver, partition 66, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,625] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 67.0 in stage 0.0 (TID 67) (c1224b457a2d, executor driver, partition 67, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,625] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 58.0 in stage 0.0 (TID 58) in 53 ms on c1224b457a2d (executor driver) (63/70)
[2023-05-28 03:13:14,626] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 64.0 in stage 0.0 (TID 64) in 21 ms on c1224b457a2d (executor driver) (64/70)
[2023-05-28 03:13:14,626] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 67.0 in stage 0.0 (TID 67)
[2023-05-28 03:13:14,627] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 66.0 in stage 0.0 (TID 66)
[2023-05-28 03:13:14,627] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 68.0 in stage 0.0 (TID 68) (c1224b457a2d, executor driver, partition 68, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,627] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 62.0 in stage 0.0 (TID 62) in 30 ms on c1224b457a2d (executor driver) (65/70)
[2023-05-28 03:13:14,637] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 65.0 in stage 0.0 (TID 65). 1924 bytes result sent to driver
[2023-05-28 03:13:14,642] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Starting task 69.0 in stage 0.0 (TID 69) (c1224b457a2d, executor driver, partition 69, PROCESS_LOCAL, 7498 bytes)
[2023-05-28 03:13:14,642] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 65.0 in stage 0.0 (TID 65) in 28 ms on c1224b457a2d (executor driver) (66/70)
[2023-05-28 03:13:14,643] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 69.0 in stage 0.0 (TID 69)
[2023-05-28 03:13:14,643] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 66.0 in stage 0.0 (TID 66). 1967 bytes result sent to driver
[2023-05-28 03:13:14,644] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 67.0 in stage 0.0 (TID 67). 1924 bytes result sent to driver
[2023-05-28 03:13:14,644] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Running task 68.0 in stage 0.0 (TID 68)
[2023-05-28 03:13:14,649] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 67.0 in stage 0.0 (TID 67) in 24 ms on c1224b457a2d (executor driver) (67/70)
[2023-05-28 03:13:14,671] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 69.0 in stage 0.0 (TID 69). 2010 bytes result sent to driver
[2023-05-28 03:13:14,673] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 66.0 in stage 0.0 (TID 66) in 54 ms on c1224b457a2d (executor driver) (68/70)
[2023-05-28 03:13:14,676] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO Executor: Finished task 68.0 in stage 0.0 (TID 68). 1967 bytes result sent to driver
[2023-05-28 03:13:14,677] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 68.0 in stage 0.0 (TID 68) in 51 ms on c1224b457a2d (executor driver) (69/70)
[2023-05-28 03:13:14,679] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSetManager: Finished task 69.0 in stage 0.0 (TID 69) in 37 ms on c1224b457a2d (executor driver) (70/70)
[2023-05-28 03:13:14,679] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-05-28 03:13:14,682] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 1.232 s
[2023-05-28 03:13:14,684] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-28 03:13:14,685] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-05-28 03:13:14,687] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 1.291220 s
[2023-05-28 03:13:14,724] {spark_submit.py:490} INFO - 23/05/28 03:13:14 INFO InMemoryFileIndex: It took 1778 ms to list leaf files for 70 paths.
[2023-05-28 03:13:15,102] {spark_submit.py:490} INFO - 23/05/28 03:13:15 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-05-28 03:13:15,111] {spark_submit.py:490} INFO - 23/05/28 03:13:15 INFO DAGScheduler: Got job 1 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-28 03:13:15,111] {spark_submit.py:490} INFO - 23/05/28 03:13:15 INFO DAGScheduler: Final stage: ResultStage 1 (load at NativeMethodAccessorImpl.java:0)
[2023-05-28 03:13:15,112] {spark_submit.py:490} INFO - 23/05/28 03:13:15 INFO DAGScheduler: Parents of final stage: List()
[2023-05-28 03:13:15,113] {spark_submit.py:490} INFO - 23/05/28 03:13:15 INFO DAGScheduler: Missing parents: List()
[2023-05-28 03:13:15,115] {spark_submit.py:490} INFO - 23/05/28 03:13:15 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-28 03:13:15,122] {spark_submit.py:490} INFO - 23/05/28 03:13:15 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 103.2 KiB, free 434.2 MiB)
[2023-05-28 03:13:15,124] {spark_submit.py:490} INFO - 23/05/28 03:13:15 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 37.1 KiB, free 434.1 MiB)
[2023-05-28 03:13:15,124] {spark_submit.py:490} INFO - 23/05/28 03:13:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on c1224b457a2d:43525 (size: 37.1 KiB, free: 434.3 MiB)
[2023-05-28 03:13:15,126] {spark_submit.py:490} INFO - 23/05/28 03:13:15 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535
[2023-05-28 03:13:15,127] {spark_submit.py:490} INFO - 23/05/28 03:13:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-28 03:13:15,128] {spark_submit.py:490} INFO - 23/05/28 03:13:15 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-05-28 03:13:15,131] {spark_submit.py:490} INFO - 23/05/28 03:13:15 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 70) (c1224b457a2d, executor driver, partition 0, PROCESS_LOCAL, 7594 bytes)
[2023-05-28 03:13:15,132] {spark_submit.py:490} INFO - 23/05/28 03:13:15 INFO Executor: Running task 0.0 in stage 1.0 (TID 70)
[2023-05-28 03:13:15,778] {spark_submit.py:490} INFO - 23/05/28 03:13:15 INFO Executor: Finished task 0.0 in stage 1.0 (TID 70). 2072 bytes result sent to driver
[2023-05-28 03:13:15,782] {spark_submit.py:490} INFO - 23/05/28 03:13:15 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 70) in 651 ms on c1224b457a2d (executor driver) (1/1)
[2023-05-28 03:13:15,782] {spark_submit.py:490} INFO - 23/05/28 03:13:15 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-05-28 03:13:15,785] {spark_submit.py:490} INFO - 23/05/28 03:13:15 INFO DAGScheduler: ResultStage 1 (load at NativeMethodAccessorImpl.java:0) finished in 0.669 s
[2023-05-28 03:13:15,785] {spark_submit.py:490} INFO - 23/05/28 03:13:15 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-28 03:13:15,786] {spark_submit.py:490} INFO - 23/05/28 03:13:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-05-28 03:13:15,786] {spark_submit.py:490} INFO - 23/05/28 03:13:15 INFO DAGScheduler: Job 1 finished: load at NativeMethodAccessorImpl.java:0, took 0.681675 s
[2023-05-28 03:13:16,245] {spark_submit.py:490} INFO - 23/05/28 03:13:16 INFO BlockManagerInfo: Removed broadcast_1_piece0 on c1224b457a2d:43525 in memory (size: 37.1 KiB, free: 434.4 MiB)
[2023-05-28 03:13:16,275] {spark_submit.py:490} INFO - 23/05/28 03:13:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on c1224b457a2d:43525 in memory (size: 36.9 KiB, free: 434.4 MiB)
[2023-05-28 03:13:16,964] {spark_submit.py:490} INFO - /usr/local/spark/staging/20230528/feature_engineering/etfs.parquet
[2023-05-28 03:13:17,323] {spark_submit.py:490} INFO - 23/05/28 03:13:17 INFO FileSourceStrategy: Pushed Filters:
[2023-05-28 03:13:17,325] {spark_submit.py:490} INFO - 23/05/28 03:13:17 INFO FileSourceStrategy: Post-Scan Filters:
[2023-05-28 03:13:17,710] {spark_submit.py:490} INFO - 23/05/28 03:13:17 INFO CodeGenerator: Code generated in 176.212167 ms
[2023-05-28 03:13:17,721] {spark_submit.py:490} INFO - 23/05/28 03:13:17 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-05-28 03:13:17,726] {spark_submit.py:490} INFO - 23/05/28 03:13:17 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 434.2 MiB)
[2023-05-28 03:13:17,727] {spark_submit.py:490} INFO - 23/05/28 03:13:17 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on c1224b457a2d:43525 (size: 35.0 KiB, free: 434.4 MiB)
[2023-05-28 03:13:17,730] {spark_submit.py:490} INFO - 23/05/28 03:13:17 INFO SparkContext: Created broadcast 2 from parquet at NativeMethodAccessorImpl.java:0
[2023-05-28 03:13:17,742] {spark_submit.py:490} INFO - 23/05/28 03:13:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 102726416 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-28 03:13:17,795] {spark_submit.py:490} INFO - 23/05/28 03:13:17 INFO DAGScheduler: Registering RDD 8 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2023-05-28 03:13:17,796] {spark_submit.py:490} INFO - 23/05/28 03:13:17 INFO DAGScheduler: Got map stage job 2 (parquet at NativeMethodAccessorImpl.java:0) with 4 output partitions
[2023-05-28 03:13:17,797] {spark_submit.py:490} INFO - 23/05/28 03:13:17 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (parquet at NativeMethodAccessorImpl.java:0)
[2023-05-28 03:13:17,797] {spark_submit.py:490} INFO - 23/05/28 03:13:17 INFO DAGScheduler: Parents of final stage: List()
[2023-05-28 03:13:17,798] {spark_submit.py:490} INFO - 23/05/28 03:13:17 INFO DAGScheduler: Missing parents: List()
[2023-05-28 03:13:17,801] {spark_submit.py:490} INFO - 23/05/28 03:13:17 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[8] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-28 03:13:17,881] {spark_submit.py:490} INFO - 23/05/28 03:13:17 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 21.8 KiB, free 434.1 MiB)
[2023-05-28 03:13:17,882] {spark_submit.py:490} INFO - 23/05/28 03:13:17 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 434.1 MiB)
[2023-05-28 03:13:17,882] {spark_submit.py:490} INFO - 23/05/28 03:13:17 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on c1224b457a2d:43525 (size: 8.6 KiB, free: 434.4 MiB)
[2023-05-28 03:13:17,883] {spark_submit.py:490} INFO - 23/05/28 03:13:17 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535
[2023-05-28 03:13:17,884] {spark_submit.py:490} INFO - 23/05/28 03:13:17 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[8] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
[2023-05-28 03:13:17,884] {spark_submit.py:490} INFO - 23/05/28 03:13:17 INFO TaskSchedulerImpl: Adding task set 2.0 with 4 tasks resource profile 0
[2023-05-28 03:13:17,896] {spark_submit.py:490} INFO - 23/05/28 03:13:17 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 71) (c1224b457a2d, executor driver, partition 0, PROCESS_LOCAL, 10261 bytes)
[2023-05-28 03:13:17,896] {spark_submit.py:490} INFO - 23/05/28 03:13:17 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 72) (c1224b457a2d, executor driver, partition 1, PROCESS_LOCAL, 10825 bytes)
[2023-05-28 03:13:17,897] {spark_submit.py:490} INFO - 23/05/28 03:13:17 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 73) (c1224b457a2d, executor driver, partition 2, PROCESS_LOCAL, 11389 bytes)
[2023-05-28 03:13:17,897] {spark_submit.py:490} INFO - 23/05/28 03:13:17 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 74) (c1224b457a2d, executor driver, partition 3, PROCESS_LOCAL, 11953 bytes)
[2023-05-28 03:13:17,897] {spark_submit.py:490} INFO - 23/05/28 03:13:17 INFO Executor: Running task 0.0 in stage 2.0 (TID 71)
[2023-05-28 03:13:17,898] {spark_submit.py:490} INFO - 23/05/28 03:13:17 INFO Executor: Running task 2.0 in stage 2.0 (TID 73)
[2023-05-28 03:13:17,900] {spark_submit.py:490} INFO - 23/05/28 03:13:17 INFO Executor: Running task 3.0 in stage 2.0 (TID 74)
[2023-05-28 03:13:17,901] {spark_submit.py:490} INFO - 23/05/28 03:13:17 INFO Executor: Running task 1.0 in stage 2.0 (TID 72)
[2023-05-28 03:13:17,981] {spark_submit.py:490} INFO - 23/05/28 03:13:17 INFO CodeGenerator: Code generated in 24.95 ms
[2023-05-28 03:13:17,989] {spark_submit.py:490} INFO - 23/05/28 03:13:17 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00013-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-2772726, partition values: [empty row]
[2023-05-28 03:13:17,990] {spark_submit.py:490} INFO - 23/05/28 03:13:17 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00048-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-873860, partition values: [empty row]
[2023-05-28 03:13:17,991] {spark_submit.py:490} INFO - 23/05/28 03:13:17 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00029-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-1615525, partition values: [empty row]
[2023-05-28 03:13:17,992] {spark_submit.py:490} INFO - 23/05/28 03:13:17 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00001-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-4835447, partition values: [empty row]
[2023-05-28 03:13:18,239] {spark_submit.py:490} INFO - 23/05/28 03:13:18 INFO CodecPool: Got brand-new decompressor [.lz4]
[2023-05-28 03:13:18,240] {spark_submit.py:490} INFO - 23/05/28 03:13:18 INFO CodecPool: Got brand-new decompressor [.lz4]
[2023-05-28 03:13:18,247] {spark_submit.py:490} INFO - 23/05/28 03:13:18 INFO CodecPool: Got brand-new decompressor [.lz4]
[2023-05-28 03:13:18,274] {spark_submit.py:490} INFO - 23/05/28 03:13:18 INFO CodecPool: Got brand-new decompressor [.lz4]
[2023-05-28 03:13:18,873] {spark_submit.py:490} INFO - 23/05/28 03:13:18 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00049-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-855197, partition values: [empty row]
[2023-05-28 03:13:19,077] {spark_submit.py:490} INFO - 23/05/28 03:13:19 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00050-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-781384, partition values: [empty row]
[2023-05-28 03:13:19,282] {spark_submit.py:490} INFO - 23/05/28 03:13:19 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00030-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-1613396, partition values: [empty row]
[2023-05-28 03:13:19,518] {spark_submit.py:490} INFO - 23/05/28 03:13:19 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00051-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-737422, partition values: [empty row]
[2023-05-28 03:13:19,589] {spark_submit.py:490} INFO - 23/05/28 03:13:19 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00014-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-2765417, partition values: [empty row]
[2023-05-28 03:13:19,839] {spark_submit.py:490} INFO - 23/05/28 03:13:19 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00054-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-642737, partition values: [empty row]
[2023-05-28 03:13:19,991] {spark_submit.py:490} INFO - 23/05/28 03:13:19 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00032-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-1540399, partition values: [empty row]
[2023-05-28 03:13:20,001] {spark_submit.py:490} INFO - 23/05/28 03:13:20 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00052-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-635005, partition values: [empty row]
[2023-05-28 03:13:20,253] {spark_submit.py:490} INFO - 23/05/28 03:13:20 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00056-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-615903, partition values: [empty row]
[2023-05-28 03:13:20,430] {spark_submit.py:490} INFO - 23/05/28 03:13:20 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00000-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-4816385, partition values: [empty row]
[2023-05-28 03:13:20,608] {spark_submit.py:490} INFO - 23/05/28 03:13:20 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00053-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-611998, partition values: [empty row]
[2023-05-28 03:13:20,697] {spark_submit.py:490} INFO - 23/05/28 03:13:20 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00031-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-1469975, partition values: [empty row]
[2023-05-28 03:13:20,740] {spark_submit.py:490} INFO - 23/05/28 03:13:20 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00055-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-608266, partition values: [empty row]
[2023-05-28 03:13:20,743] {spark_submit.py:490} INFO - 23/05/28 03:13:20 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00019-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-2524655, partition values: [empty row]
[2023-05-28 03:13:20,902] {spark_submit.py:490} INFO - 23/05/28 03:13:20 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00057-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-534861, partition values: [empty row]
[2023-05-28 03:13:21,100] {spark_submit.py:490} INFO - 23/05/28 03:13:21 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00061-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-432970, partition values: [empty row]
[2023-05-28 03:13:21,131] {spark_submit.py:490} INFO - 23/05/28 03:13:21 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00037-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-1467222, partition values: [empty row]
[2023-05-28 03:13:21,156] {spark_submit.py:490} INFO - 23/05/28 03:13:21 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00017-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-2514129, partition values: [empty row]
[2023-05-28 03:13:21,247] {spark_submit.py:490} INFO - 23/05/28 03:13:21 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00060-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-393355, partition values: [empty row]
[2023-05-28 03:13:21,464] {spark_submit.py:490} INFO - 23/05/28 03:13:21 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00036-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-1457226, partition values: [empty row]
[2023-05-28 03:13:21,490] {spark_submit.py:490} INFO - 23/05/28 03:13:21 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00062-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-367405, partition values: [empty row]
[2023-05-28 03:13:21,555] {spark_submit.py:490} INFO - 23/05/28 03:13:21 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00002-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-4457170, partition values: [empty row]
[2023-05-28 03:13:21,574] {spark_submit.py:490} INFO - 23/05/28 03:13:21 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00058-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-363769, partition values: [empty row]
[2023-05-28 03:13:21,665] {spark_submit.py:490} INFO - 23/05/28 03:13:21 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00059-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-354206, partition values: [empty row]
[2023-05-28 03:13:21,837] {spark_submit.py:490} INFO - 23/05/28 03:13:21 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00063-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-298098, partition values: [empty row]
[2023-05-28 03:13:21,841] {spark_submit.py:490} INFO - 23/05/28 03:13:21 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00033-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-1393139, partition values: [empty row]
[2023-05-28 03:13:21,866] {spark_submit.py:490} INFO - 23/05/28 03:13:21 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00016-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-2461782, partition values: [empty row]
[2023-05-28 03:13:21,951] {spark_submit.py:490} INFO - 23/05/28 03:13:21 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00064-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-260866, partition values: [empty row]
[2023-05-28 03:13:22,009] {spark_submit.py:490} INFO - 23/05/28 03:13:22 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00065-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-193408, partition values: [empty row]
[2023-05-28 03:13:22,191] {spark_submit.py:490} INFO - 23/05/28 03:13:22 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00066-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-186692, partition values: [empty row]
[2023-05-28 03:13:22,262] {spark_submit.py:490} INFO - 23/05/28 03:13:22 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00003-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-4118260, partition values: [empty row]
[2023-05-28 03:13:22,263] {spark_submit.py:490} INFO - 23/05/28 03:13:22 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00067-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-153512, partition values: [empty row]
[2023-05-28 03:13:22,278] {spark_submit.py:490} INFO - 23/05/28 03:13:22 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00035-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-1359884, partition values: [empty row]
[2023-05-28 03:13:22,312] {spark_submit.py:490} INFO - 23/05/28 03:13:22 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00068-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-106971, partition values: [empty row]
[2023-05-28 03:13:22,406] {spark_submit.py:490} INFO - 23/05/28 03:13:22 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00069-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-43395, partition values: [empty row]
[2023-05-28 03:13:22,773] {spark_submit.py:490} INFO - 23/05/28 03:13:22 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00038-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-1333865, partition values: [empty row]
[2023-05-28 03:13:22,956] {spark_submit.py:490} INFO - 23/05/28 03:13:22 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00021-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-2433989, partition values: [empty row]
[2023-05-28 03:13:23,532] {spark_submit.py:490} INFO - 23/05/28 03:13:23 INFO Executor: Finished task 3.0 in stage 2.0 (TID 74). 2283 bytes result sent to driver
[2023-05-28 03:13:23,537] {spark_submit.py:490} INFO - 23/05/28 03:13:23 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 74) in 5640 ms on c1224b457a2d (executor driver) (1/4)
[2023-05-28 03:13:23,546] {spark_submit.py:490} INFO - 23/05/28 03:13:23 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00034-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-1306468, partition values: [empty row]
[2023-05-28 03:13:23,556] {spark_submit.py:490} INFO - 23/05/28 03:13:23 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00004-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-3614584, partition values: [empty row]
[2023-05-28 03:13:23,808] {spark_submit.py:490} INFO - 23/05/28 03:13:23 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00015-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-2425257, partition values: [empty row]
[2023-05-28 03:13:23,913] {spark_submit.py:490} INFO - 23/05/28 03:13:23 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00041-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-1241935, partition values: [empty row]
[2023-05-28 03:13:24,103] {spark_submit.py:490} INFO - 23/05/28 03:13:24 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00039-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-1227976, partition values: [empty row]
[2023-05-28 03:13:24,160] {spark_submit.py:490} INFO - 23/05/28 03:13:24 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00007-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-3479701, partition values: [empty row]
[2023-05-28 03:13:24,259] {spark_submit.py:490} INFO - 23/05/28 03:13:24 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00018-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-2423946, partition values: [empty row]
[2023-05-28 03:13:24,261] {spark_submit.py:490} INFO - 23/05/28 03:13:24 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00042-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-1199596, partition values: [empty row]
[2023-05-28 03:13:24,502] {spark_submit.py:490} INFO - 23/05/28 03:13:24 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00040-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-1199277, partition values: [empty row]
[2023-05-28 03:13:24,692] {spark_submit.py:490} INFO - 23/05/28 03:13:24 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00022-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-2241678, partition values: [empty row]
[2023-05-28 03:13:24,820] {spark_submit.py:490} INFO - 23/05/28 03:13:24 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00044-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-1166841, partition values: [empty row]
[2023-05-28 03:13:24,880] {spark_submit.py:490} INFO - 23/05/28 03:13:24 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00024-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-2131411, partition values: [empty row]
[2023-05-28 03:13:24,896] {spark_submit.py:490} INFO - 23/05/28 03:13:24 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00005-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-3379191, partition values: [empty row]
[2023-05-28 03:13:25,018] {spark_submit.py:490} INFO - 23/05/28 03:13:25 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00043-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-1136079, partition values: [empty row]
[2023-05-28 03:13:25,151] {spark_submit.py:490} INFO - 23/05/28 03:13:25 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00020-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-2086362, partition values: [empty row]
[2023-05-28 03:13:25,170] {spark_submit.py:490} INFO - 23/05/28 03:13:25 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00045-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-1010318, partition values: [empty row]
[2023-05-28 03:13:25,394] {spark_submit.py:490} INFO - 23/05/28 03:13:25 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00008-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-3095244, partition values: [empty row]
[2023-05-28 03:13:25,413] {spark_submit.py:490} INFO - 23/05/28 03:13:25 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00047-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-983082, partition values: [empty row]
[2023-05-28 03:13:25,593] {spark_submit.py:490} INFO - 23/05/28 03:13:25 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00023-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-2078514, partition values: [empty row]
[2023-05-28 03:13:25,594] {spark_submit.py:490} INFO - 23/05/28 03:13:25 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00046-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-933837, partition values: [empty row]
[2023-05-28 03:13:26,020] {spark_submit.py:490} INFO - 23/05/28 03:13:26 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00025-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-1951804, partition values: [empty row]
[2023-05-28 03:13:26,187] {spark_submit.py:490} INFO - 23/05/28 03:13:26 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00006-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-3000110, partition values: [empty row]
[2023-05-28 03:13:26,451] {spark_submit.py:490} INFO - 23/05/28 03:13:26 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00027-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-1856905, partition values: [empty row]
[2023-05-28 03:13:26,828] {spark_submit.py:490} INFO - 23/05/28 03:13:26 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00026-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-1812098, partition values: [empty row]
[2023-05-28 03:13:26,904] {spark_submit.py:490} INFO - 23/05/28 03:13:26 INFO Executor: Finished task 2.0 in stage 2.0 (TID 73). 2240 bytes result sent to driver
[2023-05-28 03:13:26,906] {spark_submit.py:490} INFO - 23/05/28 03:13:26 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 73) in 9011 ms on c1224b457a2d (executor driver) (2/4)
[2023-05-28 03:13:26,991] {spark_submit.py:490} INFO - 23/05/28 03:13:26 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00009-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-2999323, partition values: [empty row]
[2023-05-28 03:13:27,063] {spark_submit.py:490} INFO - 23/05/28 03:13:27 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00028-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-1649880, partition values: [empty row]
[2023-05-28 03:13:27,438] {spark_submit.py:490} INFO - 23/05/28 03:13:27 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00011-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-2943800, partition values: [empty row]
[2023-05-28 03:13:27,827] {spark_submit.py:490} INFO - 23/05/28 03:13:27 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00012-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-2872410, partition values: [empty row]
[2023-05-28 03:13:28,167] {spark_submit.py:490} INFO - 23/05/28 03:13:28 INFO FileScanRDD: Reading File path: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00010-2cebc90e-94b3-4178-b05c-e3517b545667-c000.lz4.parquet, range: 0-2854887, partition values: [empty row]
[2023-05-28 03:13:28,269] {spark_submit.py:490} INFO - 23/05/28 03:13:28 INFO Executor: Finished task 1.0 in stage 2.0 (TID 72). 2240 bytes result sent to driver
[2023-05-28 03:13:28,277] {spark_submit.py:490} INFO - 23/05/28 03:13:28 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 72) in 10376 ms on c1224b457a2d (executor driver) (3/4)
[2023-05-28 03:13:29,183] {spark_submit.py:490} INFO - 23/05/28 03:13:29 INFO Executor: Finished task 0.0 in stage 2.0 (TID 71). 2240 bytes result sent to driver
[2023-05-28 03:13:29,186] {spark_submit.py:490} INFO - 23/05/28 03:13:29 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 71) in 11302 ms on c1224b457a2d (executor driver) (4/4)
[2023-05-28 03:13:29,188] {spark_submit.py:490} INFO - 23/05/28 03:13:29 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-05-28 03:13:29,193] {spark_submit.py:490} INFO - 23/05/28 03:13:29 INFO DAGScheduler: ShuffleMapStage 2 (parquet at NativeMethodAccessorImpl.java:0) finished in 11.387 s
[2023-05-28 03:13:29,197] {spark_submit.py:490} INFO - 23/05/28 03:13:29 INFO DAGScheduler: looking for newly runnable stages
[2023-05-28 03:13:29,199] {spark_submit.py:490} INFO - 23/05/28 03:13:29 INFO DAGScheduler: running: Set()
[2023-05-28 03:13:29,199] {spark_submit.py:490} INFO - 23/05/28 03:13:29 INFO DAGScheduler: waiting: Set()
[2023-05-28 03:13:29,200] {spark_submit.py:490} INFO - 23/05/28 03:13:29 INFO DAGScheduler: failed: Set()
[2023-05-28 03:13:29,251] {spark_submit.py:490} INFO - 23/05/28 03:13:29 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 67108864, minimum partition size: 1048576
[2023-05-28 03:13:29,408] {spark_submit.py:490} INFO - 23/05/28 03:13:29 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-05-28 03:13:29,450] {spark_submit.py:490} INFO - 23/05/28 03:13:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-28 03:13:29,453] {spark_submit.py:490} INFO - 23/05/28 03:13:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-28 03:13:29,455] {spark_submit.py:490} INFO - 23/05/28 03:13:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-05-28 03:13:29,458] {spark_submit.py:490} INFO - 23/05/28 03:13:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-28 03:13:29,459] {spark_submit.py:490} INFO - 23/05/28 03:13:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-28 03:13:29,461] {spark_submit.py:490} INFO - 23/05/28 03:13:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-05-28 03:13:29,641] {spark_submit.py:490} INFO - 23/05/28 03:13:29 INFO CodeGenerator: Code generated in 43.573333 ms
[2023-05-28 03:13:29,699] {spark_submit.py:490} INFO - 23/05/28 03:13:29 INFO CodeGenerator: Code generated in 37.308583 ms
[2023-05-28 03:13:29,733] {spark_submit.py:490} INFO - 23/05/28 03:13:29 INFO CodeGenerator: Code generated in 18.128458 ms
[2023-05-28 03:13:29,967] {spark_submit.py:490} INFO - 23/05/28 03:13:29 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2023-05-28 03:13:29,968] {spark_submit.py:490} INFO - 23/05/28 03:13:29 INFO DAGScheduler: Got job 3 (parquet at NativeMethodAccessorImpl.java:0) with 5 output partitions
[2023-05-28 03:13:29,968] {spark_submit.py:490} INFO - 23/05/28 03:13:29 INFO DAGScheduler: Final stage: ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0)
[2023-05-28 03:13:29,969] {spark_submit.py:490} INFO - 23/05/28 03:13:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
[2023-05-28 03:13:29,971] {spark_submit.py:490} INFO - 23/05/28 03:13:29 INFO DAGScheduler: Missing parents: List()
[2023-05-28 03:13:29,972] {spark_submit.py:490} INFO - 23/05/28 03:13:29 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[15] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-28 03:13:30,031] {spark_submit.py:490} INFO - 23/05/28 03:13:30 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 247.7 KiB, free 433.9 MiB)
[2023-05-28 03:13:30,034] {spark_submit.py:490} INFO - 23/05/28 03:13:30 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 90.0 KiB, free 433.8 MiB)
[2023-05-28 03:13:30,035] {spark_submit.py:490} INFO - 23/05/28 03:13:30 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on c1224b457a2d:43525 (size: 90.0 KiB, free: 434.3 MiB)
[2023-05-28 03:13:30,036] {spark_submit.py:490} INFO - 23/05/28 03:13:30 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1535
[2023-05-28 03:13:30,037] {spark_submit.py:490} INFO - 23/05/28 03:13:30 INFO DAGScheduler: Submitting 5 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
[2023-05-28 03:13:30,037] {spark_submit.py:490} INFO - 23/05/28 03:13:30 INFO TaskSchedulerImpl: Adding task set 4.0 with 5 tasks resource profile 0
[2023-05-28 03:13:30,069] {spark_submit.py:490} INFO - 23/05/28 03:13:30 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 75) (c1224b457a2d, executor driver, partition 0, NODE_LOCAL, 7363 bytes)
[2023-05-28 03:13:30,076] {spark_submit.py:490} INFO - 23/05/28 03:13:30 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 76) (c1224b457a2d, executor driver, partition 1, NODE_LOCAL, 7363 bytes)
[2023-05-28 03:13:30,080] {spark_submit.py:490} INFO - 23/05/28 03:13:30 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 77) (c1224b457a2d, executor driver, partition 2, NODE_LOCAL, 7363 bytes)
[2023-05-28 03:13:30,080] {spark_submit.py:490} INFO - 23/05/28 03:13:30 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 78) (c1224b457a2d, executor driver, partition 3, NODE_LOCAL, 7363 bytes)
[2023-05-28 03:13:30,091] {spark_submit.py:490} INFO - 23/05/28 03:13:30 INFO Executor: Running task 1.0 in stage 4.0 (TID 76)
[2023-05-28 03:13:30,091] {spark_submit.py:490} INFO - 23/05/28 03:13:30 INFO Executor: Running task 0.0 in stage 4.0 (TID 75)
[2023-05-28 03:13:30,095] {spark_submit.py:490} INFO - 23/05/28 03:13:30 INFO Executor: Running task 2.0 in stage 4.0 (TID 77)
[2023-05-28 03:13:30,105] {spark_submit.py:490} INFO - 23/05/28 03:13:30 INFO Executor: Running task 3.0 in stage 4.0 (TID 78)
[2023-05-28 03:13:30,199] {spark_submit.py:490} INFO - 23/05/28 03:13:30 INFO ShuffleBlockFetcherIterator: Getting 4 (63.0 MiB) non-empty blocks including 4 (63.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-05-28 03:13:30,200] {spark_submit.py:490} INFO - 23/05/28 03:13:30 INFO ShuffleBlockFetcherIterator: Getting 4 (62.7 MiB) non-empty blocks including 4 (62.7 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-05-28 03:13:30,201] {spark_submit.py:490} INFO - 23/05/28 03:13:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
[2023-05-28 03:13:30,203] {spark_submit.py:490} INFO - 23/05/28 03:13:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
[2023-05-28 03:13:30,204] {spark_submit.py:490} INFO - 23/05/28 03:13:30 INFO ShuffleBlockFetcherIterator: Getting 4 (63.2 MiB) non-empty blocks including 4 (63.2 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-05-28 03:13:30,205] {spark_submit.py:490} INFO - 23/05/28 03:13:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms
[2023-05-28 03:13:30,222] {spark_submit.py:490} INFO - 23/05/28 03:13:30 INFO ShuffleBlockFetcherIterator: Getting 4 (62.6 MiB) non-empty blocks including 4 (62.6 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-05-28 03:13:30,224] {spark_submit.py:490} INFO - 23/05/28 03:13:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 32 ms
[2023-05-28 03:13:30,236] {spark_submit.py:490} INFO - 23/05/28 03:13:30 INFO CodeGenerator: Code generated in 15.700667 ms
[2023-05-28 03:13:30,268] {spark_submit.py:490} INFO - 23/05/28 03:13:30 INFO CodeGenerator: Code generated in 16.991209 ms
[2023-05-28 03:13:30,333] {spark_submit.py:490} INFO - 23/05/28 03:13:30 INFO CodeGenerator: Code generated in 24.177 ms
[2023-05-28 03:13:30,373] {spark_submit.py:490} INFO - 23/05/28 03:13:30 INFO CodeGenerator: Code generated in 27.480166 ms
[2023-05-28 03:13:30,733] {spark_submit.py:490} INFO - 23/05/28 03:13:30 INFO BlockManagerInfo: Removed broadcast_3_piece0 on c1224b457a2d:43525 in memory (size: 8.6 KiB, free: 434.3 MiB)
[2023-05-28 03:13:30,929] {spark_submit.py:490} INFO - 23/05/28 03:13:30 INFO UnsafeExternalSorter: Thread 63 spilling sort data of 100.0 MiB to disk (0  time so far)
[2023-05-28 03:13:30,948] {spark_submit.py:490} INFO - 23/05/28 03:13:30 INFO UnsafeExternalSorter: Thread 62 spilling sort data of 100.0 MiB to disk (0  time so far)
[2023-05-28 03:13:31,036] {spark_submit.py:490} INFO - 23/05/28 03:13:31 INFO UnsafeExternalSorter: Thread 70 spilling sort data of 100.0 MiB to disk (0  time so far)
[2023-05-28 03:13:31,049] {spark_submit.py:490} INFO - 23/05/28 03:13:31 INFO UnsafeExternalSorter: Thread 72 spilling sort data of 100.0 MiB to disk (0  time so far)
[2023-05-28 03:13:31,882] {spark_submit.py:490} INFO - 23/05/28 03:13:31 INFO UnsafeExternalSorter: Thread 72 spilling sort data of 104.0 MiB to disk (1  time so far)
[2023-05-28 03:13:31,977] {spark_submit.py:490} INFO - 23/05/28 03:13:31 INFO UnsafeExternalSorter: Thread 63 spilling sort data of 104.0 MiB to disk (1  time so far)
[2023-05-28 03:13:32,011] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO UnsafeExternalSorter: Thread 62 spilling sort data of 104.0 MiB to disk (1  time so far)
[2023-05-28 03:13:32,053] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO UnsafeExternalSorter: Thread 70 spilling sort data of 104.0 MiB to disk (1  time so far)
[2023-05-28 03:13:32,561] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO CodeGenerator: Code generated in 27.44875 ms
[2023-05-28 03:13:32,619] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO CodeGenerator: Code generated in 39.965875 ms
[2023-05-28 03:13:32,634] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO CodeGenerator: Code generated in 8.471958 ms
[2023-05-28 03:13:32,644] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO CodeGenerator: Code generated in 6.481625 ms
[2023-05-28 03:13:32,665] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO CodeGenerator: Code generated in 8.41825 ms
[2023-05-28 03:13:32,696] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO CodeGenerator: Code generated in 5.012334 ms
[2023-05-28 03:13:32,707] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO CodeGenerator: Code generated in 9.901208 ms
[2023-05-28 03:13:32,742] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-28 03:13:32,747] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-28 03:13:32,748] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-05-28 03:13:32,750] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-28 03:13:32,751] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-28 03:13:32,752] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-05-28 03:13:32,752] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-28 03:13:32,753] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-28 03:13:32,753] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-05-28 03:13:32,753] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-28 03:13:32,753] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-28 03:13:32,754] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-05-28 03:13:32,754] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-28 03:13:32,755] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-28 03:13:32,755] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-05-28 03:13:32,755] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-28 03:13:32,755] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-28 03:13:32,756] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-05-28 03:13:32,759] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-28 03:13:32,760] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-28 03:13:32,760] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-05-28 03:13:32,761] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-28 03:13:32,761] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-28 03:13:32,762] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-05-28 03:13:32,769] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO CodecConfig: Compression: LZ4
[2023-05-28 03:13:32,772] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO CodecConfig: Compression: LZ4
[2023-05-28 03:13:32,820] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO CodecConfig: Compression: LZ4
[2023-05-28 03:13:32,821] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO CodecConfig: Compression: LZ4
[2023-05-28 03:13:32,821] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO CodecConfig: Compression: LZ4
[2023-05-28 03:13:32,821] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO CodecConfig: Compression: LZ4
[2023-05-28 03:13:32,822] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO CodecConfig: Compression: LZ4
[2023-05-28 03:13:32,822] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO CodecConfig: Compression: LZ4
[2023-05-28 03:13:32,824] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2023-05-28 03:13:32,825] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2023-05-28 03:13:32,825] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2023-05-28 03:13:32,825] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2023-05-28 03:13:32,830] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-05-28 03:13:32,830] {spark_submit.py:490} INFO - {
[2023-05-28 03:13:32,831] {spark_submit.py:490} INFO - "type" : "struct",
[2023-05-28 03:13:32,831] {spark_submit.py:490} INFO - "fields" : [ {
[2023-05-28 03:13:32,831] {spark_submit.py:490} INFO - "name" : "Symbol",
[2023-05-28 03:13:32,832] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:13:32,832] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,832] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,833] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,833] {spark_submit.py:490} INFO - "name" : "Security Name",
[2023-05-28 03:13:32,833] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:13:32,834] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,834] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,836] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,837] {spark_submit.py:490} INFO - "name" : "Date",
[2023-05-28 03:13:32,837] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:13:32,837] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,838] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,838] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,838] {spark_submit.py:490} INFO - "name" : "Open",
[2023-05-28 03:13:32,841] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:13:32,841] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,842] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,842] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,843] {spark_submit.py:490} INFO - "name" : "High",
[2023-05-28 03:13:32,843] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:13:32,843] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,843] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,844] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,844] {spark_submit.py:490} INFO - "name" : "Low",
[2023-05-28 03:13:32,844] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:13:32,845] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,845] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,846] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,847] {spark_submit.py:490} INFO - "name" : "Close",
[2023-05-28 03:13:32,848] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:13:32,848] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,848] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,849] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,850] {spark_submit.py:490} INFO - "name" : "Adj Close",
[2023-05-28 03:13:32,850] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:13:32,851] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,852] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,853] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,853] {spark_submit.py:490} INFO - "name" : "Volume",
[2023-05-28 03:13:32,854] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:13:32,858] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,859] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,860] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,860] {spark_submit.py:490} INFO - "name" : "vol_moving_avg",
[2023-05-28 03:13:32,861] {spark_submit.py:490} INFO - "type" : "double",
[2023-05-28 03:13:32,861] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,862] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,863] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,864] {spark_submit.py:490} INFO - "name" : "adj_close_rolling_med",
[2023-05-28 03:13:32,867] {spark_submit.py:490} INFO - "type" : "double",
[2023-05-28 03:13:32,868] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,869] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,870] {spark_submit.py:490} INFO - } ]
[2023-05-28 03:13:32,871] {spark_submit.py:490} INFO - }
[2023-05-28 03:13:32,871] {spark_submit.py:490} INFO - and corresponding Parquet message type:
[2023-05-28 03:13:32,871] {spark_submit.py:490} INFO - message spark_schema {
[2023-05-28 03:13:32,874] {spark_submit.py:490} INFO - optional binary Symbol (STRING);
[2023-05-28 03:13:32,874] {spark_submit.py:490} INFO - optional binary Security Name (STRING);
[2023-05-28 03:13:32,875] {spark_submit.py:490} INFO - optional binary Date (STRING);
[2023-05-28 03:13:32,875] {spark_submit.py:490} INFO - optional binary Open (STRING);
[2023-05-28 03:13:32,876] {spark_submit.py:490} INFO - optional binary High (STRING);
[2023-05-28 03:13:32,876] {spark_submit.py:490} INFO - optional binary Low (STRING);
[2023-05-28 03:13:32,883] {spark_submit.py:490} INFO - optional binary Close (STRING);
[2023-05-28 03:13:32,883] {spark_submit.py:490} INFO - optional binary Adj Close (STRING);
[2023-05-28 03:13:32,884] {spark_submit.py:490} INFO - optional binary Volume (STRING);
[2023-05-28 03:13:32,884] {spark_submit.py:490} INFO - optional double vol_moving_avg;
[2023-05-28 03:13:32,885] {spark_submit.py:490} INFO - optional double adj_close_rolling_med;
[2023-05-28 03:13:32,885] {spark_submit.py:490} INFO - }
[2023-05-28 03:13:32,885] {spark_submit.py:490} INFO - 
[2023-05-28 03:13:32,886] {spark_submit.py:490} INFO - 
[2023-05-28 03:13:32,887] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-05-28 03:13:32,888] {spark_submit.py:490} INFO - {
[2023-05-28 03:13:32,888] {spark_submit.py:490} INFO - "type" : "struct",
[2023-05-28 03:13:32,889] {spark_submit.py:490} INFO - "fields" : [ {
[2023-05-28 03:13:32,890] {spark_submit.py:490} INFO - "name" : "Symbol",
[2023-05-28 03:13:32,890] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:13:32,890] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,891] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,891] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,892] {spark_submit.py:490} INFO - "name" : "Security Name",
[2023-05-28 03:13:32,892] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:13:32,896] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,900] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,903] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,904] {spark_submit.py:490} INFO - "name" : "Date",
[2023-05-28 03:13:32,905] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:13:32,905] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,905] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,906] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,906] {spark_submit.py:490} INFO - "name" : "Open",
[2023-05-28 03:13:32,907] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:13:32,907] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,908] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,909] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,909] {spark_submit.py:490} INFO - "name" : "High",
[2023-05-28 03:13:32,909] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:13:32,909] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,910] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,910] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,911] {spark_submit.py:490} INFO - "name" : "Low",
[2023-05-28 03:13:32,911] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:13:32,911] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,912] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,912] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,913] {spark_submit.py:490} INFO - "name" : "Close",
[2023-05-28 03:13:32,913] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:13:32,914] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,914] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,915] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,915] {spark_submit.py:490} INFO - "name" : "Adj Close",
[2023-05-28 03:13:32,915] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:13:32,916] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,916] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,916] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,917] {spark_submit.py:490} INFO - "name" : "Volume",
[2023-05-28 03:13:32,917] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:13:32,917] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,918] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,918] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,919] {spark_submit.py:490} INFO - "name" : "vol_moving_avg",
[2023-05-28 03:13:32,919] {spark_submit.py:490} INFO - "type" : "double",
[2023-05-28 03:13:32,920] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,920] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,920] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,921] {spark_submit.py:490} INFO - "name" : "adj_close_rolling_med",
[2023-05-28 03:13:32,922] {spark_submit.py:490} INFO - "type" : "double",
[2023-05-28 03:13:32,923] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,923] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,923] {spark_submit.py:490} INFO - } ]
[2023-05-28 03:13:32,924] {spark_submit.py:490} INFO - }
[2023-05-28 03:13:32,924] {spark_submit.py:490} INFO - and corresponding Parquet message type:
[2023-05-28 03:13:32,925] {spark_submit.py:490} INFO - message spark_schema {
[2023-05-28 03:13:32,925] {spark_submit.py:490} INFO - optional binary Symbol (STRING);
[2023-05-28 03:13:32,926] {spark_submit.py:490} INFO - optional binary Security Name (STRING);
[2023-05-28 03:13:32,926] {spark_submit.py:490} INFO - optional binary Date (STRING);
[2023-05-28 03:13:32,927] {spark_submit.py:490} INFO - optional binary Open (STRING);
[2023-05-28 03:13:32,927] {spark_submit.py:490} INFO - optional binary High (STRING);
[2023-05-28 03:13:32,928] {spark_submit.py:490} INFO - optional binary Low (STRING);
[2023-05-28 03:13:32,928] {spark_submit.py:490} INFO - optional binary Close (STRING);
[2023-05-28 03:13:32,929] {spark_submit.py:490} INFO - optional binary Adj Close (STRING);
[2023-05-28 03:13:32,929] {spark_submit.py:490} INFO - optional binary Volume (STRING);
[2023-05-28 03:13:32,930] {spark_submit.py:490} INFO - optional double vol_moving_avg;
[2023-05-28 03:13:32,930] {spark_submit.py:490} INFO - optional double adj_close_rolling_med;
[2023-05-28 03:13:32,930] {spark_submit.py:490} INFO - }
[2023-05-28 03:13:32,930] {spark_submit.py:490} INFO - 
[2023-05-28 03:13:32,931] {spark_submit.py:490} INFO - 
[2023-05-28 03:13:32,931] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-05-28 03:13:32,931] {spark_submit.py:490} INFO - {
[2023-05-28 03:13:32,932] {spark_submit.py:490} INFO - "type" : "struct",
[2023-05-28 03:13:32,932] {spark_submit.py:490} INFO - "fields" : [ {
[2023-05-28 03:13:32,932] {spark_submit.py:490} INFO - "name" : "Symbol",
[2023-05-28 03:13:32,933] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:13:32,933] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,933] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,934] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,934] {spark_submit.py:490} INFO - "name" : "Security Name",
[2023-05-28 03:13:32,934] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:13:32,935] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,935] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,935] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,936] {spark_submit.py:490} INFO - "name" : "Date",
[2023-05-28 03:13:32,936] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:13:32,936] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,936] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,936] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,937] {spark_submit.py:490} INFO - "name" : "Open",
[2023-05-28 03:13:32,937] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:13:32,937] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,937] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,938] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,938] {spark_submit.py:490} INFO - "name" : "High",
[2023-05-28 03:13:32,938] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:13:32,939] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,939] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,939] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,939] {spark_submit.py:490} INFO - "name" : "Low",
[2023-05-28 03:13:32,941] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:13:32,941] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,941] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,942] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,942] {spark_submit.py:490} INFO - "name" : "Close",
[2023-05-28 03:13:32,943] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:13:32,943] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,944] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,944] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,944] {spark_submit.py:490} INFO - "name" : "Adj Close",
[2023-05-28 03:13:32,944] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:13:32,945] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,945] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,946] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,946] {spark_submit.py:490} INFO - "name" : "Volume",
[2023-05-28 03:13:32,946] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:13:32,947] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,947] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,947] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,947] {spark_submit.py:490} INFO - "name" : "vol_moving_avg",
[2023-05-28 03:13:32,948] {spark_submit.py:490} INFO - "type" : "double",
[2023-05-28 03:13:32,948] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,948] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,949] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,949] {spark_submit.py:490} INFO - "name" : "adj_close_rolling_med",
[2023-05-28 03:13:32,949] {spark_submit.py:490} INFO - "type" : "double",
[2023-05-28 03:13:32,950] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,950] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,950] {spark_submit.py:490} INFO - } ]
[2023-05-28 03:13:32,950] {spark_submit.py:490} INFO - }
[2023-05-28 03:13:32,951] {spark_submit.py:490} INFO - and corresponding Parquet message type:
[2023-05-28 03:13:32,951] {spark_submit.py:490} INFO - message spark_schema {
[2023-05-28 03:13:32,952] {spark_submit.py:490} INFO - optional binary Symbol (STRING);
[2023-05-28 03:13:32,952] {spark_submit.py:490} INFO - optional binary Security Name (STRING);
[2023-05-28 03:13:32,953] {spark_submit.py:490} INFO - optional binary Date (STRING);
[2023-05-28 03:13:32,953] {spark_submit.py:490} INFO - optional binary Open (STRING);
[2023-05-28 03:13:32,957] {spark_submit.py:490} INFO - optional binary High (STRING);
[2023-05-28 03:13:32,959] {spark_submit.py:490} INFO - optional binary Low (STRING);
[2023-05-28 03:13:32,959] {spark_submit.py:490} INFO - optional binary Close (STRING);
[2023-05-28 03:13:32,960] {spark_submit.py:490} INFO - optional binary Adj Close (STRING);
[2023-05-28 03:13:32,961] {spark_submit.py:490} INFO - optional binary Volume (STRING);
[2023-05-28 03:13:32,961] {spark_submit.py:490} INFO - optional double vol_moving_avg;
[2023-05-28 03:13:32,961] {spark_submit.py:490} INFO - optional double adj_close_rolling_med;
[2023-05-28 03:13:32,962] {spark_submit.py:490} INFO - }
[2023-05-28 03:13:32,962] {spark_submit.py:490} INFO - 
[2023-05-28 03:13:32,962] {spark_submit.py:490} INFO - 
[2023-05-28 03:13:32,962] {spark_submit.py:490} INFO - 23/05/28 03:13:32 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-05-28 03:13:32,963] {spark_submit.py:490} INFO - {
[2023-05-28 03:13:32,963] {spark_submit.py:490} INFO - "type" : "struct",
[2023-05-28 03:13:32,963] {spark_submit.py:490} INFO - "fields" : [ {
[2023-05-28 03:13:32,963] {spark_submit.py:490} INFO - "name" : "Symbol",
[2023-05-28 03:13:32,964] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:13:32,964] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,964] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,964] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,965] {spark_submit.py:490} INFO - "name" : "Security Name",
[2023-05-28 03:13:32,965] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:13:32,966] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,966] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,967] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,967] {spark_submit.py:490} INFO - "name" : "Date",
[2023-05-28 03:13:32,968] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:13:32,968] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,969] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,969] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,969] {spark_submit.py:490} INFO - "name" : "Open",
[2023-05-28 03:13:32,970] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:13:32,970] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,970] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,971] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,971] {spark_submit.py:490} INFO - "name" : "High",
[2023-05-28 03:13:32,971] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:13:32,972] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,972] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,973] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,973] {spark_submit.py:490} INFO - "name" : "Low",
[2023-05-28 03:13:32,973] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:13:32,973] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,974] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,974] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,975] {spark_submit.py:490} INFO - "name" : "Close",
[2023-05-28 03:13:32,975] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:13:32,975] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,976] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,976] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,976] {spark_submit.py:490} INFO - "name" : "Adj Close",
[2023-05-28 03:13:32,977] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:13:32,977] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,978] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,978] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,978] {spark_submit.py:490} INFO - "name" : "Volume",
[2023-05-28 03:13:32,978] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:13:32,979] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,979] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,980] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,980] {spark_submit.py:490} INFO - "name" : "vol_moving_avg",
[2023-05-28 03:13:32,980] {spark_submit.py:490} INFO - "type" : "double",
[2023-05-28 03:13:32,980] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,985] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,986] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:13:32,986] {spark_submit.py:490} INFO - "name" : "adj_close_rolling_med",
[2023-05-28 03:13:32,987] {spark_submit.py:490} INFO - "type" : "double",
[2023-05-28 03:13:32,987] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:13:32,988] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:13:32,988] {spark_submit.py:490} INFO - } ]
[2023-05-28 03:13:32,988] {spark_submit.py:490} INFO - }
[2023-05-28 03:13:32,989] {spark_submit.py:490} INFO - and corresponding Parquet message type:
[2023-05-28 03:13:32,989] {spark_submit.py:490} INFO - message spark_schema {
[2023-05-28 03:13:32,990] {spark_submit.py:490} INFO - optional binary Symbol (STRING);
[2023-05-28 03:13:32,990] {spark_submit.py:490} INFO - optional binary Security Name (STRING);
[2023-05-28 03:13:32,990] {spark_submit.py:490} INFO - optional binary Date (STRING);
[2023-05-28 03:13:32,991] {spark_submit.py:490} INFO - optional binary Open (STRING);
[2023-05-28 03:13:32,991] {spark_submit.py:490} INFO - optional binary High (STRING);
[2023-05-28 03:13:32,991] {spark_submit.py:490} INFO - optional binary Low (STRING);
[2023-05-28 03:13:32,992] {spark_submit.py:490} INFO - optional binary Close (STRING);
[2023-05-28 03:13:32,992] {spark_submit.py:490} INFO - optional binary Adj Close (STRING);
[2023-05-28 03:13:32,993] {spark_submit.py:490} INFO - optional binary Volume (STRING);
[2023-05-28 03:13:32,993] {spark_submit.py:490} INFO - optional double vol_moving_avg;
[2023-05-28 03:13:32,994] {spark_submit.py:490} INFO - optional double adj_close_rolling_med;
[2023-05-28 03:13:32,994] {spark_submit.py:490} INFO - }
[2023-05-28 03:13:32,994] {spark_submit.py:490} INFO - 
[2023-05-28 03:13:32,994] {spark_submit.py:490} INFO - 
[2023-05-28 03:13:33,013] {spark_submit.py:490} INFO - 23/05/28 03:13:33 INFO CodecPool: Got brand-new compressor [.lz4]
[2023-05-28 03:13:33,019] {spark_submit.py:490} INFO - 23/05/28 03:13:33 INFO CodecPool: Got brand-new compressor [.lz4]
[2023-05-28 03:13:33,020] {spark_submit.py:490} INFO - 23/05/28 03:13:33 INFO CodecPool: Got brand-new compressor [.lz4]
[2023-05-28 03:13:33,021] {spark_submit.py:490} INFO - 23/05/28 03:13:33 INFO CodecPool: Got brand-new compressor [.lz4]
[2023-05-28 03:13:35,892] {spark_submit.py:490} INFO - 23/05/28 03:13:35 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:13:36,166] {spark_submit.py:490} INFO - 23/05/28 03:13:36 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:13:36,334] {spark_submit.py:490} INFO - 23/05/28 03:13:36 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:13:36,517] {spark_submit.py:490} INFO - 23/05/28 03:13:36 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:13:56,250] {spark_submit.py:490} INFO - 23/05/28 03:13:56 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@3bfa35f5)) by listener AppStatusListener took 2.389101126s.
[2023-05-28 03:13:58,089] {spark_submit.py:490} INFO - 23/05/28 03:13:58 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:13:58,106] {spark_submit.py:490} INFO - 23/05/28 03:13:58 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:13:58,469] {spark_submit.py:490} INFO - 23/05/28 03:13:58 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:13:58,473] {spark_submit.py:490} INFO - 23/05/28 03:13:58 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:13:58,705] {spark_submit.py:490} INFO - 23/05/28 03:13:58 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:13:58,847] {spark_submit.py:490} INFO - 23/05/28 03:13:58 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:13:59,626] {spark_submit.py:490} INFO - 23/05/28 03:13:59 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:13:59,650] {spark_submit.py:490} INFO - 23/05/28 03:13:59 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:00,504] {spark_submit.py:490} INFO - 23/05/28 03:14:00 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:00,668] {spark_submit.py:490} INFO - 23/05/28 03:14:00 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:01,246] {spark_submit.py:490} INFO - 23/05/28 03:14:01 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:01,286] {spark_submit.py:490} INFO - 23/05/28 03:14:01 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:03,470] {spark_submit.py:490} INFO - 23/05/28 03:14:03 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:03,535] {spark_submit.py:490} INFO - 23/05/28 03:14:03 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:04,556] {spark_submit.py:490} INFO - 23/05/28 03:14:04 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:04,740] {spark_submit.py:490} INFO - 23/05/28 03:14:04 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:07,343] {spark_submit.py:490} INFO - 23/05/28 03:14:07 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:07,371] {spark_submit.py:490} INFO - 23/05/28 03:14:07 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:08,211] {spark_submit.py:490} INFO - 23/05/28 03:14:08 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:08,454] {spark_submit.py:490} INFO - 23/05/28 03:14:08 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:09,009] {spark_submit.py:490} INFO - 23/05/28 03:14:09 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:09,060] {spark_submit.py:490} INFO - 23/05/28 03:14:09 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:10,133] {spark_submit.py:490} INFO - 23/05/28 03:14:10 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:10,353] {spark_submit.py:490} INFO - 23/05/28 03:14:10 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:10,549] {spark_submit.py:490} INFO - 23/05/28 03:14:10 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:10,694] {spark_submit.py:490} INFO - 23/05/28 03:14:10 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:11,581] {spark_submit.py:490} INFO - 23/05/28 03:14:11 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:11,649] {spark_submit.py:490} INFO - 23/05/28 03:14:11 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:12,276] {spark_submit.py:490} INFO - 23/05/28 03:14:12 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:12,476] {spark_submit.py:490} INFO - 23/05/28 03:14:12 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:12,835] {spark_submit.py:490} INFO - 23/05/28 03:14:12 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:12,988] {spark_submit.py:490} INFO - 23/05/28 03:14:12 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:12,991] {spark_submit.py:490} INFO - 23/05/28 03:14:12 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:13,423] {spark_submit.py:490} INFO - 23/05/28 03:14:13 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:13,429] {spark_submit.py:490} INFO - 23/05/28 03:14:13 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:13,432] {spark_submit.py:490} INFO - 23/05/28 03:14:13 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:13,691] {spark_submit.py:490} INFO - 23/05/28 03:14:13 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:13,875] {spark_submit.py:490} INFO - 23/05/28 03:14:13 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:13,878] {spark_submit.py:490} INFO - 23/05/28 03:14:13 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:14,014] {spark_submit.py:490} INFO - 23/05/28 03:14:14 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:14,016] {spark_submit.py:490} INFO - 23/05/28 03:14:14 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:14,587] {spark_submit.py:490} INFO - 23/05/28 03:14:14 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:15,010] {spark_submit.py:490} INFO - 23/05/28 03:14:15 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:15,064] {spark_submit.py:490} INFO - 23/05/28 03:14:15 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:15,067] {spark_submit.py:490} INFO - 23/05/28 03:14:15 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:15,170] {spark_submit.py:490} INFO - 23/05/28 03:14:15 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:17,826] {spark_submit.py:490} INFO - 23/05/28 03:14:17 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:18,441] {spark_submit.py:490} INFO - 23/05/28 03:14:18 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:20,073] {spark_submit.py:490} INFO - 23/05/28 03:14:20 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:20,130] {spark_submit.py:490} INFO - 23/05/28 03:14:20 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:21,765] {spark_submit.py:490} INFO - 23/05/28 03:14:21 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:21,801] {spark_submit.py:490} INFO - 23/05/28 03:14:21 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:23,143] {spark_submit.py:490} INFO - 23/05/28 03:14:23 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:23,270] {spark_submit.py:490} INFO - 23/05/28 03:14:23 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:23,273] {spark_submit.py:490} INFO - 23/05/28 03:14:23 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:24,039] {spark_submit.py:490} INFO - 23/05/28 03:14:24 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:24,892] {spark_submit.py:490} INFO - 23/05/28 03:14:24 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:24,911] {spark_submit.py:490} INFO - 23/05/28 03:14:24 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:25,225] {spark_submit.py:490} INFO - 23/05/28 03:14:25 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:25,557] {spark_submit.py:490} INFO - 23/05/28 03:14:25 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:26,728] {spark_submit.py:490} INFO - 23/05/28 03:14:26 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:26,782] {spark_submit.py:490} INFO - 23/05/28 03:14:26 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:27,057] {spark_submit.py:490} INFO - 23/05/28 03:14:27 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:27,129] {spark_submit.py:490} INFO - 23/05/28 03:14:27 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:27,893] {spark_submit.py:490} INFO - 23/05/28 03:14:27 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:28,014] {spark_submit.py:490} INFO - 23/05/28 03:14:28 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:28,031] {spark_submit.py:490} INFO - 23/05/28 03:14:28 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:28,038] {spark_submit.py:490} INFO - 23/05/28 03:14:28 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:28,042] {spark_submit.py:490} INFO - 23/05/28 03:14:28 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:28,050] {spark_submit.py:490} INFO - 23/05/28 03:14:28 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:28,059] {spark_submit.py:490} INFO - 23/05/28 03:14:28 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:29,228] {spark_submit.py:490} INFO - 23/05/28 03:14:29 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:29,454] {spark_submit.py:490} INFO - 23/05/28 03:14:29 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:29,457] {spark_submit.py:490} INFO - 23/05/28 03:14:29 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:29,628] {spark_submit.py:490} INFO - 23/05/28 03:14:29 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:29,730] {spark_submit.py:490} INFO - 23/05/28 03:14:29 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:29,773] {spark_submit.py:490} INFO - 23/05/28 03:14:29 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:30,290] {spark_submit.py:490} INFO - 23/05/28 03:14:30 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:30,338] {spark_submit.py:490} INFO - 23/05/28 03:14:30 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:30,340] {spark_submit.py:490} INFO - 23/05/28 03:14:30 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:30,613] {spark_submit.py:490} INFO - 23/05/28 03:14:30 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:30,643] {spark_submit.py:490} INFO - 23/05/28 03:14:30 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:30,645] {spark_submit.py:490} INFO - 23/05/28 03:14:30 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:30,659] {spark_submit.py:490} INFO - 23/05/28 03:14:30 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:31,209] {spark_submit.py:490} INFO - 23/05/28 03:14:31 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:31,213] {spark_submit.py:490} INFO - 23/05/28 03:14:31 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:31,359] {spark_submit.py:490} INFO - 23/05/28 03:14:31 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:31,504] {spark_submit.py:490} INFO - 23/05/28 03:14:31 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:31,823] {spark_submit.py:490} INFO - 23/05/28 03:14:31 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:32,133] {spark_submit.py:490} INFO - 23/05/28 03:14:32 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:32,134] {spark_submit.py:490} INFO - 23/05/28 03:14:32 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:32,595] {spark_submit.py:490} INFO - 23/05/28 03:14:32 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:32,792] {spark_submit.py:490} INFO - 23/05/28 03:14:32 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:32,923] {spark_submit.py:490} INFO - 23/05/28 03:14:32 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:32,929] {spark_submit.py:490} INFO - 23/05/28 03:14:32 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:32,931] {spark_submit.py:490} INFO - 23/05/28 03:14:32 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:33,099] {spark_submit.py:490} INFO - 23/05/28 03:14:33 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:33,110] {spark_submit.py:490} INFO - 23/05/28 03:14:33 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:33,635] {spark_submit.py:490} INFO - 23/05/28 03:14:33 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:33,830] {spark_submit.py:490} INFO - 23/05/28 03:14:33 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:33,832] {spark_submit.py:490} INFO - 23/05/28 03:14:33 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:34,641] {spark_submit.py:490} INFO - 23/05/28 03:14:34 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:34,643] {spark_submit.py:490} INFO - 23/05/28 03:14:34 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:35,047] {spark_submit.py:490} INFO - 23/05/28 03:14:35 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:35,139] {spark_submit.py:490} INFO - 23/05/28 03:14:35 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:35,252] {spark_submit.py:490} INFO - 23/05/28 03:14:35 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:35,413] {spark_submit.py:490} INFO - 23/05/28 03:14:35 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:35,534] {spark_submit.py:490} INFO - 23/05/28 03:14:35 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:35,548] {spark_submit.py:490} INFO - 23/05/28 03:14:35 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:36,008] {spark_submit.py:490} INFO - 23/05/28 03:14:36 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:36,257] {spark_submit.py:490} INFO - 23/05/28 03:14:36 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:36,258] {spark_submit.py:490} INFO - 23/05/28 03:14:36 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:36,358] {spark_submit.py:490} INFO - 23/05/28 03:14:36 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:36,360] {spark_submit.py:490} INFO - 23/05/28 03:14:36 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:36,690] {spark_submit.py:490} INFO - 23/05/28 03:14:36 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:36,919] {spark_submit.py:490} INFO - 23/05/28 03:14:36 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:37,124] {spark_submit.py:490} INFO - 23/05/28 03:14:37 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:37,135] {spark_submit.py:490} INFO - 23/05/28 03:14:37 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:37,137] {spark_submit.py:490} INFO - 23/05/28 03:14:37 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:37,986] {spark_submit.py:490} INFO - 23/05/28 03:14:37 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:37,988] {spark_submit.py:490} INFO - 23/05/28 03:14:37 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:38,144] {spark_submit.py:490} INFO - 23/05/28 03:14:38 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:38,378] {spark_submit.py:490} INFO - 23/05/28 03:14:38 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:38,379] {spark_submit.py:490} INFO - 23/05/28 03:14:38 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:38,595] {spark_submit.py:490} INFO - 23/05/28 03:14:38 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:38,597] {spark_submit.py:490} INFO - 23/05/28 03:14:38 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:38,701] {spark_submit.py:490} INFO - 23/05/28 03:14:38 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:38,763] {spark_submit.py:490} INFO - 23/05/28 03:14:38 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:38,971] {spark_submit.py:490} INFO - 23/05/28 03:14:38 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:38,973] {spark_submit.py:490} INFO - 23/05/28 03:14:38 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:39,409] {spark_submit.py:490} INFO - 23/05/28 03:14:39 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:39,412] {spark_submit.py:490} INFO - 23/05/28 03:14:39 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:39,991] {spark_submit.py:490} INFO - 23/05/28 03:14:39 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:40,012] {spark_submit.py:490} INFO - 23/05/28 03:14:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:40,207] {spark_submit.py:490} INFO - 23/05/28 03:14:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:40,209] {spark_submit.py:490} INFO - 23/05/28 03:14:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:40,486] {spark_submit.py:490} INFO - 23/05/28 03:14:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:41,095] {spark_submit.py:490} INFO - 23/05/28 03:14:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:41,098] {spark_submit.py:490} INFO - 23/05/28 03:14:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:41,336] {spark_submit.py:490} INFO - 23/05/28 03:14:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:43,754] {spark_submit.py:490} INFO - 23/05/28 03:14:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:43,765] {spark_submit.py:490} INFO - 23/05/28 03:14:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:45,232] {spark_submit.py:490} INFO - 23/05/28 03:14:45 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:45,250] {spark_submit.py:490} INFO - 23/05/28 03:14:45 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:45,356] {spark_submit.py:490} INFO - 23/05/28 03:14:45 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:45,370] {spark_submit.py:490} INFO - 23/05/28 03:14:45 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:47,059] {spark_submit.py:490} INFO - 23/05/28 03:14:47 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:47,461] {spark_submit.py:490} INFO - 23/05/28 03:14:47 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:49,334] {spark_submit.py:490} INFO - 23/05/28 03:14:49 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:49,341] {spark_submit.py:490} INFO - 23/05/28 03:14:49 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:52,166] {spark_submit.py:490} INFO - 23/05/28 03:14:52 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:52,265] {spark_submit.py:490} INFO - 23/05/28 03:14:52 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:54,733] {spark_submit.py:490} INFO - 23/05/28 03:14:54 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:54,742] {spark_submit.py:490} INFO - 23/05/28 03:14:54 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:55,800] {spark_submit.py:490} INFO - 23/05/28 03:14:55 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:55,867] {spark_submit.py:490} INFO - 23/05/28 03:14:55 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:55,869] {spark_submit.py:490} INFO - 23/05/28 03:14:55 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:56,541] {spark_submit.py:490} INFO - 23/05/28 03:14:56 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:57,313] {spark_submit.py:490} INFO - 23/05/28 03:14:57 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:14:57,359] {spark_submit.py:490} INFO - 23/05/28 03:14:57 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:00,070] {spark_submit.py:490} INFO - 23/05/28 03:15:00 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:00,238] {spark_submit.py:490} INFO - 23/05/28 03:15:00 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:00,241] {spark_submit.py:490} INFO - 23/05/28 03:15:00 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:00,765] {spark_submit.py:490} INFO - 23/05/28 03:15:00 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:01,059] {spark_submit.py:490} INFO - 23/05/28 03:15:01 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:01,112] {spark_submit.py:490} INFO - 23/05/28 03:15:01 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:01,399] {spark_submit.py:490} INFO - 23/05/28 03:15:01 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:01,645] {spark_submit.py:490} INFO - 23/05/28 03:15:01 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:01,762] {spark_submit.py:490} INFO - 23/05/28 03:15:01 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:02,069] {spark_submit.py:490} INFO - 23/05/28 03:15:02 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:02,109] {spark_submit.py:490} INFO - 23/05/28 03:15:02 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:02,542] {spark_submit.py:490} INFO - 23/05/28 03:15:02 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:02,565] {spark_submit.py:490} INFO - 23/05/28 03:15:02 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:02,580] {spark_submit.py:490} INFO - 23/05/28 03:15:02 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:02,598] {spark_submit.py:490} INFO - 23/05/28 03:15:02 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:02,926] {spark_submit.py:490} INFO - 23/05/28 03:15:02 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:03,694] {spark_submit.py:490} INFO - 23/05/28 03:15:03 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:03,932] {spark_submit.py:490} INFO - 23/05/28 03:15:03 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:05,871] {spark_submit.py:490} INFO - 23/05/28 03:15:05 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:05,931] {spark_submit.py:490} INFO - 23/05/28 03:15:05 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:06,775] {spark_submit.py:490} INFO - 23/05/28 03:15:06 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:07,097] {spark_submit.py:490} INFO - 23/05/28 03:15:07 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:10,913] {spark_submit.py:490} INFO - 23/05/28 03:15:10 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:11,453] {spark_submit.py:490} INFO - 23/05/28 03:15:11 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:13,084] {spark_submit.py:490} INFO - 23/05/28 03:15:13 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:13,211] {spark_submit.py:490} INFO - 23/05/28 03:15:13 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:13,529] {spark_submit.py:490} INFO - 23/05/28 03:15:13 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:13,955] {spark_submit.py:490} INFO - 23/05/28 03:15:13 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:14,842] {spark_submit.py:490} INFO - 23/05/28 03:15:14 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:15,326] {spark_submit.py:490} INFO - 23/05/28 03:15:15 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:15,334] {spark_submit.py:490} INFO - 23/05/28 03:15:15 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:16,399] {spark_submit.py:490} INFO - 23/05/28 03:15:16 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:16,433] {spark_submit.py:490} INFO - 23/05/28 03:15:16 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:17,664] {spark_submit.py:490} INFO - 23/05/28 03:15:17 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:17,667] {spark_submit.py:490} INFO - 23/05/28 03:15:17 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:18,241] {spark_submit.py:490} INFO - 23/05/28 03:15:18 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:18,451] {spark_submit.py:490} INFO - 23/05/28 03:15:18 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:18,661] {spark_submit.py:490} INFO - 23/05/28 03:15:18 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:19,587] {spark_submit.py:490} INFO - 23/05/28 03:15:19 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:19,928] {spark_submit.py:490} INFO - 23/05/28 03:15:19 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:20,594] {spark_submit.py:490} INFO - 23/05/28 03:15:20 INFO FileOutputCommitter: Saved output of task 'attempt_202305280313298261293925689436040_0004_m_000003_78' to file:/usr/local/spark/staging/20230528/feature_engineering/etfs.parquet/_temporary/0/task_202305280313298261293925689436040_0004_m_000003
[2023-05-28 03:15:20,596] {spark_submit.py:490} INFO - 23/05/28 03:15:20 INFO SparkHadoopMapRedUtil: attempt_202305280313298261293925689436040_0004_m_000003_78: Committed. Elapsed time: 3 ms.
[2023-05-28 03:15:20,616] {spark_submit.py:490} INFO - 23/05/28 03:15:20 INFO Executor: Finished task 3.0 in stage 4.0 (TID 78). 6113 bytes result sent to driver
[2023-05-28 03:15:20,618] {spark_submit.py:490} INFO - 23/05/28 03:15:20 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 79) (c1224b457a2d, executor driver, partition 4, NODE_LOCAL, 7363 bytes)
[2023-05-28 03:15:20,619] {spark_submit.py:490} INFO - 23/05/28 03:15:20 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 78) in 110549 ms on c1224b457a2d (executor driver) (1/5)
[2023-05-28 03:15:20,630] {spark_submit.py:490} INFO - 23/05/28 03:15:20 INFO Executor: Running task 4.0 in stage 4.0 (TID 79)
[2023-05-28 03:15:20,652] {spark_submit.py:490} INFO - 23/05/28 03:15:20 INFO ShuffleBlockFetcherIterator: Getting 4 (46.8 MiB) non-empty blocks including 4 (46.8 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-05-28 03:15:20,659] {spark_submit.py:490} INFO - 23/05/28 03:15:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2023-05-28 03:15:21,043] {spark_submit.py:490} INFO - 23/05/28 03:15:21 INFO UnsafeExternalSorter: Thread 121 spilling sort data of 144.0 MiB to disk (0  time so far)
[2023-05-28 03:15:21,225] {spark_submit.py:490} INFO - 23/05/28 03:15:21 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:21,231] {spark_submit.py:490} INFO - 23/05/28 03:15:21 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:21,677] {spark_submit.py:490} INFO - 23/05/28 03:15:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-28 03:15:21,677] {spark_submit.py:490} INFO - 23/05/28 03:15:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-28 03:15:21,680] {spark_submit.py:490} INFO - 23/05/28 03:15:21 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-05-28 03:15:21,680] {spark_submit.py:490} INFO - 23/05/28 03:15:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-28 03:15:21,681] {spark_submit.py:490} INFO - 23/05/28 03:15:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-28 03:15:21,681] {spark_submit.py:490} INFO - 23/05/28 03:15:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-05-28 03:15:21,685] {spark_submit.py:490} INFO - 23/05/28 03:15:21 INFO CodecConfig: Compression: LZ4
[2023-05-28 03:15:21,686] {spark_submit.py:490} INFO - 23/05/28 03:15:21 INFO CodecConfig: Compression: LZ4
[2023-05-28 03:15:21,686] {spark_submit.py:490} INFO - 23/05/28 03:15:21 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2023-05-28 03:15:21,689] {spark_submit.py:490} INFO - 23/05/28 03:15:21 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-05-28 03:15:21,689] {spark_submit.py:490} INFO - {
[2023-05-28 03:15:21,690] {spark_submit.py:490} INFO - "type" : "struct",
[2023-05-28 03:15:21,691] {spark_submit.py:490} INFO - "fields" : [ {
[2023-05-28 03:15:21,691] {spark_submit.py:490} INFO - "name" : "Symbol",
[2023-05-28 03:15:21,691] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:15:21,691] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:15:21,692] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:15:21,692] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:15:21,693] {spark_submit.py:490} INFO - "name" : "Security Name",
[2023-05-28 03:15:21,693] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:15:21,693] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:15:21,699] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:15:21,699] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:15:21,700] {spark_submit.py:490} INFO - "name" : "Date",
[2023-05-28 03:15:21,700] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:15:21,701] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:15:21,701] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:15:21,701] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:15:21,702] {spark_submit.py:490} INFO - "name" : "Open",
[2023-05-28 03:15:21,703] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:15:21,704] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:15:21,704] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:15:21,704] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:15:21,705] {spark_submit.py:490} INFO - "name" : "High",
[2023-05-28 03:15:21,705] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:15:21,705] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:15:21,706] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:15:21,706] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:15:21,706] {spark_submit.py:490} INFO - "name" : "Low",
[2023-05-28 03:15:21,706] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:15:21,707] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:15:21,707] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:15:21,707] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:15:21,708] {spark_submit.py:490} INFO - "name" : "Close",
[2023-05-28 03:15:21,708] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:15:21,708] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:15:21,709] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:15:21,709] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:15:21,709] {spark_submit.py:490} INFO - "name" : "Adj Close",
[2023-05-28 03:15:21,710] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:15:21,710] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:15:21,710] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:15:21,711] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:15:21,711] {spark_submit.py:490} INFO - "name" : "Volume",
[2023-05-28 03:15:21,711] {spark_submit.py:490} INFO - "type" : "string",
[2023-05-28 03:15:21,712] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:15:21,712] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:15:21,713] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:15:21,713] {spark_submit.py:490} INFO - "name" : "vol_moving_avg",
[2023-05-28 03:15:21,714] {spark_submit.py:490} INFO - "type" : "double",
[2023-05-28 03:15:21,714] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:15:21,714] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:15:21,715] {spark_submit.py:490} INFO - }, {
[2023-05-28 03:15:21,715] {spark_submit.py:490} INFO - "name" : "adj_close_rolling_med",
[2023-05-28 03:15:21,715] {spark_submit.py:490} INFO - "type" : "double",
[2023-05-28 03:15:21,716] {spark_submit.py:490} INFO - "nullable" : true,
[2023-05-28 03:15:21,716] {spark_submit.py:490} INFO - "metadata" : { }
[2023-05-28 03:15:21,717] {spark_submit.py:490} INFO - } ]
[2023-05-28 03:15:21,717] {spark_submit.py:490} INFO - }
[2023-05-28 03:15:21,717] {spark_submit.py:490} INFO - and corresponding Parquet message type:
[2023-05-28 03:15:21,717] {spark_submit.py:490} INFO - message spark_schema {
[2023-05-28 03:15:21,718] {spark_submit.py:490} INFO - optional binary Symbol (STRING);
[2023-05-28 03:15:21,718] {spark_submit.py:490} INFO - optional binary Security Name (STRING);
[2023-05-28 03:15:21,719] {spark_submit.py:490} INFO - optional binary Date (STRING);
[2023-05-28 03:15:21,719] {spark_submit.py:490} INFO - optional binary Open (STRING);
[2023-05-28 03:15:21,719] {spark_submit.py:490} INFO - optional binary High (STRING);
[2023-05-28 03:15:21,720] {spark_submit.py:490} INFO - optional binary Low (STRING);
[2023-05-28 03:15:21,720] {spark_submit.py:490} INFO - optional binary Close (STRING);
[2023-05-28 03:15:21,721] {spark_submit.py:490} INFO - optional binary Adj Close (STRING);
[2023-05-28 03:15:21,721] {spark_submit.py:490} INFO - optional binary Volume (STRING);
[2023-05-28 03:15:21,722] {spark_submit.py:490} INFO - optional double vol_moving_avg;
[2023-05-28 03:15:21,722] {spark_submit.py:490} INFO - optional double adj_close_rolling_med;
[2023-05-28 03:15:21,723] {spark_submit.py:490} INFO - }
[2023-05-28 03:15:21,724] {spark_submit.py:490} INFO - 
[2023-05-28 03:15:21,724] {spark_submit.py:490} INFO - 
[2023-05-28 03:15:22,947] {spark_submit.py:490} INFO - 23/05/28 03:15:22 INFO FileOutputCommitter: Saved output of task 'attempt_202305280313298261293925689436040_0004_m_000001_76' to file:/usr/local/spark/staging/20230528/feature_engineering/etfs.parquet/_temporary/0/task_202305280313298261293925689436040_0004_m_000001
[2023-05-28 03:15:22,949] {spark_submit.py:490} INFO - 23/05/28 03:15:22 INFO SparkHadoopMapRedUtil: attempt_202305280313298261293925689436040_0004_m_000001_76: Committed. Elapsed time: 8 ms.
[2023-05-28 03:15:22,959] {spark_submit.py:490} INFO - 23/05/28 03:15:22 INFO Executor: Finished task 1.0 in stage 4.0 (TID 76). 6113 bytes result sent to driver
[2023-05-28 03:15:22,960] {spark_submit.py:490} INFO - 23/05/28 03:15:22 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 76) in 112891 ms on c1224b457a2d (executor driver) (2/5)
[2023-05-28 03:15:26,129] {spark_submit.py:490} INFO - 23/05/28 03:15:26 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:26,335] {spark_submit.py:490} INFO - 23/05/28 03:15:26 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:26,760] {spark_submit.py:490} INFO - 23/05/28 03:15:26 INFO FileOutputCommitter: Saved output of task 'attempt_202305280313298261293925689436040_0004_m_000000_75' to file:/usr/local/spark/staging/20230528/feature_engineering/etfs.parquet/_temporary/0/task_202305280313298261293925689436040_0004_m_000000
[2023-05-28 03:15:26,760] {spark_submit.py:490} INFO - 23/05/28 03:15:26 INFO SparkHadoopMapRedUtil: attempt_202305280313298261293925689436040_0004_m_000000_75: Committed. Elapsed time: 2 ms.
[2023-05-28 03:15:26,765] {spark_submit.py:490} INFO - 23/05/28 03:15:26 INFO Executor: Finished task 0.0 in stage 4.0 (TID 75). 6070 bytes result sent to driver
[2023-05-28 03:15:26,768] {spark_submit.py:490} INFO - 23/05/28 03:15:26 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 75) in 116698 ms on c1224b457a2d (executor driver) (3/5)
[2023-05-28 03:15:27,926] {spark_submit.py:490} INFO - 23/05/28 03:15:27 INFO FileOutputCommitter: Saved output of task 'attempt_202305280313298261293925689436040_0004_m_000002_77' to file:/usr/local/spark/staging/20230528/feature_engineering/etfs.parquet/_temporary/0/task_202305280313298261293925689436040_0004_m_000002
[2023-05-28 03:15:27,926] {spark_submit.py:490} INFO - 23/05/28 03:15:27 INFO SparkHadoopMapRedUtil: attempt_202305280313298261293925689436040_0004_m_000002_77: Committed. Elapsed time: 2 ms.
[2023-05-28 03:15:27,929] {spark_submit.py:490} INFO - 23/05/28 03:15:27 INFO Executor: Finished task 2.0 in stage 4.0 (TID 77). 6070 bytes result sent to driver
[2023-05-28 03:15:27,929] {spark_submit.py:490} INFO - 23/05/28 03:15:27 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 77) in 117859 ms on c1224b457a2d (executor driver) (4/5)
[2023-05-28 03:15:28,009] {spark_submit.py:490} INFO - 23/05/28 03:15:28 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:28,322] {spark_submit.py:490} INFO - 23/05/28 03:15:28 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:28,903] {spark_submit.py:490} INFO - 23/05/28 03:15:28 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:29,193] {spark_submit.py:490} INFO - 23/05/28 03:15:29 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:30,561] {spark_submit.py:490} INFO - 23/05/28 03:15:30 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:30,624] {spark_submit.py:490} INFO - 23/05/28 03:15:30 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:31,934] {spark_submit.py:490} INFO - 23/05/28 03:15:31 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:31,996] {spark_submit.py:490} INFO - 23/05/28 03:15:31 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:31,998] {spark_submit.py:490} INFO - 23/05/28 03:15:31 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:32,814] {spark_submit.py:490} INFO - 23/05/28 03:15:32 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:32,816] {spark_submit.py:490} INFO - 23/05/28 03:15:32 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:33,648] {spark_submit.py:490} INFO - 23/05/28 03:15:33 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:33,650] {spark_submit.py:490} INFO - 23/05/28 03:15:33 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:34,486] {spark_submit.py:490} INFO - 23/05/28 03:15:34 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:35,273] {spark_submit.py:490} INFO - 23/05/28 03:15:35 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:35,278] {spark_submit.py:490} INFO - 23/05/28 03:15:35 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:35,280] {spark_submit.py:490} INFO - 23/05/28 03:15:35 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:36,064] {spark_submit.py:490} INFO - 23/05/28 03:15:36 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:36,066] {spark_submit.py:490} INFO - 23/05/28 03:15:36 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:36,621] {spark_submit.py:490} INFO - 23/05/28 03:15:36 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:36,623] {spark_submit.py:490} INFO - 23/05/28 03:15:36 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:37,439] {spark_submit.py:490} INFO - 23/05/28 03:15:37 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:37,441] {spark_submit.py:490} INFO - 23/05/28 03:15:37 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:38,402] {spark_submit.py:490} INFO - 23/05/28 03:15:38 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:39,115] {spark_submit.py:490} INFO - 23/05/28 03:15:39 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:39,139] {spark_submit.py:490} INFO - 23/05/28 03:15:39 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:39,140] {spark_submit.py:490} INFO - 23/05/28 03:15:39 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:39,571] {spark_submit.py:490} INFO - 23/05/28 03:15:39 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:42,858] {spark_submit.py:490} INFO - 23/05/28 03:15:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:43,019] {spark_submit.py:490} INFO - 23/05/28 03:15:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:43,514] {spark_submit.py:490} INFO - 23/05/28 03:15:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:43,517] {spark_submit.py:490} INFO - 23/05/28 03:15:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:44,573] {spark_submit.py:490} INFO - 23/05/28 03:15:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:45,007] {spark_submit.py:490} INFO - 23/05/28 03:15:45 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:45,009] {spark_submit.py:490} INFO - 23/05/28 03:15:45 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:45,603] {spark_submit.py:490} INFO - 23/05/28 03:15:45 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:45,604] {spark_submit.py:490} INFO - 23/05/28 03:15:45 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:46,172] {spark_submit.py:490} INFO - 23/05/28 03:15:46 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:46,173] {spark_submit.py:490} INFO - 23/05/28 03:15:46 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:46,735] {spark_submit.py:490} INFO - 23/05/28 03:15:46 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:46,737] {spark_submit.py:490} INFO - 23/05/28 03:15:46 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:47,243] {spark_submit.py:490} INFO - 23/05/28 03:15:47 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:47,246] {spark_submit.py:490} INFO - 23/05/28 03:15:47 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:47,796] {spark_submit.py:490} INFO - 23/05/28 03:15:47 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:49,567] {spark_submit.py:490} INFO - 23/05/28 03:15:49 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:49,651] {spark_submit.py:490} INFO - 23/05/28 03:15:49 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:53,452] {spark_submit.py:490} INFO - 23/05/28 03:15:53 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:15:53,755] {spark_submit.py:490} INFO - 23/05/28 03:15:53 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:16:11,417] {spark_submit.py:490} INFO - 23/05/28 03:16:11 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:16:11,578] {spark_submit.py:490} INFO - 23/05/28 03:16:11 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:16:11,580] {spark_submit.py:490} INFO - 23/05/28 03:16:11 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:16:12,337] {spark_submit.py:490} INFO - 23/05/28 03:16:12 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-05-28 03:16:17,150] {spark_submit.py:490} INFO - 23/05/28 03:16:17 INFO FileOutputCommitter: Saved output of task 'attempt_202305280313298261293925689436040_0004_m_000004_79' to file:/usr/local/spark/staging/20230528/feature_engineering/etfs.parquet/_temporary/0/task_202305280313298261293925689436040_0004_m_000004
[2023-05-28 03:16:17,152] {spark_submit.py:490} INFO - 23/05/28 03:16:17 INFO SparkHadoopMapRedUtil: attempt_202305280313298261293925689436040_0004_m_000004_79: Committed. Elapsed time: 3 ms.
[2023-05-28 03:16:17,173] {spark_submit.py:490} INFO - 23/05/28 03:16:17 INFO Executor: Finished task 4.0 in stage 4.0 (TID 79). 6113 bytes result sent to driver
[2023-05-28 03:16:17,176] {spark_submit.py:490} INFO - 23/05/28 03:16:17 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 79) in 56558 ms on c1224b457a2d (executor driver) (5/5)
[2023-05-28 03:16:17,179] {spark_submit.py:490} INFO - 23/05/28 03:16:17 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-05-28 03:16:17,181] {spark_submit.py:490} INFO - 23/05/28 03:16:17 INFO DAGScheduler: ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0) finished in 167.188 s
[2023-05-28 03:16:17,182] {spark_submit.py:490} INFO - 23/05/28 03:16:17 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-28 03:16:17,185] {spark_submit.py:490} INFO - 23/05/28 03:16:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-05-28 03:16:17,186] {spark_submit.py:490} INFO - 23/05/28 03:16:17 INFO DAGScheduler: Job 3 finished: parquet at NativeMethodAccessorImpl.java:0, took 167.212975 s
[2023-05-28 03:16:17,198] {spark_submit.py:490} INFO - 23/05/28 03:16:17 INFO FileFormatWriter: Start to commit write Job cd1aa5d0-3192-429b-8525-3fe40f0f9c85.
[2023-05-28 03:16:17,361] {spark_submit.py:490} INFO - 23/05/28 03:16:17 INFO FileFormatWriter: Write Job cd1aa5d0-3192-429b-8525-3fe40f0f9c85 committed. Elapsed time: 162 ms.
[2023-05-28 03:16:17,365] {spark_submit.py:490} INFO - 23/05/28 03:16:17 INFO FileFormatWriter: Finished processing stats for write job cd1aa5d0-3192-429b-8525-3fe40f0f9c85.
[2023-05-28 03:16:17,464] {spark_submit.py:490} INFO - 23/05/28 03:16:17 INFO SparkContext: Invoking stop() from shutdown hook
[2023-05-28 03:16:17,471] {spark_submit.py:490} INFO - 23/05/28 03:16:17 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2023-05-28 03:16:17,483] {spark_submit.py:490} INFO - 23/05/28 03:16:17 INFO SparkUI: Stopped Spark web UI at http://c1224b457a2d:4040
[2023-05-28 03:16:17,511] {spark_submit.py:490} INFO - 23/05/28 03:16:17 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-05-28 03:16:17,600] {spark_submit.py:490} INFO - 23/05/28 03:16:17 INFO MemoryStore: MemoryStore cleared
[2023-05-28 03:16:17,602] {spark_submit.py:490} INFO - 23/05/28 03:16:17 INFO BlockManager: BlockManager stopped
[2023-05-28 03:16:17,607] {spark_submit.py:490} INFO - 23/05/28 03:16:17 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-05-28 03:16:17,608] {spark_submit.py:490} INFO - 23/05/28 03:16:17 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-05-28 03:16:17,617] {spark_submit.py:490} INFO - 23/05/28 03:16:17 INFO SparkContext: Successfully stopped SparkContext
[2023-05-28 03:16:17,619] {spark_submit.py:490} INFO - 23/05/28 03:16:17 INFO ShutdownHookManager: Shutdown hook called
[2023-05-28 03:16:17,619] {spark_submit.py:490} INFO - 23/05/28 03:16:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-9d19a92c-9d94-4821-8eda-466f5c1dcf0f/pyspark-920f11bd-e42a-4209-8bc2-66bee79a1e39
[2023-05-28 03:16:17,623] {spark_submit.py:490} INFO - 23/05/28 03:16:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-29b25817-8be7-4290-9f9c-1bb30cefc4ad
[2023-05-28 03:16:17,628] {spark_submit.py:490} INFO - 23/05/28 03:16:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-9d19a92c-9d94-4821-8eda-466f5c1dcf0f
[2023-05-28 03:16:18,498] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=stock_spark_***, task_id=feature_engineering.etf_feature_processing, execution_date=20230528T031039, start_date=20230528T031308, end_date=20230528T031618
[2023-05-28 03:16:18,554] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-05-28 03:16:18,596] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
