[2023-05-28 14:16:16,651] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: stock_spark_airflow.feature_engineering.etf_feature_processing manual__2023-05-28T14:06:12.061445+00:00 [upstream_failed]>
[2023-05-28 14:16:16,652] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: stock_spark_airflow.feature_engineering.etf_feature_processing manual__2023-05-28T14:06:12.061445+00:00 [upstream_failed]>
[2023-05-28 14:16:16,653] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-05-28 14:16:16,653] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2023-05-28 14:16:16,654] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-05-28 14:16:16,662] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): feature_engineering.etf_feature_processing> on 2023-05-28 14:06:12.061445+00:00
[2023-05-28 14:16:16,667] {standard_task_runner.py:52} INFO - Started process 1998 to run task
[2023-05-28 14:16:16,670] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'stock_spark_***', 'feature_engineering.etf_feature_processing', 'manual__2023-05-28T14:06:12.061445+00:00', '--job-id', '346', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/stock_spark_ml.py', '--cfg-path', '/tmp/tmpj9z1b0p9', '--error-file', '/tmp/tmpk0j2m46d']
[2023-05-28 14:16:16,672] {standard_task_runner.py:80} INFO - Job 346: Subtask feature_engineering.etf_feature_processing
[2023-05-28 14:16:16,719] {task_command.py:369} INFO - Running <TaskInstance: stock_spark_airflow.feature_engineering.etf_feature_processing manual__2023-05-28T14:06:12.061445+00:00 [running]> on host 58e417c56c9d
[2023-05-28 14:16:16,771] {taskinstance.py:1571} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=stock_spark_***
AIRFLOW_CTX_TASK_ID=feature_engineering.etf_feature_processing
AIRFLOW_CTX_EXECUTION_DATE=2023-05-28T14:06:12.061445+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=manual__2023-05-28T14:06:12.061445+00:00
[2023-05-28 14:16:16,781] {base.py:68} INFO - Using connection ID 'spark_local' for task execution.
[2023-05-28 14:16:16,782] {spark_submit.py:339} INFO - Spark-Submit cmd: spark-submit --master spark://spark:7077 --name arrow-spark /usr/local/spark/app/feature_engineering_processing.py etfs
[2023-05-28 14:16:16,864] {spark_submit.py:490} INFO - /home/***/.local/lib/python3.7/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2023-05-28 14:16:18,786] {spark_submit.py:490} INFO - 23/05/28 14:16:18 INFO SparkContext: Running Spark version 3.4.0
[2023-05-28 14:16:18,831] {spark_submit.py:490} INFO - 23/05/28 14:16:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-05-28 14:16:18,885] {spark_submit.py:490} INFO - 23/05/28 14:16:18 INFO ResourceUtils: ==============================================================
[2023-05-28 14:16:18,887] {spark_submit.py:490} INFO - 23/05/28 14:16:18 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-05-28 14:16:18,889] {spark_submit.py:490} INFO - 23/05/28 14:16:18 INFO ResourceUtils: ==============================================================
[2023-05-28 14:16:18,890] {spark_submit.py:490} INFO - 23/05/28 14:16:18 INFO SparkContext: Submitted application: random_reg
[2023-05-28 14:16:18,902] {spark_submit.py:490} INFO - 23/05/28 14:16:18 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-05-28 14:16:18,910] {spark_submit.py:490} INFO - 23/05/28 14:16:18 INFO ResourceProfile: Limiting resource is cpu
[2023-05-28 14:16:18,911] {spark_submit.py:490} INFO - 23/05/28 14:16:18 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-05-28 14:16:18,948] {spark_submit.py:490} INFO - 23/05/28 14:16:18 INFO SecurityManager: Changing view acls to: default
[2023-05-28 14:16:18,952] {spark_submit.py:490} INFO - 23/05/28 14:16:18 INFO SecurityManager: Changing modify acls to: default
[2023-05-28 14:16:18,953] {spark_submit.py:490} INFO - 23/05/28 14:16:18 INFO SecurityManager: Changing view acls groups to:
[2023-05-28 14:16:18,953] {spark_submit.py:490} INFO - 23/05/28 14:16:18 INFO SecurityManager: Changing modify acls groups to:
[2023-05-28 14:16:18,953] {spark_submit.py:490} INFO - 23/05/28 14:16:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
[2023-05-28 14:16:19,148] {spark_submit.py:490} INFO - 23/05/28 14:16:19 INFO Utils: Successfully started service 'sparkDriver' on port 40273.
[2023-05-28 14:16:19,179] {spark_submit.py:490} INFO - 23/05/28 14:16:19 INFO SparkEnv: Registering MapOutputTracker
[2023-05-28 14:16:19,202] {spark_submit.py:490} INFO - 23/05/28 14:16:19 INFO SparkEnv: Registering BlockManagerMaster
[2023-05-28 14:16:19,217] {spark_submit.py:490} INFO - 23/05/28 14:16:19 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-05-28 14:16:19,218] {spark_submit.py:490} INFO - 23/05/28 14:16:19 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-05-28 14:16:19,221] {spark_submit.py:490} INFO - 23/05/28 14:16:19 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-05-28 14:16:19,236] {spark_submit.py:490} INFO - 23/05/28 14:16:19 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d81299e3-579c-4c16-8e48-7316633a1bf6
[2023-05-28 14:16:19,247] {spark_submit.py:490} INFO - 23/05/28 14:16:19 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-05-28 14:16:19,258] {spark_submit.py:490} INFO - 23/05/28 14:16:19 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-05-28 14:16:19,356] {spark_submit.py:490} INFO - 23/05/28 14:16:19 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2023-05-28 14:16:19,410] {spark_submit.py:490} INFO - 23/05/28 14:16:19 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-05-28 14:16:19,505] {spark_submit.py:490} INFO - 23/05/28 14:16:19 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark:7077...
[2023-05-28 14:16:19,541] {spark_submit.py:490} INFO - 23/05/28 14:16:19 INFO TransportClientFactory: Successfully created connection to spark/172.23.0.6:7077 after 19 ms (0 ms spent in bootstraps)
[2023-05-28 14:16:19,613] {spark_submit.py:490} INFO - 23/05/28 14:16:19 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20230528141619-0005
[2023-05-28 14:16:19,616] {spark_submit.py:490} INFO - 23/05/28 14:16:19 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20230528141619-0005/0 on worker-20230528055114-172.23.0.4-41329 (172.23.0.4:41329) with 1 core(s)
[2023-05-28 14:16:19,628] {spark_submit.py:490} INFO - 23/05/28 14:16:19 INFO StandaloneSchedulerBackend: Granted executor ID app-20230528141619-0005/0 on hostPort 172.23.0.4:41329 with 1 core(s), 1024.0 MiB RAM
[2023-05-28 14:16:19,629] {spark_submit.py:490} INFO - 23/05/28 14:16:19 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20230528141619-0005/1 on worker-20230528055114-172.23.0.3-42839 (172.23.0.3:42839) with 1 core(s)
[2023-05-28 14:16:19,629] {spark_submit.py:490} INFO - 23/05/28 14:16:19 INFO StandaloneSchedulerBackend: Granted executor ID app-20230528141619-0005/1 on hostPort 172.23.0.3:42839 with 1 core(s), 1024.0 MiB RAM
[2023-05-28 14:16:19,630] {spark_submit.py:490} INFO - 23/05/28 14:16:19 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20230528141619-0005/2 on worker-20230528055114-172.23.0.7-36005 (172.23.0.7:36005) with 1 core(s)
[2023-05-28 14:16:19,632] {spark_submit.py:490} INFO - 23/05/28 14:16:19 INFO StandaloneSchedulerBackend: Granted executor ID app-20230528141619-0005/2 on hostPort 172.23.0.7:36005 with 1 core(s), 1024.0 MiB RAM
[2023-05-28 14:16:19,635] {spark_submit.py:490} INFO - 23/05/28 14:16:19 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44457.
[2023-05-28 14:16:19,635] {spark_submit.py:490} INFO - 23/05/28 14:16:19 INFO NettyBlockTransferService: Server created on 58e417c56c9d:44457
[2023-05-28 14:16:19,636] {spark_submit.py:490} INFO - 23/05/28 14:16:19 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-05-28 14:16:19,665] {spark_submit.py:490} INFO - 23/05/28 14:16:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 58e417c56c9d, 44457, None)
[2023-05-28 14:16:19,681] {spark_submit.py:490} INFO - 23/05/28 14:16:19 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230528141619-0005/2 is now RUNNING
[2023-05-28 14:16:19,686] {spark_submit.py:490} INFO - 23/05/28 14:16:19 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230528141619-0005/0 is now RUNNING
[2023-05-28 14:16:19,689] {spark_submit.py:490} INFO - 23/05/28 14:16:19 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230528141619-0005/1 is now RUNNING
[2023-05-28 14:16:19,697] {spark_submit.py:490} INFO - 23/05/28 14:16:19 INFO BlockManagerMasterEndpoint: Registering block manager 58e417c56c9d:44457 with 434.4 MiB RAM, BlockManagerId(driver, 58e417c56c9d, 44457, None)
[2023-05-28 14:16:19,699] {spark_submit.py:490} INFO - 23/05/28 14:16:19 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 58e417c56c9d, 44457, None)
[2023-05-28 14:16:19,700] {spark_submit.py:490} INFO - 23/05/28 14:16:19 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 58e417c56c9d, 44457, None)
[2023-05-28 14:16:19,903] {spark_submit.py:490} INFO - 23/05/28 14:16:19 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2023-05-28 14:16:19,933] {spark_submit.py:490} INFO - /home/***/.local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.
[2023-05-28 14:16:19,934] {spark_submit.py:490} INFO - warnings.warn("Python 3.7 support is deprecated in Spark 3.4.", FutureWarning)
[2023-05-28 14:16:20,246] {spark_submit.py:490} INFO - 23/05/28 14:16:20 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-05-28 14:16:20,265] {spark_submit.py:490} INFO - 23/05/28 14:16:20 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-05-28 14:16:21,852] {spark_submit.py:490} INFO - 23/05/28 14:16:21 INFO HadoopFSUtils: Listing leaf files and directories in parallel under 70 paths. The first several paths are: file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00000-67bff78d-fd22-4b95-bc03-9a44ae59680f-c000.lz4.parquet, file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00001-67bff78d-fd22-4b95-bc03-9a44ae59680f-c000.lz4.parquet, file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00002-67bff78d-fd22-4b95-bc03-9a44ae59680f-c000.lz4.parquet, file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00003-67bff78d-fd22-4b95-bc03-9a44ae59680f-c000.lz4.parquet, file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00004-67bff78d-fd22-4b95-bc03-9a44ae59680f-c000.lz4.parquet, file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00005-67bff78d-fd22-4b95-bc03-9a44ae59680f-c000.lz4.parquet, file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00006-67bff78d-fd22-4b95-bc03-9a44ae59680f-c000.lz4.parquet, file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00007-67bff78d-fd22-4b95-bc03-9a44ae59680f-c000.lz4.parquet, file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00008-67bff78d-fd22-4b95-bc03-9a44ae59680f-c000.lz4.parquet, file:/usr/local/spark/staging/20230528/raw_data_processing/etfs.parquet/part-00009-67bff78d-fd22-4b95-bc03-9a44ae59680f-c000.lz4.parquet.
[2023-05-28 14:16:22,678] {spark_submit.py:490} INFO - 23/05/28 14:16:22 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-05-28 14:16:22,710] {spark_submit.py:490} INFO - 23/05/28 14:16:22 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 70 output partitions
[2023-05-28 14:16:22,718] {spark_submit.py:490} INFO - 23/05/28 14:16:22 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-05-28 14:16:22,720] {spark_submit.py:490} INFO - 23/05/28 14:16:22 INFO DAGScheduler: Parents of final stage: List()
[2023-05-28 14:16:22,730] {spark_submit.py:490} INFO - 23/05/28 14:16:22 INFO DAGScheduler: Missing parents: List()
[2023-05-28 14:16:22,765] {spark_submit.py:490} INFO - 23/05/28 14:16:22 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-28 14:16:23,111] {spark_submit.py:490} INFO - 23/05/28 14:16:23 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 102.8 KiB, free 434.3 MiB)
[2023-05-28 14:16:23,256] {spark_submit.py:490} INFO - 23/05/28 14:16:23 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 434.3 MiB)
[2023-05-28 14:16:23,262] {spark_submit.py:490} INFO - 23/05/28 14:16:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 58e417c56c9d:44457 (size: 36.9 KiB, free: 434.4 MiB)
[2023-05-28 14:16:23,266] {spark_submit.py:490} INFO - 23/05/28 14:16:23 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535
[2023-05-28 14:16:23,351] {spark_submit.py:490} INFO - 23/05/28 14:16:23 INFO DAGScheduler: Submitting 70 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2023-05-28 14:16:23,356] {spark_submit.py:490} INFO - 23/05/28 14:16:23 INFO TaskSchedulerImpl: Adding task set 0.0 with 70 tasks resource profile 0
[2023-05-28 14:16:23,692] {spark_submit.py:490} INFO - 23/05/28 14:16:23 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.23.0.7:35004) with ID 2,  ResourceProfileId 0
[2023-05-28 14:16:23,705] {spark_submit.py:490} INFO - 23/05/28 14:16:23 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.23.0.3:52384) with ID 1,  ResourceProfileId 0
[2023-05-28 14:16:23,732] {spark_submit.py:490} INFO - 23/05/28 14:16:23 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.23.0.4:36092) with ID 0,  ResourceProfileId 0
[2023-05-28 14:16:23,798] {spark_submit.py:490} INFO - 23/05/28 14:16:23 INFO BlockManagerMasterEndpoint: Registering block manager 172.23.0.7:33561 with 434.4 MiB RAM, BlockManagerId(2, 172.23.0.7, 33561, None)
[2023-05-28 14:16:23,810] {spark_submit.py:490} INFO - 23/05/28 14:16:23 INFO BlockManagerMasterEndpoint: Registering block manager 172.23.0.3:39979 with 434.4 MiB RAM, BlockManagerId(1, 172.23.0.3, 39979, None)
[2023-05-28 14:16:23,831] {spark_submit.py:490} INFO - 23/05/28 14:16:23 INFO BlockManagerMasterEndpoint: Registering block manager 172.23.0.4:42027 with 434.4 MiB RAM, BlockManagerId(0, 172.23.0.4, 42027, None)
[2023-05-28 14:16:23,869] {spark_submit.py:490} INFO - 23/05/28 14:16:23 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.23.0.7, executor 2, partition 0, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:23,883] {spark_submit.py:490} INFO - 23/05/28 14:16:23 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (172.23.0.3, executor 1, partition 1, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:23,896] {spark_submit.py:490} INFO - 23/05/28 14:16:23 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (172.23.0.4, executor 0, partition 2, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:24,174] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.23.0.3:39979 (size: 36.9 KiB, free: 434.4 MiB)
[2023-05-28 14:16:24,180] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.23.0.7:33561 (size: 36.9 KiB, free: 434.4 MiB)
[2023-05-28 14:16:24,186] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.23.0.4:42027 (size: 36.9 KiB, free: 434.4 MiB)
[2023-05-28 14:16:24,520] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (172.23.0.7, executor 2, partition 3, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:24,542] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (172.23.0.4, executor 0, partition 4, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:24,568] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (172.23.0.3, executor 1, partition 5, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:24,573] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (172.23.0.7, executor 2, partition 6, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:24,578] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 692 ms on 172.23.0.3 (executor 1) (1/70)
[2023-05-28 14:16:24,597] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 701 ms on 172.23.0.4 (executor 0) (2/70)
[2023-05-28 14:16:24,601] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 751 ms on 172.23.0.7 (executor 2) (3/70)
[2023-05-28 14:16:24,603] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 82 ms on 172.23.0.7 (executor 2) (4/70)
[2023-05-28 14:16:24,605] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (172.23.0.4, executor 0, partition 7, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:24,606] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 65 ms on 172.23.0.4 (executor 0) (5/70)
[2023-05-28 14:16:24,607] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (172.23.0.7, executor 2, partition 8, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:24,609] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 37 ms on 172.23.0.7 (executor 2) (6/70)
[2023-05-28 14:16:24,614] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (172.23.0.3, executor 1, partition 9, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:24,615] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 45 ms on 172.23.0.3 (executor 1) (7/70)
[2023-05-28 14:16:24,637] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 31 ms on 172.23.0.4 (executor 0) (8/70)
[2023-05-28 14:16:24,654] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10) (172.23.0.4, executor 0, partition 10, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:24,655] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11) (172.23.0.3, executor 1, partition 11, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:24,662] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 43 ms on 172.23.0.3 (executor 1) (9/70)
[2023-05-28 14:16:24,665] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12) (172.23.0.7, executor 2, partition 12, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:24,672] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 59 ms on 172.23.0.7 (executor 2) (10/70)
[2023-05-28 14:16:24,695] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13) (172.23.0.4, executor 0, partition 13, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:24,696] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 57 ms on 172.23.0.4 (executor 0) (11/70)
[2023-05-28 14:16:24,703] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14) (172.23.0.3, executor 1, partition 14, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:24,704] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 49 ms on 172.23.0.3 (executor 1) (12/70)
[2023-05-28 14:16:24,713] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15) (172.23.0.7, executor 2, partition 15, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:24,717] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 48 ms on 172.23.0.7 (executor 2) (13/70)
[2023-05-28 14:16:24,736] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Starting task 16.0 in stage 0.0 (TID 16) (172.23.0.4, executor 0, partition 16, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:24,737] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 42 ms on 172.23.0.4 (executor 0) (14/70)
[2023-05-28 14:16:24,754] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Starting task 17.0 in stage 0.0 (TID 17) (172.23.0.3, executor 1, partition 17, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:24,761] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Starting task 18.0 in stage 0.0 (TID 18) (172.23.0.7, executor 2, partition 18, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:24,762] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 45 ms on 172.23.0.7 (executor 2) (15/70)
[2023-05-28 14:16:24,762] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 56 ms on 172.23.0.3 (executor 1) (16/70)
[2023-05-28 14:16:24,771] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Starting task 19.0 in stage 0.0 (TID 19) (172.23.0.4, executor 0, partition 19, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:24,772] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Finished task 16.0 in stage 0.0 (TID 16) in 35 ms on 172.23.0.4 (executor 0) (17/70)
[2023-05-28 14:16:24,787] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Starting task 20.0 in stage 0.0 (TID 20) (172.23.0.3, executor 1, partition 20, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:24,788] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Finished task 17.0 in stage 0.0 (TID 17) in 33 ms on 172.23.0.3 (executor 1) (18/70)
[2023-05-28 14:16:24,807] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Starting task 21.0 in stage 0.0 (TID 21) (172.23.0.7, executor 2, partition 21, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:24,808] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Starting task 22.0 in stage 0.0 (TID 22) (172.23.0.4, executor 0, partition 22, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:24,808] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Finished task 19.0 in stage 0.0 (TID 19) in 37 ms on 172.23.0.4 (executor 0) (19/70)
[2023-05-28 14:16:24,809] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Finished task 18.0 in stage 0.0 (TID 18) in 53 ms on 172.23.0.7 (executor 2) (20/70)
[2023-05-28 14:16:24,830] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Starting task 23.0 in stage 0.0 (TID 23) (172.23.0.3, executor 1, partition 23, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:24,831] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Finished task 20.0 in stage 0.0 (TID 20) in 44 ms on 172.23.0.3 (executor 1) (21/70)
[2023-05-28 14:16:24,840] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Starting task 24.0 in stage 0.0 (TID 24) (172.23.0.7, executor 2, partition 24, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:24,841] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Finished task 21.0 in stage 0.0 (TID 21) in 36 ms on 172.23.0.7 (executor 2) (22/70)
[2023-05-28 14:16:24,849] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Starting task 25.0 in stage 0.0 (TID 25) (172.23.0.4, executor 0, partition 25, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:24,849] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Finished task 22.0 in stage 0.0 (TID 22) in 42 ms on 172.23.0.4 (executor 0) (23/70)
[2023-05-28 14:16:24,873] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Starting task 26.0 in stage 0.0 (TID 26) (172.23.0.3, executor 1, partition 26, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:24,873] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Finished task 23.0 in stage 0.0 (TID 23) in 44 ms on 172.23.0.3 (executor 1) (24/70)
[2023-05-28 14:16:24,900] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Starting task 27.0 in stage 0.0 (TID 27) (172.23.0.7, executor 2, partition 27, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:24,901] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Finished task 24.0 in stage 0.0 (TID 24) in 61 ms on 172.23.0.7 (executor 2) (25/70)
[2023-05-28 14:16:24,903] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Starting task 28.0 in stage 0.0 (TID 28) (172.23.0.4, executor 0, partition 28, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:24,910] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Finished task 25.0 in stage 0.0 (TID 25) in 57 ms on 172.23.0.4 (executor 0) (26/70)
[2023-05-28 14:16:24,917] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Starting task 29.0 in stage 0.0 (TID 29) (172.23.0.3, executor 1, partition 29, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:24,918] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Finished task 26.0 in stage 0.0 (TID 26) in 40 ms on 172.23.0.3 (executor 1) (27/70)
[2023-05-28 14:16:24,931] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Starting task 30.0 in stage 0.0 (TID 30) (172.23.0.7, executor 2, partition 30, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:24,932] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Finished task 27.0 in stage 0.0 (TID 27) in 29 ms on 172.23.0.7 (executor 2) (28/70)
[2023-05-28 14:16:24,942] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Starting task 31.0 in stage 0.0 (TID 31) (172.23.0.4, executor 0, partition 31, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:24,942] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Finished task 28.0 in stage 0.0 (TID 28) in 38 ms on 172.23.0.4 (executor 0) (29/70)
[2023-05-28 14:16:24,956] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Starting task 32.0 in stage 0.0 (TID 32) (172.23.0.3, executor 1, partition 32, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:24,957] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Finished task 29.0 in stage 0.0 (TID 29) in 42 ms on 172.23.0.3 (executor 1) (30/70)
[2023-05-28 14:16:24,961] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Starting task 33.0 in stage 0.0 (TID 33) (172.23.0.4, executor 0, partition 33, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:24,962] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Finished task 31.0 in stage 0.0 (TID 31) in 26 ms on 172.23.0.4 (executor 0) (31/70)
[2023-05-28 14:16:24,968] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Finished task 32.0 in stage 0.0 (TID 32) in 17 ms on 172.23.0.3 (executor 1) (32/70)
[2023-05-28 14:16:24,969] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Starting task 34.0 in stage 0.0 (TID 34) (172.23.0.3, executor 1, partition 34, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:24,973] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Finished task 30.0 in stage 0.0 (TID 30) in 46 ms on 172.23.0.7 (executor 2) (33/70)
[2023-05-28 14:16:24,974] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Starting task 35.0 in stage 0.0 (TID 35) (172.23.0.7, executor 2, partition 35, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:24,982] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Finished task 33.0 in stage 0.0 (TID 33) in 22 ms on 172.23.0.4 (executor 0) (34/70)
[2023-05-28 14:16:24,985] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Starting task 36.0 in stage 0.0 (TID 36) (172.23.0.4, executor 0, partition 36, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:24,998] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Finished task 34.0 in stage 0.0 (TID 34) in 21 ms on 172.23.0.3 (executor 1) (35/70)
[2023-05-28 14:16:24,998] {spark_submit.py:490} INFO - 23/05/28 14:16:24 INFO TaskSetManager: Starting task 37.0 in stage 0.0 (TID 37) (172.23.0.3, executor 1, partition 37, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:25,010] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Finished task 36.0 in stage 0.0 (TID 36) in 26 ms on 172.23.0.4 (executor 0) (36/70)
[2023-05-28 14:16:25,016] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Starting task 38.0 in stage 0.0 (TID 38) (172.23.0.4, executor 0, partition 38, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:25,024] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Starting task 39.0 in stage 0.0 (TID 39) (172.23.0.7, executor 2, partition 39, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:25,025] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Finished task 35.0 in stage 0.0 (TID 35) in 45 ms on 172.23.0.7 (executor 2) (37/70)
[2023-05-28 14:16:25,027] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Starting task 40.0 in stage 0.0 (TID 40) (172.23.0.3, executor 1, partition 40, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:25,044] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Finished task 37.0 in stage 0.0 (TID 37) in 44 ms on 172.23.0.3 (executor 1) (38/70)
[2023-05-28 14:16:25,045] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Starting task 41.0 in stage 0.0 (TID 41) (172.23.0.4, executor 0, partition 41, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:25,045] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Starting task 42.0 in stage 0.0 (TID 42) (172.23.0.7, executor 2, partition 42, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:25,046] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Finished task 38.0 in stage 0.0 (TID 38) in 35 ms on 172.23.0.4 (executor 0) (39/70)
[2023-05-28 14:16:25,051] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Finished task 39.0 in stage 0.0 (TID 39) in 40 ms on 172.23.0.7 (executor 2) (40/70)
[2023-05-28 14:16:25,072] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Starting task 43.0 in stage 0.0 (TID 43) (172.23.0.3, executor 1, partition 43, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:25,073] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Starting task 44.0 in stage 0.0 (TID 44) (172.23.0.4, executor 0, partition 44, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:25,074] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Finished task 40.0 in stage 0.0 (TID 40) in 43 ms on 172.23.0.3 (executor 1) (41/70)
[2023-05-28 14:16:25,075] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Finished task 41.0 in stage 0.0 (TID 41) in 33 ms on 172.23.0.4 (executor 0) (42/70)
[2023-05-28 14:16:25,095] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Starting task 45.0 in stage 0.0 (TID 45) (172.23.0.7, executor 2, partition 45, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:25,097] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Starting task 46.0 in stage 0.0 (TID 46) (172.23.0.4, executor 0, partition 46, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:25,110] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Starting task 47.0 in stage 0.0 (TID 47) (172.23.0.3, executor 1, partition 47, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:25,122] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Finished task 42.0 in stage 0.0 (TID 42) in 77 ms on 172.23.0.7 (executor 2) (43/70)
[2023-05-28 14:16:25,134] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Finished task 44.0 in stage 0.0 (TID 44) in 56 ms on 172.23.0.4 (executor 0) (44/70)
[2023-05-28 14:16:25,137] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Finished task 43.0 in stage 0.0 (TID 43) in 73 ms on 172.23.0.3 (executor 1) (45/70)
[2023-05-28 14:16:25,138] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Starting task 48.0 in stage 0.0 (TID 48) (172.23.0.7, executor 2, partition 48, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:25,143] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Finished task 45.0 in stage 0.0 (TID 45) in 55 ms on 172.23.0.7 (executor 2) (46/70)
[2023-05-28 14:16:25,144] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Finished task 47.0 in stage 0.0 (TID 47) in 27 ms on 172.23.0.3 (executor 1) (47/70)
[2023-05-28 14:16:25,144] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Starting task 49.0 in stage 0.0 (TID 49) (172.23.0.3, executor 1, partition 49, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:25,145] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Starting task 50.0 in stage 0.0 (TID 50) (172.23.0.4, executor 0, partition 50, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:25,150] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Finished task 46.0 in stage 0.0 (TID 46) in 47 ms on 172.23.0.4 (executor 0) (48/70)
[2023-05-28 14:16:25,172] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Starting task 51.0 in stage 0.0 (TID 51) (172.23.0.7, executor 2, partition 51, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:25,173] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Starting task 52.0 in stage 0.0 (TID 52) (172.23.0.3, executor 1, partition 52, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:25,174] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Finished task 48.0 in stage 0.0 (TID 48) in 39 ms on 172.23.0.7 (executor 2) (49/70)
[2023-05-28 14:16:25,175] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Finished task 49.0 in stage 0.0 (TID 49) in 31 ms on 172.23.0.3 (executor 1) (50/70)
[2023-05-28 14:16:25,185] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Starting task 53.0 in stage 0.0 (TID 53) (172.23.0.4, executor 0, partition 53, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:25,199] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Finished task 52.0 in stage 0.0 (TID 52) in 28 ms on 172.23.0.3 (executor 1) (51/70)
[2023-05-28 14:16:25,205] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Starting task 54.0 in stage 0.0 (TID 54) (172.23.0.3, executor 1, partition 54, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:25,210] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Finished task 50.0 in stage 0.0 (TID 50) in 65 ms on 172.23.0.4 (executor 0) (52/70)
[2023-05-28 14:16:25,231] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Starting task 55.0 in stage 0.0 (TID 55) (172.23.0.7, executor 2, partition 55, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:25,234] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Finished task 51.0 in stage 0.0 (TID 51) in 57 ms on 172.23.0.7 (executor 2) (53/70)
[2023-05-28 14:16:25,235] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Starting task 56.0 in stage 0.0 (TID 56) (172.23.0.3, executor 1, partition 56, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:25,236] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Finished task 54.0 in stage 0.0 (TID 54) in 34 ms on 172.23.0.3 (executor 1) (54/70)
[2023-05-28 14:16:25,243] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Finished task 53.0 in stage 0.0 (TID 53) in 58 ms on 172.23.0.4 (executor 0) (55/70)
[2023-05-28 14:16:25,255] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Starting task 57.0 in stage 0.0 (TID 57) (172.23.0.4, executor 0, partition 57, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:25,274] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Starting task 58.0 in stage 0.0 (TID 58) (172.23.0.7, executor 2, partition 58, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:25,278] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Finished task 55.0 in stage 0.0 (TID 55) in 60 ms on 172.23.0.7 (executor 2) (56/70)
[2023-05-28 14:16:25,308] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Finished task 56.0 in stage 0.0 (TID 56) in 75 ms on 172.23.0.3 (executor 1) (57/70)
[2023-05-28 14:16:25,314] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Starting task 59.0 in stage 0.0 (TID 59) (172.23.0.3, executor 1, partition 59, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:25,319] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Starting task 60.0 in stage 0.0 (TID 60) (172.23.0.4, executor 0, partition 60, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:25,327] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Finished task 57.0 in stage 0.0 (TID 57) in 82 ms on 172.23.0.4 (executor 0) (58/70)
[2023-05-28 14:16:25,335] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Starting task 61.0 in stage 0.0 (TID 61) (172.23.0.7, executor 2, partition 61, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:25,345] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Finished task 58.0 in stage 0.0 (TID 58) in 78 ms on 172.23.0.7 (executor 2) (59/70)
[2023-05-28 14:16:25,359] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Finished task 60.0 in stage 0.0 (TID 60) in 40 ms on 172.23.0.4 (executor 0) (60/70)
[2023-05-28 14:16:25,378] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Starting task 62.0 in stage 0.0 (TID 62) (172.23.0.4, executor 0, partition 62, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:25,384] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Finished task 59.0 in stage 0.0 (TID 59) in 70 ms on 172.23.0.3 (executor 1) (61/70)
[2023-05-28 14:16:25,392] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Starting task 63.0 in stage 0.0 (TID 63) (172.23.0.3, executor 1, partition 63, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:25,400] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Starting task 64.0 in stage 0.0 (TID 64) (172.23.0.7, executor 2, partition 64, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:25,421] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Finished task 61.0 in stage 0.0 (TID 61) in 80 ms on 172.23.0.7 (executor 2) (62/70)
[2023-05-28 14:16:25,426] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Finished task 62.0 in stage 0.0 (TID 62) in 44 ms on 172.23.0.4 (executor 0) (63/70)
[2023-05-28 14:16:25,432] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Starting task 65.0 in stage 0.0 (TID 65) (172.23.0.4, executor 0, partition 65, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:25,443] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Finished task 63.0 in stage 0.0 (TID 63) in 52 ms on 172.23.0.3 (executor 1) (64/70)
[2023-05-28 14:16:25,450] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Starting task 66.0 in stage 0.0 (TID 66) (172.23.0.3, executor 1, partition 66, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:25,457] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Finished task 64.0 in stage 0.0 (TID 64) in 56 ms on 172.23.0.7 (executor 2) (65/70)
[2023-05-28 14:16:25,462] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Starting task 67.0 in stage 0.0 (TID 67) (172.23.0.7, executor 2, partition 67, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:25,477] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Finished task 65.0 in stage 0.0 (TID 65) in 50 ms on 172.23.0.4 (executor 0) (66/70)
[2023-05-28 14:16:25,492] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Starting task 68.0 in stage 0.0 (TID 68) (172.23.0.4, executor 0, partition 68, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:25,512] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Finished task 66.0 in stage 0.0 (TID 66) in 63 ms on 172.23.0.3 (executor 1) (67/70)
[2023-05-28 14:16:25,536] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Starting task 69.0 in stage 0.0 (TID 69) (172.23.0.3, executor 1, partition 69, PROCESS_LOCAL, 7502 bytes)
[2023-05-28 14:16:25,560] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Finished task 68.0 in stage 0.0 (TID 68) in 71 ms on 172.23.0.4 (executor 0) (68/70)
[2023-05-28 14:16:25,570] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Finished task 67.0 in stage 0.0 (TID 67) in 108 ms on 172.23.0.7 (executor 2) (69/70)
[2023-05-28 14:16:25,581] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSetManager: Finished task 69.0 in stage 0.0 (TID 69) in 44 ms on 172.23.0.3 (executor 1) (70/70)
[2023-05-28 14:16:25,592] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-05-28 14:16:25,593] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 2.734 s
[2023-05-28 14:16:25,594] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-28 14:16:25,596] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-05-28 14:16:25,600] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 2.921153 s
[2023-05-28 14:16:25,628] {spark_submit.py:490} INFO - 23/05/28 14:16:25 INFO InMemoryFileIndex: It took 3823 ms to list leaf files for 70 paths.
[2023-05-28 14:16:26,076] {spark_submit.py:490} INFO - 23/05/28 14:16:26 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-05-28 14:16:26,084] {spark_submit.py:490} INFO - 23/05/28 14:16:26 INFO DAGScheduler: Got job 1 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-28 14:16:26,085] {spark_submit.py:490} INFO - 23/05/28 14:16:26 INFO DAGScheduler: Final stage: ResultStage 1 (load at NativeMethodAccessorImpl.java:0)
[2023-05-28 14:16:26,085] {spark_submit.py:490} INFO - 23/05/28 14:16:26 INFO DAGScheduler: Parents of final stage: List()
[2023-05-28 14:16:26,086] {spark_submit.py:490} INFO - 23/05/28 14:16:26 INFO DAGScheduler: Missing parents: List()
[2023-05-28 14:16:26,090] {spark_submit.py:490} INFO - 23/05/28 14:16:26 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-28 14:16:26,092] {spark_submit.py:490} INFO - 23/05/28 14:16:26 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 103.2 KiB, free 434.2 MiB)
[2023-05-28 14:16:26,096] {spark_submit.py:490} INFO - 23/05/28 14:16:26 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 37.1 KiB, free 434.1 MiB)
[2023-05-28 14:16:26,104] {spark_submit.py:490} INFO - 23/05/28 14:16:26 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 58e417c56c9d:44457 (size: 37.1 KiB, free: 434.3 MiB)
[2023-05-28 14:16:26,107] {spark_submit.py:490} INFO - 23/05/28 14:16:26 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535
[2023-05-28 14:16:26,108] {spark_submit.py:490} INFO - 23/05/28 14:16:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-28 14:16:26,108] {spark_submit.py:490} INFO - 23/05/28 14:16:26 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-05-28 14:16:26,111] {spark_submit.py:490} INFO - 23/05/28 14:16:26 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 70) (172.23.0.3, executor 1, partition 0, PROCESS_LOCAL, 7598 bytes)
[2023-05-28 14:16:26,133] {spark_submit.py:490} INFO - 23/05/28 14:16:26 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.23.0.3:39979 (size: 37.1 KiB, free: 434.3 MiB)
[2023-05-28 14:16:26,531] {spark_submit.py:490} INFO - 23/05/28 14:16:26 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 70) in 421 ms on 172.23.0.3 (executor 1) (1/1)
[2023-05-28 14:16:26,532] {spark_submit.py:490} INFO - 23/05/28 14:16:26 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-05-28 14:16:26,533] {spark_submit.py:490} INFO - 23/05/28 14:16:26 INFO DAGScheduler: ResultStage 1 (load at NativeMethodAccessorImpl.java:0) finished in 0.445 s
[2023-05-28 14:16:26,534] {spark_submit.py:490} INFO - 23/05/28 14:16:26 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-28 14:16:26,535] {spark_submit.py:490} INFO - 23/05/28 14:16:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-05-28 14:16:26,536] {spark_submit.py:490} INFO - 23/05/28 14:16:26 INFO DAGScheduler: Job 1 finished: load at NativeMethodAccessorImpl.java:0, took 0.452059 s
[2023-05-28 14:16:26,663] {spark_submit.py:490} INFO - 23/05/28 14:16:26 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.23.0.3:39979 in memory (size: 36.9 KiB, free: 434.4 MiB)
[2023-05-28 14:16:26,665] {spark_submit.py:490} INFO - 23/05/28 14:16:26 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.23.0.7:33561 in memory (size: 36.9 KiB, free: 434.4 MiB)
[2023-05-28 14:16:26,667] {spark_submit.py:490} INFO - 23/05/28 14:16:26 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.23.0.4:42027 in memory (size: 36.9 KiB, free: 434.4 MiB)
[2023-05-28 14:16:26,684] {spark_submit.py:490} INFO - 23/05/28 14:16:26 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 58e417c56c9d:44457 in memory (size: 36.9 KiB, free: 434.4 MiB)
[2023-05-28 14:16:26,704] {spark_submit.py:490} INFO - 23/05/28 14:16:26 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.23.0.3:39979 in memory (size: 37.1 KiB, free: 434.4 MiB)
[2023-05-28 14:16:26,708] {spark_submit.py:490} INFO - 23/05/28 14:16:26 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 58e417c56c9d:44457 in memory (size: 37.1 KiB, free: 434.4 MiB)
[2023-05-28 14:16:27,982] {spark_submit.py:490} INFO - /usr/local/spark/staging/20230528/feature_engineering/etfs.parquet
[2023-05-28 14:16:28,343] {spark_submit.py:490} INFO - 23/05/28 14:16:28 INFO FileSourceStrategy: Pushed Filters:
[2023-05-28 14:16:28,344] {spark_submit.py:490} INFO - 23/05/28 14:16:28 INFO FileSourceStrategy: Post-Scan Filters:
[2023-05-28 14:16:28,662] {spark_submit.py:490} INFO - 23/05/28 14:16:28 INFO CodeGenerator: Code generated in 157.674583 ms
[2023-05-28 14:16:28,695] {spark_submit.py:490} INFO - 23/05/28 14:16:28 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-05-28 14:16:28,715] {spark_submit.py:490} INFO - 23/05/28 14:16:28 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 434.2 MiB)
[2023-05-28 14:16:28,716] {spark_submit.py:490} INFO - 23/05/28 14:16:28 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 58e417c56c9d:44457 (size: 35.0 KiB, free: 434.4 MiB)
[2023-05-28 14:16:28,723] {spark_submit.py:490} INFO - 23/05/28 14:16:28 INFO SparkContext: Created broadcast 2 from parquet at NativeMethodAccessorImpl.java:0
[2023-05-28 14:16:28,747] {spark_submit.py:490} INFO - 23/05/28 14:16:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-28 14:16:28,813] {spark_submit.py:490} INFO - 23/05/28 14:16:28 INFO DAGScheduler: Registering RDD 8 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2023-05-28 14:16:28,817] {spark_submit.py:490} INFO - 23/05/28 14:16:28 INFO DAGScheduler: Got map stage job 2 (parquet at NativeMethodAccessorImpl.java:0) with 4 output partitions
[2023-05-28 14:16:28,818] {spark_submit.py:490} INFO - 23/05/28 14:16:28 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (parquet at NativeMethodAccessorImpl.java:0)
[2023-05-28 14:16:28,818] {spark_submit.py:490} INFO - 23/05/28 14:16:28 INFO DAGScheduler: Parents of final stage: List()
[2023-05-28 14:16:28,819] {spark_submit.py:490} INFO - 23/05/28 14:16:28 INFO DAGScheduler: Missing parents: List()
[2023-05-28 14:16:28,820] {spark_submit.py:490} INFO - 23/05/28 14:16:28 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[8] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-28 14:16:28,865] {spark_submit.py:490} INFO - 23/05/28 14:16:28 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 21.8 KiB, free 434.1 MiB)
[2023-05-28 14:16:28,866] {spark_submit.py:490} INFO - 23/05/28 14:16:28 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 434.1 MiB)
[2023-05-28 14:16:28,866] {spark_submit.py:490} INFO - 23/05/28 14:16:28 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 58e417c56c9d:44457 (size: 8.6 KiB, free: 434.4 MiB)
[2023-05-28 14:16:28,867] {spark_submit.py:490} INFO - 23/05/28 14:16:28 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535
[2023-05-28 14:16:28,868] {spark_submit.py:490} INFO - 23/05/28 14:16:28 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[8] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
[2023-05-28 14:16:28,869] {spark_submit.py:490} INFO - 23/05/28 14:16:28 INFO TaskSchedulerImpl: Adding task set 2.0 with 4 tasks resource profile 0
[2023-05-28 14:16:28,876] {spark_submit.py:490} INFO - 23/05/28 14:16:28 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 71) (172.23.0.7, executor 2, partition 0, PROCESS_LOCAL, 11205 bytes)
[2023-05-28 14:16:28,876] {spark_submit.py:490} INFO - 23/05/28 14:16:28 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 72) (172.23.0.3, executor 1, partition 1, PROCESS_LOCAL, 12145 bytes)
[2023-05-28 14:16:28,877] {spark_submit.py:490} INFO - 23/05/28 14:16:28 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 73) (172.23.0.4, executor 0, partition 2, PROCESS_LOCAL, 13085 bytes)
[2023-05-28 14:16:28,914] {spark_submit.py:490} INFO - 23/05/28 14:16:28 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.23.0.7:33561 (size: 8.6 KiB, free: 434.4 MiB)
[2023-05-28 14:16:28,923] {spark_submit.py:490} INFO - 23/05/28 14:16:28 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.23.0.3:39979 (size: 8.6 KiB, free: 434.4 MiB)
[2023-05-28 14:16:28,935] {spark_submit.py:490} INFO - 23/05/28 14:16:28 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.23.0.4:42027 (size: 8.6 KiB, free: 434.4 MiB)
[2023-05-28 14:16:30,139] {spark_submit.py:490} INFO - 23/05/28 14:16:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.23.0.4:42027 (size: 35.0 KiB, free: 434.4 MiB)
[2023-05-28 14:16:30,215] {spark_submit.py:490} INFO - 23/05/28 14:16:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.23.0.3:39979 (size: 35.0 KiB, free: 434.4 MiB)
[2023-05-28 14:16:30,317] {spark_submit.py:490} INFO - 23/05/28 14:16:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.23.0.7:33561 (size: 35.0 KiB, free: 434.4 MiB)
[2023-05-28 14:16:34,338] {spark_submit.py:490} INFO - 23/05/28 14:16:34 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 74) (172.23.0.4, executor 0, partition 3, PROCESS_LOCAL, 8009 bytes)
[2023-05-28 14:16:34,350] {spark_submit.py:490} INFO - 23/05/28 14:16:34 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 73) in 5473 ms on 172.23.0.4 (executor 0) (1/4)
[2023-05-28 14:16:34,424] {spark_submit.py:490} INFO - 23/05/28 14:16:34 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 74) in 90 ms on 172.23.0.4 (executor 0) (2/4)
[2023-05-28 14:16:35,849] {spark_submit.py:490} INFO - 23/05/28 14:16:35 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 72) in 6971 ms on 172.23.0.3 (executor 1) (3/4)
[2023-05-28 14:16:37,515] {spark_submit.py:490} INFO - 23/05/28 14:16:37 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 71) in 8643 ms on 172.23.0.7 (executor 2) (4/4)
[2023-05-28 14:16:37,516] {spark_submit.py:490} INFO - 23/05/28 14:16:37 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-05-28 14:16:37,517] {spark_submit.py:490} INFO - 23/05/28 14:16:37 INFO DAGScheduler: ShuffleMapStage 2 (parquet at NativeMethodAccessorImpl.java:0) finished in 8.691 s
[2023-05-28 14:16:37,517] {spark_submit.py:490} INFO - 23/05/28 14:16:37 INFO DAGScheduler: looking for newly runnable stages
[2023-05-28 14:16:37,518] {spark_submit.py:490} INFO - 23/05/28 14:16:37 INFO DAGScheduler: running: Set()
[2023-05-28 14:16:37,518] {spark_submit.py:490} INFO - 23/05/28 14:16:37 INFO DAGScheduler: waiting: Set()
[2023-05-28 14:16:37,519] {spark_submit.py:490} INFO - 23/05/28 14:16:37 INFO DAGScheduler: failed: Set()
[2023-05-28 14:16:37,548] {spark_submit.py:490} INFO - 23/05/28 14:16:37 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 67108864, minimum partition size: 1048576
[2023-05-28 14:16:37,629] {spark_submit.py:490} INFO - 23/05/28 14:16:37 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-05-28 14:16:37,647] {spark_submit.py:490} INFO - 23/05/28 14:16:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-28 14:16:37,648] {spark_submit.py:490} INFO - 23/05/28 14:16:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-28 14:16:37,648] {spark_submit.py:490} INFO - 23/05/28 14:16:37 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-05-28 14:16:37,649] {spark_submit.py:490} INFO - 23/05/28 14:16:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-28 14:16:37,650] {spark_submit.py:490} INFO - 23/05/28 14:16:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-28 14:16:37,651] {spark_submit.py:490} INFO - 23/05/28 14:16:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-05-28 14:16:37,735] {spark_submit.py:490} INFO - 23/05/28 14:16:37 INFO CodeGenerator: Code generated in 14.294667 ms
[2023-05-28 14:16:37,754] {spark_submit.py:490} INFO - 23/05/28 14:16:37 INFO CodeGenerator: Code generated in 12.137083 ms
[2023-05-28 14:16:37,767] {spark_submit.py:490} INFO - 23/05/28 14:16:37 INFO CodeGenerator: Code generated in 8.926333 ms
[2023-05-28 14:16:37,821] {spark_submit.py:490} INFO - 23/05/28 14:16:37 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2023-05-28 14:16:37,835] {spark_submit.py:490} INFO - 23/05/28 14:16:37 INFO DAGScheduler: Got job 3 (parquet at NativeMethodAccessorImpl.java:0) with 5 output partitions
[2023-05-28 14:16:37,842] {spark_submit.py:490} INFO - 23/05/28 14:16:37 INFO DAGScheduler: Final stage: ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0)
[2023-05-28 14:16:37,847] {spark_submit.py:490} INFO - 23/05/28 14:16:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
[2023-05-28 14:16:37,848] {spark_submit.py:490} INFO - 23/05/28 14:16:37 INFO DAGScheduler: Missing parents: List()
[2023-05-28 14:16:37,848] {spark_submit.py:490} INFO - 23/05/28 14:16:37 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[15] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-28 14:16:37,856] {spark_submit.py:490} INFO - 23/05/28 14:16:37 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 247.9 KiB, free 433.9 MiB)
[2023-05-28 14:16:37,860] {spark_submit.py:490} INFO - 23/05/28 14:16:37 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 90.0 KiB, free 433.8 MiB)
[2023-05-28 14:16:37,861] {spark_submit.py:490} INFO - 23/05/28 14:16:37 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 58e417c56c9d:44457 (size: 90.0 KiB, free: 434.3 MiB)
[2023-05-28 14:16:37,861] {spark_submit.py:490} INFO - 23/05/28 14:16:37 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1535
[2023-05-28 14:16:37,862] {spark_submit.py:490} INFO - 23/05/28 14:16:37 INFO DAGScheduler: Submitting 5 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
[2023-05-28 14:16:37,862] {spark_submit.py:490} INFO - 23/05/28 14:16:37 INFO TaskSchedulerImpl: Adding task set 4.0 with 5 tasks resource profile 0
[2023-05-28 14:16:37,866] {spark_submit.py:490} INFO - 23/05/28 14:16:37 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 75) (172.23.0.4, executor 0, partition 0, NODE_LOCAL, 7367 bytes)
[2023-05-28 14:16:37,867] {spark_submit.py:490} INFO - 23/05/28 14:16:37 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 76) (172.23.0.3, executor 1, partition 1, NODE_LOCAL, 7367 bytes)
[2023-05-28 14:16:37,867] {spark_submit.py:490} INFO - 23/05/28 14:16:37 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 77) (172.23.0.7, executor 2, partition 2, NODE_LOCAL, 7367 bytes)
[2023-05-28 14:16:37,910] {spark_submit.py:490} INFO - 23/05/28 14:16:37 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.23.0.7:33561 (size: 90.0 KiB, free: 434.3 MiB)
[2023-05-28 14:16:37,923] {spark_submit.py:490} INFO - 23/05/28 14:16:37 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.23.0.3:39979 (size: 90.0 KiB, free: 434.3 MiB)
[2023-05-28 14:16:37,926] {spark_submit.py:490} INFO - 23/05/28 14:16:37 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.23.0.4:42027 (size: 90.0 KiB, free: 434.3 MiB)
[2023-05-28 14:16:38,151] {spark_submit.py:490} INFO - 23/05/28 14:16:38 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.23.0.7:35004
[2023-05-28 14:16:38,152] {spark_submit.py:490} INFO - 23/05/28 14:16:38 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.23.0.4:36092
[2023-05-28 14:16:38,153] {spark_submit.py:490} INFO - 23/05/28 14:16:38 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.23.0.3:52384
[2023-05-28 14:16:50,931] {spark_submit.py:490} INFO - 23/05/28 14:16:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230528141619-0005/2 is now EXITED (Command exited with code 137)
[2023-05-28 14:16:50,934] {spark_submit.py:490} INFO - 23/05/28 14:16:50 INFO StandaloneSchedulerBackend: Executor app-20230528141619-0005/2 removed: Command exited with code 137
[2023-05-28 14:16:50,939] {spark_submit.py:490} INFO - 23/05/28 14:16:50 ERROR TaskSchedulerImpl: Lost executor 2 on 172.23.0.7: Command exited with code 137
[2023-05-28 14:16:50,941] {spark_submit.py:490} INFO - 23/05/28 14:16:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20230528141619-0005/3 on worker-20230528055114-172.23.0.7-36005 (172.23.0.7:36005) with 1 core(s)
[2023-05-28 14:16:50,942] {spark_submit.py:490} INFO - 23/05/28 14:16:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20230528141619-0005/3 on hostPort 172.23.0.7:36005 with 1 core(s), 1024.0 MiB RAM
[2023-05-28 14:16:51,005] {spark_submit.py:490} INFO - 23/05/28 14:16:51 WARN TaskSetManager: Lost task 2.0 in stage 4.0 (TID 77) (172.23.0.7 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Command exited with code 137
[2023-05-28 14:16:51,020] {spark_submit.py:490} INFO - 23/05/28 14:16:51 INFO DAGScheduler: Executor lost: 2 (epoch 1)
[2023-05-28 14:16:51,022] {spark_submit.py:490} INFO - 23/05/28 14:16:51 INFO BlockManagerMasterEndpoint: Trying to remove executor 2 from BlockManagerMaster.
[2023-05-28 14:16:51,024] {spark_submit.py:490} INFO - 23/05/28 14:16:51 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(2, 172.23.0.7, 33561, None)
[2023-05-28 14:16:51,027] {spark_submit.py:490} INFO - 23/05/28 14:16:51 INFO BlockManagerMaster: Removed 2 successfully in removeExecutor
[2023-05-28 14:16:51,027] {spark_submit.py:490} INFO - 23/05/28 14:16:51 INFO DAGScheduler: Shuffle files lost for executor: 2 (epoch 1)
[2023-05-28 14:16:51,040] {spark_submit.py:490} INFO - 23/05/28 14:16:51 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230528141619-0005/3 is now RUNNING
[2023-05-28 14:16:55,209] {spark_submit.py:490} INFO - 23/05/28 14:16:55 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230528141619-0005/0 is now EXITED (Command exited with code 137)
[2023-05-28 14:16:55,210] {spark_submit.py:490} INFO - 23/05/28 14:16:55 INFO StandaloneSchedulerBackend: Executor app-20230528141619-0005/0 removed: Command exited with code 137
[2023-05-28 14:16:55,210] {spark_submit.py:490} INFO - 23/05/28 14:16:55 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20230528141619-0005/4 on worker-20230528055114-172.23.0.4-41329 (172.23.0.4:41329) with 1 core(s)
[2023-05-28 14:16:55,210] {spark_submit.py:490} INFO - 23/05/28 14:16:55 INFO StandaloneSchedulerBackend: Granted executor ID app-20230528141619-0005/4 on hostPort 172.23.0.4:41329 with 1 core(s), 1024.0 MiB RAM
[2023-05-28 14:16:55,211] {spark_submit.py:490} INFO - 23/05/28 14:16:55 ERROR TaskSchedulerImpl: Lost executor 0 on 172.23.0.4: Command exited with code 137
[2023-05-28 14:16:55,211] {spark_submit.py:490} INFO - 23/05/28 14:16:55 WARN TaskSetManager: Lost task 0.0 in stage 4.0 (TID 75) (172.23.0.4 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Command exited with code 137
[2023-05-28 14:16:55,212] {spark_submit.py:490} INFO - 23/05/28 14:16:55 INFO DAGScheduler: Executor lost: 0 (epoch 2)
[2023-05-28 14:16:55,212] {spark_submit.py:490} INFO - 23/05/28 14:16:55 INFO BlockManagerMasterEndpoint: Trying to remove executor 0 from BlockManagerMaster.
[2023-05-28 14:16:55,213] {spark_submit.py:490} INFO - 23/05/28 14:16:55 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(0, 172.23.0.4, 42027, None)
[2023-05-28 14:16:55,213] {spark_submit.py:490} INFO - 23/05/28 14:16:55 INFO BlockManagerMaster: Removed 0 successfully in removeExecutor
[2023-05-28 14:16:55,214] {spark_submit.py:490} INFO - 23/05/28 14:16:55 INFO DAGScheduler: Shuffle files lost for executor: 0 (epoch 2)
[2023-05-28 14:16:55,283] {spark_submit.py:490} INFO - 23/05/28 14:16:55 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230528141619-0005/4 is now RUNNING
[2023-05-28 14:16:56,808] {spark_submit.py:490} INFO - 23/05/28 14:16:56 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.23.0.7:35680) with ID 3,  ResourceProfileId 0
[2023-05-28 14:16:56,884] {spark_submit.py:490} INFO - 23/05/28 14:16:56 INFO BlockManagerMasterEndpoint: Registering block manager 172.23.0.7:36327 with 434.4 MiB RAM, BlockManagerId(3, 172.23.0.7, 36327, None)
[2023-05-28 14:16:56,949] {spark_submit.py:490} INFO - 23/05/28 14:16:56 INFO TaskSetManager: Starting task 0.1 in stage 4.0 (TID 78) (172.23.0.7, executor 3, partition 0, NODE_LOCAL, 7367 bytes)
[2023-05-28 14:16:57,184] {spark_submit.py:490} INFO - 23/05/28 14:16:57 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.23.0.7:36327 (size: 90.0 KiB, free: 434.3 MiB)
[2023-05-28 14:16:57,605] {spark_submit.py:490} INFO - 23/05/28 14:16:57 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.23.0.7:35680
[2023-05-28 14:16:57,637] {spark_submit.py:490} INFO - 23/05/28 14:16:57 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.23.0.4:53126) with ID 4,  ResourceProfileId 0
[2023-05-28 14:16:57,661] {spark_submit.py:490} INFO - 23/05/28 14:16:57 INFO TaskSetManager: Starting task 2.1 in stage 4.0 (TID 79) (172.23.0.7, executor 3, partition 2, NODE_LOCAL, 7367 bytes)
[2023-05-28 14:16:57,667] {spark_submit.py:490} INFO - 23/05/28 14:16:57 WARN TaskSetManager: Lost task 0.1 in stage 4.0 (TID 78) (172.23.0.7 executor 3): FetchFailed(null, shuffleId=0, mapIndex=-1, mapId=-1, reduceId=0, message=
[2023-05-28 14:16:57,668] {spark_submit.py:490} INFO - org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 partition 0
[2023-05-28 14:16:57,668] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1718)
[2023-05-28 14:16:57,669] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:1665)
[2023-05-28 14:16:57,669] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:1664)
[2023-05-28 14:16:57,669] {spark_submit.py:490} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-05-28 14:16:57,670] {spark_submit.py:490} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-05-28 14:16:57,670] {spark_submit.py:490} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-05-28 14:16:57,671] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1664)
[2023-05-28 14:16:57,671] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1306)
[2023-05-28 14:16:57,671] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1268)
[2023-05-28 14:16:57,672] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)
[2023-05-28 14:16:57,672] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)
[2023-05-28 14:16:57,672] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)
[2023-05-28 14:16:57,673] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)
[2023-05-28 14:16:57,673] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:200)
[2023-05-28 14:16:57,673] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:16:57,674] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:16:57,674] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:16:57,675] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:16:57,675] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:16:57,676] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:16:57,676] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:16:57,677] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:16:57,677] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:16:57,677] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:16:57,678] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:16:57,678] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:16:57,679] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:16:57,679] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:16:57,679] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:16:57,680] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:16:57,680] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:16:57,680] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:16:57,681] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:16:57,681] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:16:57,682] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
[2023-05-28 14:16:57,682] {spark_submit.py:490} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
[2023-05-28 14:16:57,683] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:139)
[2023-05-28 14:16:57,683] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
[2023-05-28 14:16:57,683] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
[2023-05-28 14:16:57,684] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
[2023-05-28 14:16:57,684] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2023-05-28 14:16:57,684] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2023-05-28 14:16:57,685] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:833)
[2023-05-28 14:16:57,685] {spark_submit.py:490} INFO - 
[2023-05-28 14:16:57,686] {spark_submit.py:490} INFO - )
[2023-05-28 14:16:57,686] {spark_submit.py:490} INFO - 23/05/28 14:16:57 INFO DAGScheduler: Marking ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0) as failed due to a fetch failure from ShuffleMapStage 3 (parquet at NativeMethodAccessorImpl.java:0)
[2023-05-28 14:16:57,686] {spark_submit.py:490} INFO - 23/05/28 14:16:57 INFO DAGScheduler: ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0) failed in 19.829 s due to org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 partition 0
[2023-05-28 14:16:57,687] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1718)
[2023-05-28 14:16:57,687] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:1665)
[2023-05-28 14:16:57,687] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:1664)
[2023-05-28 14:16:57,688] {spark_submit.py:490} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-05-28 14:16:57,689] {spark_submit.py:490} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-05-28 14:16:57,689] {spark_submit.py:490} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-05-28 14:16:57,689] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1664)
[2023-05-28 14:16:57,690] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1306)
[2023-05-28 14:16:57,690] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1268)
[2023-05-28 14:16:57,690] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)
[2023-05-28 14:16:57,691] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)
[2023-05-28 14:16:57,691] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)
[2023-05-28 14:16:57,691] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)
[2023-05-28 14:16:57,692] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:200)
[2023-05-28 14:16:57,692] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:16:57,693] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:16:57,693] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:16:57,693] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:16:57,694] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:16:57,694] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:16:57,694] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:16:57,695] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:16:57,695] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:16:57,695] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:16:57,696] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:16:57,696] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:16:57,696] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:16:57,697] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:16:57,697] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:16:57,697] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:16:57,698] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:16:57,698] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:16:57,698] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:16:57,699] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:16:57,699] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
[2023-05-28 14:16:57,700] {spark_submit.py:490} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
[2023-05-28 14:16:57,700] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:139)
[2023-05-28 14:16:57,701] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
[2023-05-28 14:16:57,701] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
[2023-05-28 14:16:57,702] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
[2023-05-28 14:16:57,702] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2023-05-28 14:16:57,702] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2023-05-28 14:16:57,702] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:833)
[2023-05-28 14:16:57,703] {spark_submit.py:490} INFO - 
[2023-05-28 14:16:57,703] {spark_submit.py:490} INFO - 23/05/28 14:16:57 INFO DAGScheduler: Resubmitting ShuffleMapStage 3 (parquet at NativeMethodAccessorImpl.java:0) and ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0) due to fetch failure
[2023-05-28 14:16:57,703] {spark_submit.py:490} INFO - 23/05/28 14:16:57 INFO TaskSetManager: task 0.1 in stage 4.0 (TID 78) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
[2023-05-28 14:16:57,704] {spark_submit.py:490} INFO - 23/05/28 14:16:57 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.23.0.7:35680
[2023-05-28 14:16:57,704] {spark_submit.py:490} INFO - 23/05/28 14:16:57 WARN TaskSetManager: Lost task 2.1 in stage 4.0 (TID 79) (172.23.0.7 executor 3): FetchFailed(null, shuffleId=0, mapIndex=-1, mapId=-1, reduceId=85, message=
[2023-05-28 14:16:57,704] {spark_submit.py:490} INFO - org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 partition 85
[2023-05-28 14:16:57,705] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1718)
[2023-05-28 14:16:57,705] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:1665)
[2023-05-28 14:16:57,705] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:1664)
[2023-05-28 14:16:57,706] {spark_submit.py:490} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-05-28 14:16:57,706] {spark_submit.py:490} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-05-28 14:16:57,706] {spark_submit.py:490} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-05-28 14:16:57,707] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1664)
[2023-05-28 14:16:57,707] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1306)
[2023-05-28 14:16:57,707] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1268)
[2023-05-28 14:16:57,708] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)
[2023-05-28 14:16:57,708] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)
[2023-05-28 14:16:57,708] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)
[2023-05-28 14:16:57,708] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)
[2023-05-28 14:16:57,709] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:200)
[2023-05-28 14:16:57,709] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:16:57,709] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:16:57,710] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:16:57,710] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:16:57,710] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:16:57,711] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:16:57,711] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:16:57,711] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:16:57,711] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:16:57,712] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:16:57,712] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:16:57,712] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:16:57,713] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:16:57,713] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:16:57,714] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:16:57,714] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:16:57,715] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:16:57,715] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:16:57,715] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:16:57,716] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:16:57,717] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
[2023-05-28 14:16:57,717] {spark_submit.py:490} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
[2023-05-28 14:16:57,718] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:139)
[2023-05-28 14:16:57,718] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
[2023-05-28 14:16:57,719] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
[2023-05-28 14:16:57,719] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
[2023-05-28 14:16:57,720] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2023-05-28 14:16:57,720] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2023-05-28 14:16:57,721] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:833)
[2023-05-28 14:16:57,722] {spark_submit.py:490} INFO - 
[2023-05-28 14:16:57,722] {spark_submit.py:490} INFO - )
[2023-05-28 14:16:57,723] {spark_submit.py:490} INFO - 23/05/28 14:16:57 INFO TaskSetManager: task 2.1 in stage 4.0 (TID 79) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
[2023-05-28 14:16:57,740] {spark_submit.py:490} INFO - 23/05/28 14:16:57 INFO BlockManagerMasterEndpoint: Registering block manager 172.23.0.4:43641 with 434.4 MiB RAM, BlockManagerId(4, 172.23.0.4, 43641, None)
[2023-05-28 14:16:57,873] {spark_submit.py:490} INFO - 23/05/28 14:16:57 INFO DAGScheduler: Resubmitting failed stages
[2023-05-28 14:16:57,874] {spark_submit.py:490} INFO - 23/05/28 14:16:57 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[8] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-28 14:16:57,877] {spark_submit.py:490} INFO - 23/05/28 14:16:57 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 21.8 KiB, free 433.8 MiB)
[2023-05-28 14:16:57,883] {spark_submit.py:490} INFO - 23/05/28 14:16:57 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 433.8 MiB)
[2023-05-28 14:16:57,885] {spark_submit.py:490} INFO - 23/05/28 14:16:57 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 58e417c56c9d:44457 (size: 8.6 KiB, free: 434.3 MiB)
[2023-05-28 14:16:57,890] {spark_submit.py:490} INFO - 23/05/28 14:16:57 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535
[2023-05-28 14:16:57,891] {spark_submit.py:490} INFO - 23/05/28 14:16:57 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[8] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 2, 3))
[2023-05-28 14:16:57,894] {spark_submit.py:490} INFO - 23/05/28 14:16:57 INFO TaskSchedulerImpl: Adding task set 3.1 with 3 tasks resource profile 0
[2023-05-28 14:16:57,894] {spark_submit.py:490} INFO - 23/05/28 14:16:57 INFO TaskSetManager: Starting task 0.0 in stage 3.1 (TID 80) (172.23.0.4, executor 4, partition 0, PROCESS_LOCAL, 11205 bytes)
[2023-05-28 14:16:57,895] {spark_submit.py:490} INFO - 23/05/28 14:16:57 INFO TaskSetManager: Starting task 1.0 in stage 3.1 (TID 81) (172.23.0.7, executor 3, partition 2, PROCESS_LOCAL, 13085 bytes)
[2023-05-28 14:16:57,908] {spark_submit.py:490} INFO - 23/05/28 14:16:57 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.23.0.7:36327 (size: 8.6 KiB, free: 434.3 MiB)
[2023-05-28 14:16:58,029] {spark_submit.py:490} INFO - 23/05/28 14:16:58 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.23.0.4:43641 (size: 8.6 KiB, free: 434.4 MiB)
[2023-05-28 14:16:58,389] {spark_submit.py:490} INFO - 23/05/28 14:16:58 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.23.0.7:36327 (size: 35.0 KiB, free: 434.3 MiB)
[2023-05-28 14:16:58,778] {spark_submit.py:490} INFO - 23/05/28 14:16:58 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.23.0.4:43641 (size: 35.0 KiB, free: 434.4 MiB)
[2023-05-28 14:17:03,843] {spark_submit.py:490} INFO - 23/05/28 14:17:03 INFO TaskSetManager: Starting task 2.0 in stage 3.1 (TID 82) (172.23.0.7, executor 3, partition 3, PROCESS_LOCAL, 8009 bytes)
[2023-05-28 14:17:03,846] {spark_submit.py:490} INFO - 23/05/28 14:17:03 INFO TaskSetManager: Finished task 1.0 in stage 3.1 (TID 81) in 5961 ms on 172.23.0.7 (executor 3) (1/3)
[2023-05-28 14:17:03,993] {spark_submit.py:490} INFO - 23/05/28 14:17:03 INFO TaskSetManager: Finished task 2.0 in stage 3.1 (TID 82) in 152 ms on 172.23.0.7 (executor 3) (2/3)
[2023-05-28 14:17:04,117] {spark_submit.py:490} INFO - 23/05/28 14:17:04 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230528141619-0005/1 is now EXITED (Command exited with code 137)
[2023-05-28 14:17:04,117] {spark_submit.py:490} INFO - 23/05/28 14:17:04 INFO StandaloneSchedulerBackend: Executor app-20230528141619-0005/1 removed: Command exited with code 137
[2023-05-28 14:17:04,118] {spark_submit.py:490} INFO - 23/05/28 14:17:04 ERROR TaskSchedulerImpl: Lost executor 1 on 172.23.0.3: Command exited with code 137
[2023-05-28 14:17:04,119] {spark_submit.py:490} INFO - 23/05/28 14:17:04 WARN TaskSetManager: Lost task 1.0 in stage 4.0 (TID 76) (172.23.0.3 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Command exited with code 137
[2023-05-28 14:17:04,120] {spark_submit.py:490} INFO - 23/05/28 14:17:04 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-05-28 14:17:04,121] {spark_submit.py:490} INFO - 23/05/28 14:17:04 INFO DAGScheduler: Executor lost: 1 (epoch 3)
[2023-05-28 14:17:04,121] {spark_submit.py:490} INFO - 23/05/28 14:17:04 INFO BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
[2023-05-28 14:17:04,122] {spark_submit.py:490} INFO - 23/05/28 14:17:04 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(1, 172.23.0.3, 39979, None)
[2023-05-28 14:17:04,122] {spark_submit.py:490} INFO - 23/05/28 14:17:04 INFO BlockManagerMaster: Removed 1 successfully in removeExecutor
[2023-05-28 14:17:04,123] {spark_submit.py:490} INFO - 23/05/28 14:17:04 INFO DAGScheduler: Shuffle files lost for executor: 1 (epoch 3)
[2023-05-28 14:17:04,123] {spark_submit.py:490} INFO - 23/05/28 14:17:04 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20230528141619-0005/5 on worker-20230528055114-172.23.0.3-42839 (172.23.0.3:42839) with 1 core(s)
[2023-05-28 14:17:04,123] {spark_submit.py:490} INFO - 23/05/28 14:17:04 INFO StandaloneSchedulerBackend: Granted executor ID app-20230528141619-0005/5 on hostPort 172.23.0.3:42839 with 1 core(s), 1024.0 MiB RAM
[2023-05-28 14:17:04,179] {spark_submit.py:490} INFO - 23/05/28 14:17:04 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230528141619-0005/5 is now RUNNING
[2023-05-28 14:17:06,983] {spark_submit.py:490} INFO - 23/05/28 14:17:06 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.23.0.3:55826) with ID 5,  ResourceProfileId 0
[2023-05-28 14:17:07,078] {spark_submit.py:490} INFO - 23/05/28 14:17:07 INFO BlockManagerMasterEndpoint: Registering block manager 172.23.0.3:46433 with 434.4 MiB RAM, BlockManagerId(5, 172.23.0.3, 46433, None)
[2023-05-28 14:17:07,371] {spark_submit.py:490} INFO - 23/05/28 14:17:07 INFO TaskSetManager: Finished task 0.0 in stage 3.1 (TID 80) in 9489 ms on 172.23.0.4 (executor 4) (3/3)
[2023-05-28 14:17:07,374] {spark_submit.py:490} INFO - 23/05/28 14:17:07 INFO TaskSchedulerImpl: Removed TaskSet 3.1, whose tasks have all completed, from pool
[2023-05-28 14:17:07,375] {spark_submit.py:490} INFO - 23/05/28 14:17:07 INFO DAGScheduler: ShuffleMapStage 3 (parquet at NativeMethodAccessorImpl.java:0) finished in 9.498 s
[2023-05-28 14:17:07,375] {spark_submit.py:490} INFO - 23/05/28 14:17:07 INFO DAGScheduler: looking for newly runnable stages
[2023-05-28 14:17:07,376] {spark_submit.py:490} INFO - 23/05/28 14:17:07 INFO DAGScheduler: running: Set()
[2023-05-28 14:17:07,376] {spark_submit.py:490} INFO - 23/05/28 14:17:07 INFO DAGScheduler: waiting: Set(ResultStage 4)
[2023-05-28 14:17:07,377] {spark_submit.py:490} INFO - 23/05/28 14:17:07 INFO DAGScheduler: failed: Set()
[2023-05-28 14:17:07,381] {spark_submit.py:490} INFO - 23/05/28 14:17:07 INFO DAGScheduler: Resubmitting ShuffleMapStage 3 (parquet at NativeMethodAccessorImpl.java:0) because some of its tasks had failed: 1
[2023-05-28 14:17:07,382] {spark_submit.py:490} INFO - 23/05/28 14:17:07 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[8] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-28 14:17:07,385] {spark_submit.py:490} INFO - 23/05/28 14:17:07 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 21.8 KiB, free 433.8 MiB)
[2023-05-28 14:17:07,390] {spark_submit.py:490} INFO - 23/05/28 14:17:07 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 433.7 MiB)
[2023-05-28 14:17:07,391] {spark_submit.py:490} INFO - 23/05/28 14:17:07 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 58e417c56c9d:44457 (size: 8.6 KiB, free: 434.3 MiB)
[2023-05-28 14:17:07,391] {spark_submit.py:490} INFO - 23/05/28 14:17:07 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1535
[2023-05-28 14:17:07,392] {spark_submit.py:490} INFO - 23/05/28 14:17:07 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[8] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1))
[2023-05-28 14:17:07,392] {spark_submit.py:490} INFO - 23/05/28 14:17:07 INFO TaskSchedulerImpl: Adding task set 3.2 with 1 tasks resource profile 0
[2023-05-28 14:17:07,393] {spark_submit.py:490} INFO - 23/05/28 14:17:07 INFO TaskSetManager: Starting task 0.0 in stage 3.2 (TID 83) (172.23.0.7, executor 3, partition 1, PROCESS_LOCAL, 12145 bytes)
[2023-05-28 14:17:07,415] {spark_submit.py:490} INFO - 23/05/28 14:17:07 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.23.0.7:36327 (size: 8.6 KiB, free: 434.3 MiB)
[2023-05-28 14:17:09,728] {spark_submit.py:490} INFO - 23/05/28 14:17:09 INFO TaskSetManager: Finished task 0.0 in stage 3.2 (TID 83) in 2335 ms on 172.23.0.7 (executor 3) (1/1)
[2023-05-28 14:17:09,730] {spark_submit.py:490} INFO - 23/05/28 14:17:09 INFO TaskSchedulerImpl: Removed TaskSet 3.2, whose tasks have all completed, from pool
[2023-05-28 14:17:09,731] {spark_submit.py:490} INFO - 23/05/28 14:17:09 INFO DAGScheduler: ShuffleMapStage 3 (parquet at NativeMethodAccessorImpl.java:0) finished in 2.346 s
[2023-05-28 14:17:09,731] {spark_submit.py:490} INFO - 23/05/28 14:17:09 INFO DAGScheduler: looking for newly runnable stages
[2023-05-28 14:17:09,732] {spark_submit.py:490} INFO - 23/05/28 14:17:09 INFO DAGScheduler: running: Set()
[2023-05-28 14:17:09,732] {spark_submit.py:490} INFO - 23/05/28 14:17:09 INFO DAGScheduler: waiting: Set(ResultStage 4)
[2023-05-28 14:17:09,733] {spark_submit.py:490} INFO - 23/05/28 14:17:09 INFO DAGScheduler: failed: Set()
[2023-05-28 14:17:09,733] {spark_submit.py:490} INFO - 23/05/28 14:17:09 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[15] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-28 14:17:09,734] {spark_submit.py:490} INFO - 23/05/28 14:17:09 INFO OutputCommitCoordinator: Reusing state from previous attempt of stage 4.
[2023-05-28 14:17:09,748] {spark_submit.py:490} INFO - 23/05/28 14:17:09 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 247.9 KiB, free 433.5 MiB)
[2023-05-28 14:17:09,771] {spark_submit.py:490} INFO - 23/05/28 14:17:09 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 90.0 KiB, free 433.4 MiB)
[2023-05-28 14:17:09,799] {spark_submit.py:490} INFO - 23/05/28 14:17:09 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 58e417c56c9d:44457 (size: 90.0 KiB, free: 434.2 MiB)
[2023-05-28 14:17:09,800] {spark_submit.py:490} INFO - 23/05/28 14:17:09 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.23.0.4:43641 in memory (size: 8.6 KiB, free: 434.4 MiB)
[2023-05-28 14:17:09,800] {spark_submit.py:490} INFO - 23/05/28 14:17:09 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1535
[2023-05-28 14:17:09,801] {spark_submit.py:490} INFO - 23/05/28 14:17:09 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 58e417c56c9d:44457 in memory (size: 8.6 KiB, free: 434.2 MiB)
[2023-05-28 14:17:09,801] {spark_submit.py:490} INFO - 23/05/28 14:17:09 INFO DAGScheduler: Submitting 5 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
[2023-05-28 14:17:09,803] {spark_submit.py:490} INFO - 23/05/28 14:17:09 INFO TaskSchedulerImpl: Adding task set 4.1 with 5 tasks resource profile 0
[2023-05-28 14:17:09,808] {spark_submit.py:490} INFO - 23/05/28 14:17:09 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.23.0.7:36327 in memory (size: 8.6 KiB, free: 434.3 MiB)
[2023-05-28 14:17:09,809] {spark_submit.py:490} INFO - 23/05/28 14:17:09 INFO TaskSetManager: Starting task 0.0 in stage 4.1 (TID 84) (172.23.0.7, executor 3, partition 0, NODE_LOCAL, 7367 bytes)
[2023-05-28 14:17:09,810] {spark_submit.py:490} INFO - 23/05/28 14:17:09 INFO TaskSetManager: Starting task 1.0 in stage 4.1 (TID 85) (172.23.0.4, executor 4, partition 1, NODE_LOCAL, 7367 bytes)
[2023-05-28 14:17:09,835] {spark_submit.py:490} INFO - 23/05/28 14:17:09 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.23.0.7:36327 (size: 90.0 KiB, free: 434.2 MiB)
[2023-05-28 14:17:09,854] {spark_submit.py:490} INFO - 23/05/28 14:17:09 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.23.0.4:43641 (size: 90.0 KiB, free: 434.3 MiB)
[2023-05-28 14:17:09,855] {spark_submit.py:490} INFO - 23/05/28 14:17:09 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.23.0.7:36327 in memory (size: 90.0 KiB, free: 434.3 MiB)
[2023-05-28 14:17:09,867] {spark_submit.py:490} INFO - 23/05/28 14:17:09 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 58e417c56c9d:44457 in memory (size: 90.0 KiB, free: 434.3 MiB)
[2023-05-28 14:17:09,879] {spark_submit.py:490} INFO - 23/05/28 14:17:09 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.23.0.7:35680
[2023-05-28 14:17:09,980] {spark_submit.py:490} INFO - 23/05/28 14:17:09 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 58e417c56c9d:44457 in memory (size: 8.6 KiB, free: 434.3 MiB)
[2023-05-28 14:17:10,121] {spark_submit.py:490} INFO - 23/05/28 14:17:10 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.23.0.4:53126
[2023-05-28 14:17:13,474] {spark_submit.py:490} INFO - 23/05/28 14:17:13 INFO TaskSetManager: Starting task 2.0 in stage 4.1 (TID 86) (172.23.0.3, executor 5, partition 2, ANY, 7367 bytes)
[2023-05-28 14:17:13,653] {spark_submit.py:490} INFO - 23/05/28 14:17:13 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.23.0.3:46433 (size: 90.0 KiB, free: 434.3 MiB)
[2023-05-28 14:17:14,091] {spark_submit.py:490} INFO - 23/05/28 14:17:14 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.23.0.3:55826
[2023-05-28 14:17:16,020] {spark_submit.py:490} INFO - 23/05/28 14:17:16 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230528141619-0005/4 is now EXITED (Command exited with code 137)
[2023-05-28 14:17:16,021] {spark_submit.py:490} INFO - 23/05/28 14:17:16 INFO StandaloneSchedulerBackend: Executor app-20230528141619-0005/4 removed: Command exited with code 137
[2023-05-28 14:17:16,022] {spark_submit.py:490} INFO - 23/05/28 14:17:16 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20230528141619-0005/6 on worker-20230528055114-172.23.0.4-41329 (172.23.0.4:41329) with 1 core(s)
[2023-05-28 14:17:16,023] {spark_submit.py:490} INFO - 23/05/28 14:17:16 INFO StandaloneSchedulerBackend: Granted executor ID app-20230528141619-0005/6 on hostPort 172.23.0.4:41329 with 1 core(s), 1024.0 MiB RAM
[2023-05-28 14:17:16,024] {spark_submit.py:490} INFO - 23/05/28 14:17:16 ERROR TaskSchedulerImpl: Lost executor 4 on 172.23.0.4: Command exited with code 137
[2023-05-28 14:17:16,024] {spark_submit.py:490} INFO - 23/05/28 14:17:16 WARN TaskSetManager: Lost task 1.0 in stage 4.1 (TID 85) (172.23.0.4 executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Command exited with code 137
[2023-05-28 14:17:16,025] {spark_submit.py:490} INFO - 23/05/28 14:17:16 INFO DAGScheduler: Executor lost: 4 (epoch 6)
[2023-05-28 14:17:16,025] {spark_submit.py:490} INFO - 23/05/28 14:17:16 INFO BlockManagerMasterEndpoint: Trying to remove executor 4 from BlockManagerMaster.
[2023-05-28 14:17:16,025] {spark_submit.py:490} INFO - 23/05/28 14:17:16 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(4, 172.23.0.4, 43641, None)
[2023-05-28 14:17:16,026] {spark_submit.py:490} INFO - 23/05/28 14:17:16 INFO BlockManagerMaster: Removed 4 successfully in removeExecutor
[2023-05-28 14:17:16,026] {spark_submit.py:490} INFO - 23/05/28 14:17:16 INFO DAGScheduler: Shuffle files lost for executor: 4 (epoch 6)
[2023-05-28 14:17:16,078] {spark_submit.py:490} INFO - 23/05/28 14:17:16 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230528141619-0005/6 is now RUNNING
[2023-05-28 14:17:16,865] {spark_submit.py:490} INFO - 23/05/28 14:17:16 INFO TaskSetManager: Starting task 1.1 in stage 4.1 (TID 87) (172.23.0.3, executor 5, partition 1, ANY, 7367 bytes)
[2023-05-28 14:17:16,866] {spark_submit.py:490} INFO - 23/05/28 14:17:16 WARN TaskSetManager: Lost task 2.0 in stage 4.1 (TID 86) (172.23.0.3 executor 5): FetchFailed(BlockManagerId(4, 172.23.0.4, 43641, None), shuffleId=0, mapIndex=0, mapId=80, reduceId=85, message=
[2023-05-28 14:17:16,867] {spark_submit.py:490} INFO - org.apache.spark.shuffle.FetchFailedException
[2023-05-28 14:17:16,868] {spark_submit.py:490} INFO - at org.apache.spark.errors.SparkCoreErrors$.fetchFailedError(SparkCoreErrors.scala:437)
[2023-05-28 14:17:16,868] {spark_submit.py:490} INFO - at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1235)
[2023-05-28 14:17:16,869] {spark_submit.py:490} INFO - at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:971)
[2023-05-28 14:17:16,869] {spark_submit.py:490} INFO - at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:86)
[2023-05-28 14:17:16,869] {spark_submit.py:490} INFO - at org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)
[2023-05-28 14:17:16,870] {spark_submit.py:490} INFO - at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
[2023-05-28 14:17:16,870] {spark_submit.py:490} INFO - at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
[2023-05-28 14:17:16,871] {spark_submit.py:490} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-05-28 14:17:16,871] {spark_submit.py:490} INFO - at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)
[2023-05-28 14:17:16,872] {spark_submit.py:490} INFO - at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2023-05-28 14:17:16,872] {spark_submit.py:490} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-05-28 14:17:16,873] {spark_submit.py:490} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)
[2023-05-28 14:17:16,873] {spark_submit.py:490} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
[2023-05-28 14:17:16,874] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2023-05-28 14:17:16,874] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
[2023-05-28 14:17:16,875] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:118)
[2023-05-28 14:17:16,875] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.window.WindowExec$$anon$1.<init>(WindowExec.scala:127)
[2023-05-28 14:17:16,876] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.window.WindowExec.$anonfun$doExecute$3(WindowExec.scala:107)
[2023-05-28 14:17:16,876] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)
[2023-05-28 14:17:16,876] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)
[2023-05-28 14:17:16,877] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:16,877] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:16,878] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:16,878] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:16,879] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:16,879] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:16,879] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:16,880] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:16,880] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:16,880] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:16,881] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:16,881] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:16,881] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:16,881] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:16,881] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:16,882] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
[2023-05-28 14:17:16,882] {spark_submit.py:490} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
[2023-05-28 14:17:16,882] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:139)
[2023-05-28 14:17:16,882] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
[2023-05-28 14:17:16,883] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
[2023-05-28 14:17:16,883] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
[2023-05-28 14:17:16,883] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2023-05-28 14:17:16,883] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2023-05-28 14:17:16,884] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:833)
[2023-05-28 14:17:16,884] {spark_submit.py:490} INFO - Caused by: org.apache.spark.ExecutorDeadException: The relative remote executor(Id: 4), which maintains the block data to fetch is dead.
[2023-05-28 14:17:16,884] {spark_submit.py:490} INFO - at org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:140)
[2023-05-28 14:17:16,884] {spark_submit.py:490} INFO - at org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)
[2023-05-28 14:17:16,885] {spark_submit.py:490} INFO - at org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:152)
[2023-05-28 14:17:16,885] {spark_submit.py:490} INFO - at org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:150)
[2023-05-28 14:17:16,885] {spark_submit.py:490} INFO - at org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:374)
[2023-05-28 14:17:16,885] {spark_submit.py:490} INFO - at org.apache.spark.storage.ShuffleBlockFetcherIterator.send$1(ShuffleBlockFetcherIterator.scala:1202)
[2023-05-28 14:17:16,885] {spark_submit.py:490} INFO - at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchUpToMaxBytes(ShuffleBlockFetcherIterator.scala:1194)
[2023-05-28 14:17:16,886] {spark_submit.py:490} INFO - at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:1067)
[2023-05-28 14:17:16,886] {spark_submit.py:490} INFO - ... 41 more
[2023-05-28 14:17:16,887] {spark_submit.py:490} INFO - 
[2023-05-28 14:17:16,887] {spark_submit.py:490} INFO - )
[2023-05-28 14:17:16,887] {spark_submit.py:490} INFO - 23/05/28 14:17:16 INFO DAGScheduler: Marking ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0) as failed due to a fetch failure from ShuffleMapStage 3 (parquet at NativeMethodAccessorImpl.java:0)
[2023-05-28 14:17:16,888] {spark_submit.py:490} INFO - 23/05/28 14:17:16 INFO DAGScheduler: ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0) failed in 7.130 s due to org.apache.spark.shuffle.FetchFailedException
[2023-05-28 14:17:16,888] {spark_submit.py:490} INFO - at org.apache.spark.errors.SparkCoreErrors$.fetchFailedError(SparkCoreErrors.scala:437)
[2023-05-28 14:17:16,888] {spark_submit.py:490} INFO - at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1235)
[2023-05-28 14:17:16,889] {spark_submit.py:490} INFO - at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:971)
[2023-05-28 14:17:16,890] {spark_submit.py:490} INFO - at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:86)
[2023-05-28 14:17:16,891] {spark_submit.py:490} INFO - at org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)
[2023-05-28 14:17:16,891] {spark_submit.py:490} INFO - at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
[2023-05-28 14:17:16,891] {spark_submit.py:490} INFO - at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
[2023-05-28 14:17:16,892] {spark_submit.py:490} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-05-28 14:17:16,892] {spark_submit.py:490} INFO - at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)
[2023-05-28 14:17:16,892] {spark_submit.py:490} INFO - at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2023-05-28 14:17:16,892] {spark_submit.py:490} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-05-28 14:17:16,893] {spark_submit.py:490} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)
[2023-05-28 14:17:16,893] {spark_submit.py:490} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
[2023-05-28 14:17:16,893] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2023-05-28 14:17:16,893] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
[2023-05-28 14:17:16,894] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:118)
[2023-05-28 14:17:16,894] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.window.WindowExec$$anon$1.<init>(WindowExec.scala:127)
[2023-05-28 14:17:16,894] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.window.WindowExec.$anonfun$doExecute$3(WindowExec.scala:107)
[2023-05-28 14:17:16,894] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)
[2023-05-28 14:17:16,895] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)
[2023-05-28 14:17:16,895] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:16,895] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:16,895] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:16,896] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:16,896] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:16,896] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:16,897] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:16,897] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:16,898] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:16,898] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:16,898] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:16,899] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:16,899] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:16,899] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:16,900] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:16,900] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
[2023-05-28 14:17:16,900] {spark_submit.py:490} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
[2023-05-28 14:17:16,900] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:139)
[2023-05-28 14:17:16,901] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
[2023-05-28 14:17:16,901] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
[2023-05-28 14:17:16,901] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
[2023-05-28 14:17:16,902] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2023-05-28 14:17:16,902] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2023-05-28 14:17:16,902] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:833)
[2023-05-28 14:17:16,903] {spark_submit.py:490} INFO - Caused by: org.apache.spark.ExecutorDeadException: The relative remote executor(Id: 4), which maintains the block data to fetch is dead.
[2023-05-28 14:17:16,903] {spark_submit.py:490} INFO - at org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:140)
[2023-05-28 14:17:16,904] {spark_submit.py:490} INFO - at org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)
[2023-05-28 14:17:16,904] {spark_submit.py:490} INFO - at org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:152)
[2023-05-28 14:17:16,904] {spark_submit.py:490} INFO - at org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:150)
[2023-05-28 14:17:16,905] {spark_submit.py:490} INFO - at org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:374)
[2023-05-28 14:17:16,905] {spark_submit.py:490} INFO - at org.apache.spark.storage.ShuffleBlockFetcherIterator.send$1(ShuffleBlockFetcherIterator.scala:1202)
[2023-05-28 14:17:16,905] {spark_submit.py:490} INFO - at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchUpToMaxBytes(ShuffleBlockFetcherIterator.scala:1194)
[2023-05-28 14:17:16,905] {spark_submit.py:490} INFO - at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:1067)
[2023-05-28 14:17:16,906] {spark_submit.py:490} INFO - ... 41 more
[2023-05-28 14:17:16,906] {spark_submit.py:490} INFO - 
[2023-05-28 14:17:16,906] {spark_submit.py:490} INFO - 23/05/28 14:17:16 INFO TaskSetManager: task 2.0 in stage 4.1 (TID 86) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
[2023-05-28 14:17:16,911] {spark_submit.py:490} INFO - 23/05/28 14:17:16 INFO DAGScheduler: Resubmitting ShuffleMapStage 3 (parquet at NativeMethodAccessorImpl.java:0) and ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0) due to fetch failure
[2023-05-28 14:17:17,069] {spark_submit.py:490} INFO - 23/05/28 14:17:17 WARN TaskSetManager: Lost task 1.1 in stage 4.1 (TID 87) (172.23.0.3 executor 5): FetchFailed(BlockManagerId(4, 172.23.0.4, 43641, None), shuffleId=0, mapIndex=0, mapId=80, reduceId=43, message=
[2023-05-28 14:17:17,070] {spark_submit.py:490} INFO - org.apache.spark.shuffle.FetchFailedException
[2023-05-28 14:17:17,071] {spark_submit.py:490} INFO - at org.apache.spark.errors.SparkCoreErrors$.fetchFailedError(SparkCoreErrors.scala:437)
[2023-05-28 14:17:17,071] {spark_submit.py:490} INFO - at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1235)
[2023-05-28 14:17:17,071] {spark_submit.py:490} INFO - at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:971)
[2023-05-28 14:17:17,072] {spark_submit.py:490} INFO - at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:86)
[2023-05-28 14:17:17,073] {spark_submit.py:490} INFO - at org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)
[2023-05-28 14:17:17,073] {spark_submit.py:490} INFO - at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
[2023-05-28 14:17:17,073] {spark_submit.py:490} INFO - at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
[2023-05-28 14:17:17,076] {spark_submit.py:490} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-05-28 14:17:17,077] {spark_submit.py:490} INFO - at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)
[2023-05-28 14:17:17,078] {spark_submit.py:490} INFO - at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2023-05-28 14:17:17,078] {spark_submit.py:490} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-05-28 14:17:17,079] {spark_submit.py:490} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)
[2023-05-28 14:17:17,079] {spark_submit.py:490} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
[2023-05-28 14:17:17,080] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2023-05-28 14:17:17,080] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
[2023-05-28 14:17:17,080] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:118)
[2023-05-28 14:17:17,080] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.window.WindowExec$$anon$1.<init>(WindowExec.scala:127)
[2023-05-28 14:17:17,083] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.window.WindowExec.$anonfun$doExecute$3(WindowExec.scala:107)
[2023-05-28 14:17:17,084] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)
[2023-05-28 14:17:17,084] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)
[2023-05-28 14:17:17,085] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:17,085] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:17,086] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:17,086] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:17,086] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:17,087] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:17,087] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:17,088] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:17,088] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:17,088] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:17,088] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:17,089] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:17,089] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:17,089] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:17,090] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:17,090] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
[2023-05-28 14:17:17,090] {spark_submit.py:490} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
[2023-05-28 14:17:17,092] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:139)
[2023-05-28 14:17:17,093] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
[2023-05-28 14:17:17,094] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
[2023-05-28 14:17:17,095] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
[2023-05-28 14:17:17,095] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2023-05-28 14:17:17,095] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2023-05-28 14:17:17,096] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:833)
[2023-05-28 14:17:17,096] {spark_submit.py:490} INFO - Caused by: org.apache.spark.ExecutorDeadException: The relative remote executor(Id: 4), which maintains the block data to fetch is dead.
[2023-05-28 14:17:17,098] {spark_submit.py:490} INFO - at org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:140)
[2023-05-28 14:17:17,099] {spark_submit.py:490} INFO - at org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)
[2023-05-28 14:17:17,099] {spark_submit.py:490} INFO - at org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:152)
[2023-05-28 14:17:17,099] {spark_submit.py:490} INFO - at org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:150)
[2023-05-28 14:17:17,100] {spark_submit.py:490} INFO - at org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:374)
[2023-05-28 14:17:17,100] {spark_submit.py:490} INFO - at org.apache.spark.storage.ShuffleBlockFetcherIterator.send$1(ShuffleBlockFetcherIterator.scala:1202)
[2023-05-28 14:17:17,100] {spark_submit.py:490} INFO - at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchUpToMaxBytes(ShuffleBlockFetcherIterator.scala:1194)
[2023-05-28 14:17:17,101] {spark_submit.py:490} INFO - at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:715)
[2023-05-28 14:17:17,101] {spark_submit.py:490} INFO - at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:194)
[2023-05-28 14:17:17,102] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:73)
[2023-05-28 14:17:17,102] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:232)
[2023-05-28 14:17:17,103] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:17,104] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:17,104] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:17,105] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:17,105] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:17,105] {spark_submit.py:490} INFO - ... 24 more
[2023-05-28 14:17:17,109] {spark_submit.py:490} INFO - 
[2023-05-28 14:17:17,109] {spark_submit.py:490} INFO - )
[2023-05-28 14:17:17,110] {spark_submit.py:490} INFO - 23/05/28 14:17:17 INFO TaskSetManager: task 1.1 in stage 4.1 (TID 87) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
[2023-05-28 14:17:17,111] {spark_submit.py:490} INFO - 23/05/28 14:17:17 INFO DAGScheduler: Resubmitting failed stages
[2023-05-28 14:17:17,111] {spark_submit.py:490} INFO - 23/05/28 14:17:17 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[8] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-28 14:17:17,112] {spark_submit.py:490} INFO - 23/05/28 14:17:17 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 21.8 KiB, free 433.8 MiB)
[2023-05-28 14:17:17,112] {spark_submit.py:490} INFO - 23/05/28 14:17:17 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 433.8 MiB)
[2023-05-28 14:17:17,113] {spark_submit.py:490} INFO - 23/05/28 14:17:17 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 58e417c56c9d:44457 (size: 8.6 KiB, free: 434.3 MiB)
[2023-05-28 14:17:17,113] {spark_submit.py:490} INFO - 23/05/28 14:17:17 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1535
[2023-05-28 14:17:17,113] {spark_submit.py:490} INFO - 23/05/28 14:17:17 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[8] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-28 14:17:17,114] {spark_submit.py:490} INFO - 23/05/28 14:17:17 INFO TaskSchedulerImpl: Adding task set 3.3 with 1 tasks resource profile 0
[2023-05-28 14:17:17,114] {spark_submit.py:490} INFO - 23/05/28 14:17:17 INFO TaskSetManager: Starting task 0.0 in stage 3.3 (TID 88) (172.23.0.3, executor 5, partition 0, PROCESS_LOCAL, 11205 bytes)
[2023-05-28 14:17:17,141] {spark_submit.py:490} INFO - 23/05/28 14:17:17 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.23.0.3:46433 (size: 8.6 KiB, free: 434.3 MiB)
[2023-05-28 14:17:17,531] {spark_submit.py:490} INFO - 23/05/28 14:17:17 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.23.0.3:46433 (size: 35.0 KiB, free: 434.3 MiB)
[2023-05-28 14:17:19,733] {spark_submit.py:490} INFO - 23/05/28 14:17:19 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.23.0.4:41458) with ID 6,  ResourceProfileId 0
[2023-05-28 14:17:19,784] {spark_submit.py:490} INFO - 23/05/28 14:17:19 INFO BlockManagerMasterEndpoint: Registering block manager 172.23.0.4:36825 with 434.4 MiB RAM, BlockManagerId(6, 172.23.0.4, 36825, None)
[2023-05-28 14:17:22,909] {spark_submit.py:490} INFO - 23/05/28 14:17:22 INFO TaskSetManager: Finished task 0.0 in stage 3.3 (TID 88) in 5820 ms on 172.23.0.3 (executor 5) (1/1)
[2023-05-28 14:17:22,913] {spark_submit.py:490} INFO - 23/05/28 14:17:22 INFO TaskSchedulerImpl: Removed TaskSet 3.3, whose tasks have all completed, from pool
[2023-05-28 14:17:22,914] {spark_submit.py:490} INFO - 23/05/28 14:17:22 INFO DAGScheduler: ShuffleMapStage 3 (parquet at NativeMethodAccessorImpl.java:0) finished in 5.839 s
[2023-05-28 14:17:22,914] {spark_submit.py:490} INFO - 23/05/28 14:17:22 INFO DAGScheduler: looking for newly runnable stages
[2023-05-28 14:17:22,915] {spark_submit.py:490} INFO - 23/05/28 14:17:22 INFO DAGScheduler: running: Set()
[2023-05-28 14:17:22,915] {spark_submit.py:490} INFO - 23/05/28 14:17:22 INFO DAGScheduler: waiting: Set(ResultStage 4)
[2023-05-28 14:17:22,916] {spark_submit.py:490} INFO - 23/05/28 14:17:22 INFO DAGScheduler: failed: Set()
[2023-05-28 14:17:22,916] {spark_submit.py:490} INFO - 23/05/28 14:17:22 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[15] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-28 14:17:22,917] {spark_submit.py:490} INFO - 23/05/28 14:17:22 INFO OutputCommitCoordinator: Reusing state from previous attempt of stage 4.
[2023-05-28 14:17:22,928] {spark_submit.py:490} INFO - 23/05/28 14:17:22 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 247.9 KiB, free 433.5 MiB)
[2023-05-28 14:17:22,931] {spark_submit.py:490} INFO - 23/05/28 14:17:22 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 90.0 KiB, free 433.4 MiB)
[2023-05-28 14:17:22,932] {spark_submit.py:490} INFO - 23/05/28 14:17:22 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 58e417c56c9d:44457 (size: 90.0 KiB, free: 434.2 MiB)
[2023-05-28 14:17:22,932] {spark_submit.py:490} INFO - 23/05/28 14:17:22 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1535
[2023-05-28 14:17:22,933] {spark_submit.py:490} INFO - 23/05/28 14:17:22 INFO DAGScheduler: Submitting 5 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
[2023-05-28 14:17:22,933] {spark_submit.py:490} INFO - 23/05/28 14:17:22 INFO TaskSchedulerImpl: Adding task set 4.2 with 5 tasks resource profile 0
[2023-05-28 14:17:22,935] {spark_submit.py:490} INFO - 23/05/28 14:17:22 INFO TaskSetManager: Starting task 0.0 in stage 4.2 (TID 89) (172.23.0.3, executor 5, partition 0, NODE_LOCAL, 7367 bytes)
[2023-05-28 14:17:22,953] {spark_submit.py:490} INFO - 23/05/28 14:17:22 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.23.0.3:46433 (size: 90.0 KiB, free: 434.2 MiB)
[2023-05-28 14:17:22,966] {spark_submit.py:490} INFO - 23/05/28 14:17:22 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.23.0.3:55826
[2023-05-28 14:17:27,304] {spark_submit.py:490} INFO - 23/05/28 14:17:27 INFO TaskSetManager: Starting task 1.0 in stage 4.2 (TID 90) (172.23.0.4, executor 6, partition 1, ANY, 7367 bytes)
[2023-05-28 14:17:27,427] {spark_submit.py:490} INFO - 23/05/28 14:17:27 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230528141619-0005/5 is now EXITED (Command exited with code 137)
[2023-05-28 14:17:27,428] {spark_submit.py:490} INFO - 23/05/28 14:17:27 INFO StandaloneSchedulerBackend: Executor app-20230528141619-0005/5 removed: Command exited with code 137
[2023-05-28 14:17:27,429] {spark_submit.py:490} INFO - 23/05/28 14:17:27 ERROR TaskSchedulerImpl: Lost executor 5 on 172.23.0.3: Command exited with code 137
[2023-05-28 14:17:27,430] {spark_submit.py:490} INFO - 23/05/28 14:17:27 WARN TaskSetManager: Lost task 0.0 in stage 4.2 (TID 89) (172.23.0.3 executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Command exited with code 137
[2023-05-28 14:17:27,430] {spark_submit.py:490} INFO - 23/05/28 14:17:27 INFO DAGScheduler: Executor lost: 5 (epoch 10)
[2023-05-28 14:17:27,430] {spark_submit.py:490} INFO - 23/05/28 14:17:27 INFO BlockManagerMasterEndpoint: Trying to remove executor 5 from BlockManagerMaster.
[2023-05-28 14:17:27,431] {spark_submit.py:490} INFO - 23/05/28 14:17:27 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(5, 172.23.0.3, 46433, None)
[2023-05-28 14:17:27,431] {spark_submit.py:490} INFO - 23/05/28 14:17:27 INFO BlockManagerMaster: Removed 5 successfully in removeExecutor
[2023-05-28 14:17:27,432] {spark_submit.py:490} INFO - 23/05/28 14:17:27 INFO DAGScheduler: Shuffle files lost for executor: 5 (epoch 10)
[2023-05-28 14:17:27,434] {spark_submit.py:490} INFO - 23/05/28 14:17:27 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20230528141619-0005/7 on worker-20230528055114-172.23.0.3-42839 (172.23.0.3:42839) with 1 core(s)
[2023-05-28 14:17:27,434] {spark_submit.py:490} INFO - 23/05/28 14:17:27 INFO StandaloneSchedulerBackend: Granted executor ID app-20230528141619-0005/7 on hostPort 172.23.0.3:42839 with 1 core(s), 1024.0 MiB RAM
[2023-05-28 14:17:27,509] {spark_submit.py:490} INFO - 23/05/28 14:17:27 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230528141619-0005/7 is now RUNNING
[2023-05-28 14:17:27,745] {spark_submit.py:490} INFO - 23/05/28 14:17:27 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.23.0.4:36825 (size: 90.0 KiB, free: 434.3 MiB)
[2023-05-28 14:17:28,511] {spark_submit.py:490} INFO - 23/05/28 14:17:28 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.23.0.4:41458
[2023-05-28 14:17:28,529] {spark_submit.py:490} INFO - 23/05/28 14:17:28 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 58e417c56c9d:44457 in memory (size: 8.6 KiB, free: 434.2 MiB)
[2023-05-28 14:17:28,561] {spark_submit.py:490} INFO - 23/05/28 14:17:28 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 58e417c56c9d:44457 in memory (size: 8.6 KiB, free: 434.2 MiB)
[2023-05-28 14:17:28,578] {spark_submit.py:490} INFO - 23/05/28 14:17:28 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 172.23.0.7:36327 in memory (size: 8.6 KiB, free: 434.3 MiB)
[2023-05-28 14:17:28,636] {spark_submit.py:490} INFO - 23/05/28 14:17:28 INFO TaskSetManager: Starting task 0.1 in stage 4.2 (TID 91) (172.23.0.4, executor 6, partition 0, ANY, 7367 bytes)
[2023-05-28 14:17:28,637] {spark_submit.py:490} INFO - 23/05/28 14:17:28 WARN TaskSetManager: Lost task 1.0 in stage 4.2 (TID 90) (172.23.0.4 executor 6): FetchFailed(null, shuffleId=0, mapIndex=-1, mapId=-1, reduceId=43, message=
[2023-05-28 14:17:28,639] {spark_submit.py:490} INFO - org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 partition 43
[2023-05-28 14:17:28,642] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1718)
[2023-05-28 14:17:28,642] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:1665)
[2023-05-28 14:17:28,643] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:1664)
[2023-05-28 14:17:28,645] {spark_submit.py:490} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-05-28 14:17:28,646] {spark_submit.py:490} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-05-28 14:17:28,647] {spark_submit.py:490} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-05-28 14:17:28,648] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1664)
[2023-05-28 14:17:28,648] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1306)
[2023-05-28 14:17:28,649] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1268)
[2023-05-28 14:17:28,650] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)
[2023-05-28 14:17:28,650] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)
[2023-05-28 14:17:28,652] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)
[2023-05-28 14:17:28,653] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)
[2023-05-28 14:17:28,654] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:200)
[2023-05-28 14:17:28,654] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:28,655] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:28,655] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:28,656] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:28,656] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:28,657] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:28,658] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:28,658] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:28,659] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:28,659] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:28,660] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:28,660] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:28,661] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:28,662] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:28,662] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:28,663] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:28,664] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:28,664] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:28,665] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:28,665] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:28,666] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
[2023-05-28 14:17:28,667] {spark_submit.py:490} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
[2023-05-28 14:17:28,668] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:139)
[2023-05-28 14:17:28,668] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
[2023-05-28 14:17:28,670] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
[2023-05-28 14:17:28,671] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
[2023-05-28 14:17:28,672] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2023-05-28 14:17:28,673] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2023-05-28 14:17:28,674] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:833)
[2023-05-28 14:17:28,674] {spark_submit.py:490} INFO - 
[2023-05-28 14:17:28,675] {spark_submit.py:490} INFO - )
[2023-05-28 14:17:28,676] {spark_submit.py:490} INFO - 23/05/28 14:17:28 INFO TaskSetManager: task 1.0 in stage 4.2 (TID 90) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
[2023-05-28 14:17:28,677] {spark_submit.py:490} INFO - 23/05/28 14:17:28 INFO DAGScheduler: Marking ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0) as failed due to a fetch failure from ShuffleMapStage 3 (parquet at NativeMethodAccessorImpl.java:0)
[2023-05-28 14:17:28,677] {spark_submit.py:490} INFO - 23/05/28 14:17:28 INFO DAGScheduler: ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0) failed in 5.719 s due to org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 partition 43
[2023-05-28 14:17:28,678] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1718)
[2023-05-28 14:17:28,678] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:1665)
[2023-05-28 14:17:28,678] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:1664)
[2023-05-28 14:17:28,679] {spark_submit.py:490} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-05-28 14:17:28,679] {spark_submit.py:490} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-05-28 14:17:28,680] {spark_submit.py:490} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-05-28 14:17:28,683] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1664)
[2023-05-28 14:17:28,689] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1306)
[2023-05-28 14:17:28,689] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1268)
[2023-05-28 14:17:28,689] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)
[2023-05-28 14:17:28,690] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)
[2023-05-28 14:17:28,690] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)
[2023-05-28 14:17:28,691] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)
[2023-05-28 14:17:28,691] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:200)
[2023-05-28 14:17:28,692] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:28,693] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:28,695] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:28,697] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:28,698] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:28,698] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:28,698] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:28,699] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:28,699] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:28,699] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:28,708] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:28,712] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:28,713] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:28,713] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:28,714] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:28,715] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:28,715] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:28,716] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:28,717] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:28,717] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:28,718] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
[2023-05-28 14:17:28,718] {spark_submit.py:490} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
[2023-05-28 14:17:28,719] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:139)
[2023-05-28 14:17:28,719] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
[2023-05-28 14:17:28,720] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
[2023-05-28 14:17:28,721] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
[2023-05-28 14:17:28,721] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2023-05-28 14:17:28,722] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2023-05-28 14:17:28,723] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:833)
[2023-05-28 14:17:28,725] {spark_submit.py:490} INFO - 
[2023-05-28 14:17:28,725] {spark_submit.py:490} INFO - 23/05/28 14:17:28 INFO DAGScheduler: Resubmitting ShuffleMapStage 3 (parquet at NativeMethodAccessorImpl.java:0) and ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0) due to fetch failure
[2023-05-28 14:17:28,762] {spark_submit.py:490} INFO - 23/05/28 14:17:28 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.23.0.4:41458
[2023-05-28 14:17:28,794] {spark_submit.py:490} INFO - 23/05/28 14:17:28 WARN TaskSetManager: Lost task 0.1 in stage 4.2 (TID 91) (172.23.0.4 executor 6): FetchFailed(null, shuffleId=0, mapIndex=-1, mapId=-1, reduceId=0, message=
[2023-05-28 14:17:28,794] {spark_submit.py:490} INFO - org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 partition 0
[2023-05-28 14:17:28,795] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1718)
[2023-05-28 14:17:28,795] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:1665)
[2023-05-28 14:17:28,799] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:1664)
[2023-05-28 14:17:28,799] {spark_submit.py:490} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-05-28 14:17:28,800] {spark_submit.py:490} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-05-28 14:17:28,800] {spark_submit.py:490} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-05-28 14:17:28,801] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1664)
[2023-05-28 14:17:28,801] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1306)
[2023-05-28 14:17:28,802] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1268)
[2023-05-28 14:17:28,802] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)
[2023-05-28 14:17:28,803] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)
[2023-05-28 14:17:28,803] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)
[2023-05-28 14:17:28,804] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)
[2023-05-28 14:17:28,805] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:200)
[2023-05-28 14:17:28,806] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:28,806] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:28,807] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:28,808] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:28,808] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:28,808] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:28,808] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:28,809] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:28,809] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:28,810] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:28,810] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:28,810] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:28,810] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:28,811] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:28,811] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:28,811] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:28,812] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:28,812] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:28,813] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:28,813] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:28,814] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
[2023-05-28 14:17:28,814] {spark_submit.py:490} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
[2023-05-28 14:17:28,815] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:139)
[2023-05-28 14:17:28,815] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
[2023-05-28 14:17:28,815] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
[2023-05-28 14:17:28,816] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
[2023-05-28 14:17:28,816] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2023-05-28 14:17:28,817] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2023-05-28 14:17:28,817] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:833)
[2023-05-28 14:17:28,817] {spark_submit.py:490} INFO - 
[2023-05-28 14:17:28,818] {spark_submit.py:490} INFO - )
[2023-05-28 14:17:28,818] {spark_submit.py:490} INFO - 23/05/28 14:17:28 INFO TaskSetManager: task 0.1 in stage 4.2 (TID 91) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
[2023-05-28 14:17:28,818] {spark_submit.py:490} INFO - 23/05/28 14:17:28 INFO TaskSchedulerImpl: Removed TaskSet 4.2, whose tasks have all completed, from pool
[2023-05-28 14:17:28,839] {spark_submit.py:490} INFO - 23/05/28 14:17:28 INFO DAGScheduler: Resubmitting failed stages
[2023-05-28 14:17:28,839] {spark_submit.py:490} INFO - 23/05/28 14:17:28 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[8] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-28 14:17:28,843] {spark_submit.py:490} INFO - 23/05/28 14:17:28 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 21.8 KiB, free 433.5 MiB)
[2023-05-28 14:17:28,858] {spark_submit.py:490} INFO - 23/05/28 14:17:28 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 433.5 MiB)
[2023-05-28 14:17:28,859] {spark_submit.py:490} INFO - 23/05/28 14:17:28 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 58e417c56c9d:44457 (size: 8.6 KiB, free: 434.2 MiB)
[2023-05-28 14:17:28,863] {spark_submit.py:490} INFO - 23/05/28 14:17:28 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1535
[2023-05-28 14:17:28,864] {spark_submit.py:490} INFO - 23/05/28 14:17:28 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[8] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-28 14:17:28,865] {spark_submit.py:490} INFO - 23/05/28 14:17:28 INFO TaskSchedulerImpl: Adding task set 3.4 with 1 tasks resource profile 0
[2023-05-28 14:17:28,873] {spark_submit.py:490} INFO - 23/05/28 14:17:28 INFO TaskSetManager: Starting task 0.0 in stage 3.4 (TID 92) (172.23.0.4, executor 6, partition 0, PROCESS_LOCAL, 11205 bytes)
[2023-05-28 14:17:28,899] {spark_submit.py:490} INFO - 23/05/28 14:17:28 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.23.0.4:36825 (size: 8.6 KiB, free: 434.3 MiB)
[2023-05-28 14:17:29,951] {spark_submit.py:490} INFO - 23/05/28 14:17:29 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.23.0.4:36825 (size: 35.0 KiB, free: 434.3 MiB)
[2023-05-28 14:17:31,358] {spark_submit.py:490} INFO - 23/05/28 14:17:31 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.23.0.3:51434) with ID 7,  ResourceProfileId 0
[2023-05-28 14:17:31,429] {spark_submit.py:490} INFO - 23/05/28 14:17:31 INFO BlockManagerMasterEndpoint: Registering block manager 172.23.0.3:34199 with 434.4 MiB RAM, BlockManagerId(7, 172.23.0.3, 34199, None)
[2023-05-28 14:17:35,088] {spark_submit.py:490} INFO - 23/05/28 14:17:35 INFO TaskSetManager: Finished task 0.0 in stage 3.4 (TID 92) in 6217 ms on 172.23.0.4 (executor 6) (1/1)
[2023-05-28 14:17:35,091] {spark_submit.py:490} INFO - 23/05/28 14:17:35 INFO TaskSchedulerImpl: Removed TaskSet 3.4, whose tasks have all completed, from pool
[2023-05-28 14:17:35,092] {spark_submit.py:490} INFO - 23/05/28 14:17:35 INFO DAGScheduler: ShuffleMapStage 3 (parquet at NativeMethodAccessorImpl.java:0) finished in 6.247 s
[2023-05-28 14:17:35,093] {spark_submit.py:490} INFO - 23/05/28 14:17:35 INFO DAGScheduler: looking for newly runnable stages
[2023-05-28 14:17:35,093] {spark_submit.py:490} INFO - 23/05/28 14:17:35 INFO DAGScheduler: running: Set()
[2023-05-28 14:17:35,094] {spark_submit.py:490} INFO - 23/05/28 14:17:35 INFO DAGScheduler: waiting: Set(ResultStage 4)
[2023-05-28 14:17:35,095] {spark_submit.py:490} INFO - 23/05/28 14:17:35 INFO DAGScheduler: failed: Set()
[2023-05-28 14:17:35,095] {spark_submit.py:490} INFO - 23/05/28 14:17:35 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[15] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-28 14:17:35,096] {spark_submit.py:490} INFO - 23/05/28 14:17:35 INFO OutputCommitCoordinator: Reusing state from previous attempt of stage 4.
[2023-05-28 14:17:35,107] {spark_submit.py:490} INFO - 23/05/28 14:17:35 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 247.9 KiB, free 433.2 MiB)
[2023-05-28 14:17:35,110] {spark_submit.py:490} INFO - 23/05/28 14:17:35 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 90.0 KiB, free 433.1 MiB)
[2023-05-28 14:17:35,111] {spark_submit.py:490} INFO - 23/05/28 14:17:35 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 58e417c56c9d:44457 (size: 90.0 KiB, free: 434.1 MiB)
[2023-05-28 14:17:35,112] {spark_submit.py:490} INFO - 23/05/28 14:17:35 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1535
[2023-05-28 14:17:35,112] {spark_submit.py:490} INFO - 23/05/28 14:17:35 INFO DAGScheduler: Submitting 5 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
[2023-05-28 14:17:35,113] {spark_submit.py:490} INFO - 23/05/28 14:17:35 INFO TaskSchedulerImpl: Adding task set 4.3 with 5 tasks resource profile 0
[2023-05-28 14:17:35,114] {spark_submit.py:490} INFO - 23/05/28 14:17:35 INFO TaskSetManager: Starting task 0.0 in stage 4.3 (TID 93) (172.23.0.4, executor 6, partition 0, NODE_LOCAL, 7367 bytes)
[2023-05-28 14:17:35,134] {spark_submit.py:490} INFO - 23/05/28 14:17:35 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.23.0.4:36825 (size: 90.0 KiB, free: 434.2 MiB)
[2023-05-28 14:17:35,146] {spark_submit.py:490} INFO - 23/05/28 14:17:35 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.23.0.4:41458
[2023-05-28 14:17:40,226] {spark_submit.py:490} INFO - 23/05/28 14:17:40 INFO TaskSetManager: Starting task 1.0 in stage 4.3 (TID 94) (172.23.0.3, executor 7, partition 1, ANY, 7367 bytes)
[2023-05-28 14:17:40,448] {spark_submit.py:490} INFO - 23/05/28 14:17:40 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230528141619-0005/6 is now EXITED (Command exited with code 137)
[2023-05-28 14:17:40,449] {spark_submit.py:490} INFO - 23/05/28 14:17:40 INFO StandaloneSchedulerBackend: Executor app-20230528141619-0005/6 removed: Command exited with code 137
[2023-05-28 14:17:40,450] {spark_submit.py:490} INFO - 23/05/28 14:17:40 ERROR TaskSchedulerImpl: Lost executor 6 on 172.23.0.4: Command exited with code 137
[2023-05-28 14:17:40,450] {spark_submit.py:490} INFO - 23/05/28 14:17:40 WARN TaskSetManager: Lost task 0.0 in stage 4.3 (TID 93) (172.23.0.4 executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Command exited with code 137
[2023-05-28 14:17:40,451] {spark_submit.py:490} INFO - 23/05/28 14:17:40 INFO DAGScheduler: Executor lost: 6 (epoch 12)
[2023-05-28 14:17:40,451] {spark_submit.py:490} INFO - 23/05/28 14:17:40 INFO BlockManagerMasterEndpoint: Trying to remove executor 6 from BlockManagerMaster.
[2023-05-28 14:17:40,451] {spark_submit.py:490} INFO - 23/05/28 14:17:40 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(6, 172.23.0.4, 36825, None)
[2023-05-28 14:17:40,452] {spark_submit.py:490} INFO - 23/05/28 14:17:40 INFO BlockManagerMaster: Removed 6 successfully in removeExecutor
[2023-05-28 14:17:40,452] {spark_submit.py:490} INFO - 23/05/28 14:17:40 INFO DAGScheduler: Shuffle files lost for executor: 6 (epoch 12)
[2023-05-28 14:17:40,453] {spark_submit.py:490} INFO - 23/05/28 14:17:40 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20230528141619-0005/8 on worker-20230528055114-172.23.0.4-41329 (172.23.0.4:41329) with 1 core(s)
[2023-05-28 14:17:40,453] {spark_submit.py:490} INFO - 23/05/28 14:17:40 INFO StandaloneSchedulerBackend: Granted executor ID app-20230528141619-0005/8 on hostPort 172.23.0.4:41329 with 1 core(s), 1024.0 MiB RAM
[2023-05-28 14:17:40,583] {spark_submit.py:490} INFO - 23/05/28 14:17:40 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230528141619-0005/8 is now RUNNING
[2023-05-28 14:17:40,620] {spark_submit.py:490} INFO - 23/05/28 14:17:40 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.23.0.3:34199 (size: 90.0 KiB, free: 434.3 MiB)
[2023-05-28 14:17:41,573] {spark_submit.py:490} INFO - 23/05/28 14:17:41 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.23.0.3:51434
[2023-05-28 14:17:41,673] {spark_submit.py:490} INFO - 23/05/28 14:17:41 INFO TaskSetManager: Starting task 0.1 in stage 4.3 (TID 95) (172.23.0.3, executor 7, partition 0, ANY, 7367 bytes)
[2023-05-28 14:17:41,675] {spark_submit.py:490} INFO - 23/05/28 14:17:41 WARN TaskSetManager: Lost task 1.0 in stage 4.3 (TID 94) (172.23.0.3 executor 7): FetchFailed(null, shuffleId=0, mapIndex=-1, mapId=-1, reduceId=43, message=
[2023-05-28 14:17:41,676] {spark_submit.py:490} INFO - org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 partition 43
[2023-05-28 14:17:41,677] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1718)
[2023-05-28 14:17:41,677] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:1665)
[2023-05-28 14:17:41,678] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:1664)
[2023-05-28 14:17:41,679] {spark_submit.py:490} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-05-28 14:17:41,679] {spark_submit.py:490} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-05-28 14:17:41,680] {spark_submit.py:490} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-05-28 14:17:41,681] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1664)
[2023-05-28 14:17:41,681] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1306)
[2023-05-28 14:17:41,682] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1268)
[2023-05-28 14:17:41,682] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)
[2023-05-28 14:17:41,683] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)
[2023-05-28 14:17:41,683] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)
[2023-05-28 14:17:41,684] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)
[2023-05-28 14:17:41,685] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:200)
[2023-05-28 14:17:41,685] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:41,686] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:41,686] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:41,686] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:41,687] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:41,687] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:41,688] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:41,688] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:41,689] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:41,692] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:41,693] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:41,693] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:41,694] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:41,700] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:41,700] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:41,701] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:41,701] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:41,701] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:41,702] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:41,702] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:41,703] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
[2023-05-28 14:17:41,703] {spark_submit.py:490} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
[2023-05-28 14:17:41,703] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:139)
[2023-05-28 14:17:41,704] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
[2023-05-28 14:17:41,704] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
[2023-05-28 14:17:41,705] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
[2023-05-28 14:17:41,705] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2023-05-28 14:17:41,706] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2023-05-28 14:17:41,706] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:833)
[2023-05-28 14:17:41,706] {spark_submit.py:490} INFO - 
[2023-05-28 14:17:41,707] {spark_submit.py:490} INFO - )
[2023-05-28 14:17:41,707] {spark_submit.py:490} INFO - 23/05/28 14:17:41 INFO TaskSetManager: task 1.0 in stage 4.3 (TID 94) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
[2023-05-28 14:17:41,708] {spark_submit.py:490} INFO - 23/05/28 14:17:41 INFO DAGScheduler: Marking ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0) as failed due to a fetch failure from ShuffleMapStage 3 (parquet at NativeMethodAccessorImpl.java:0)
[2023-05-28 14:17:41,709] {spark_submit.py:490} INFO - 23/05/28 14:17:41 INFO DAGScheduler: ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0) failed in 6.579 s due to org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 partition 43
[2023-05-28 14:17:41,710] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1718)
[2023-05-28 14:17:41,712] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:1665)
[2023-05-28 14:17:41,713] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:1664)
[2023-05-28 14:17:41,713] {spark_submit.py:490} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-05-28 14:17:41,714] {spark_submit.py:490} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-05-28 14:17:41,715] {spark_submit.py:490} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-05-28 14:17:41,719] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1664)
[2023-05-28 14:17:41,720] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1306)
[2023-05-28 14:17:41,720] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1268)
[2023-05-28 14:17:41,721] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)
[2023-05-28 14:17:41,723] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)
[2023-05-28 14:17:41,724] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)
[2023-05-28 14:17:41,725] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)
[2023-05-28 14:17:41,726] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:200)
[2023-05-28 14:17:41,727] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:41,728] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:41,728] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:41,729] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:41,730] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:41,731] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:41,731] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:41,733] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:41,734] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:41,734] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:41,735] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:41,735] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:41,735] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:41,735] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:41,736] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:41,736] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:41,737] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:41,737] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:41,737] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:41,738] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:41,738] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
[2023-05-28 14:17:41,739] {spark_submit.py:490} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
[2023-05-28 14:17:41,739] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:139)
[2023-05-28 14:17:41,739] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
[2023-05-28 14:17:41,740] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
[2023-05-28 14:17:41,740] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
[2023-05-28 14:17:41,740] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2023-05-28 14:17:41,741] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2023-05-28 14:17:41,741] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:833)
[2023-05-28 14:17:41,742] {spark_submit.py:490} INFO - 
[2023-05-28 14:17:41,744] {spark_submit.py:490} INFO - 23/05/28 14:17:41 INFO DAGScheduler: Job 3 failed: parquet at NativeMethodAccessorImpl.java:0, took 63.860929 s
[2023-05-28 14:17:41,745] {spark_submit.py:490} INFO - 23/05/28 14:17:41 ERROR FileFormatWriter: Aborting job 7e08b7a7-9752-4ef3-9852-809c6c13813b.
[2023-05-28 14:17:41,747] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0) has failed the maximum allowable number of times: 4. Most recent failure reason:
[2023-05-28 14:17:41,749] {spark_submit.py:490} INFO - org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 partition 43
[2023-05-28 14:17:41,750] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1718)
[2023-05-28 14:17:41,750] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:1665)
[2023-05-28 14:17:41,756] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:1664)
[2023-05-28 14:17:41,757] {spark_submit.py:490} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-05-28 14:17:41,758] {spark_submit.py:490} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-05-28 14:17:41,758] {spark_submit.py:490} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-05-28 14:17:41,759] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1664)
[2023-05-28 14:17:41,759] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1306)
[2023-05-28 14:17:41,759] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1268)
[2023-05-28 14:17:41,760] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)
[2023-05-28 14:17:41,760] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)
[2023-05-28 14:17:41,760] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)
[2023-05-28 14:17:41,761] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)
[2023-05-28 14:17:41,761] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:200)
[2023-05-28 14:17:41,761] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:41,762] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:41,762] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:41,763] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:41,763] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:41,763] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:41,764] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:41,764] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:41,764] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:41,765] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:41,765] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:41,765] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:41,766] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:41,766] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:41,766] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:41,766] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:41,767] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:41,767] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:41,767] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:41,768] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:41,768] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
[2023-05-28 14:17:41,768] {spark_submit.py:490} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
[2023-05-28 14:17:41,768] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:139)
[2023-05-28 14:17:41,769] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
[2023-05-28 14:17:41,769] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
[2023-05-28 14:17:41,769] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
[2023-05-28 14:17:41,769] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2023-05-28 14:17:41,770] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2023-05-28 14:17:41,770] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:833)
[2023-05-28 14:17:41,770] {spark_submit.py:490} INFO - 
[2023-05-28 14:17:41,771] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
[2023-05-28 14:17:41,771] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
[2023-05-28 14:17:41,773] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
[2023-05-28 14:17:41,774] {spark_submit.py:490} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-05-28 14:17:41,775] {spark_submit.py:490} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-05-28 14:17:41,784] {spark_submit.py:490} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-05-28 14:17:41,785] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
[2023-05-28 14:17:41,785] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1961)
[2023-05-28 14:17:41,786] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2978)
[2023-05-28 14:17:41,786] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
[2023-05-28 14:17:41,786] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
[2023-05-28 14:17:41,787] {spark_submit.py:490} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-05-28 14:17:41,787] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
[2023-05-28 14:17:41,788] {spark_submit.py:490} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
[2023-05-28 14:17:41,788] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)
[2023-05-28 14:17:41,789] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)
[2023-05-28 14:17:41,789] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
[2023-05-28 14:17:41,789] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
[2023-05-28 14:17:41,790] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
[2023-05-28 14:17:41,790] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
[2023-05-28 14:17:41,790] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
[2023-05-28 14:17:41,791] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
[2023-05-28 14:17:41,791] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:354)
[2023-05-28 14:17:41,792] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:382)
[2023-05-28 14:17:41,792] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)
[2023-05-28 14:17:41,792] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-05-28 14:17:41,793] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
[2023-05-28 14:17:41,793] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
[2023-05-28 14:17:41,793] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
[2023-05-28 14:17:41,793] {spark_submit.py:490} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
[2023-05-28 14:17:41,794] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
[2023-05-28 14:17:41,795] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-05-28 14:17:41,795] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-05-28 14:17:41,796] {spark_submit.py:490} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
[2023-05-28 14:17:41,796] {spark_submit.py:490} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
[2023-05-28 14:17:41,797] {spark_submit.py:490} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
[2023-05-28 14:17:41,797] {spark_submit.py:490} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
[2023-05-28 14:17:41,798] {spark_submit.py:490} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-05-28 14:17:41,798] {spark_submit.py:490} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-05-28 14:17:41,799] {spark_submit.py:490} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
[2023-05-28 14:17:41,799] {spark_submit.py:490} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
[2023-05-28 14:17:41,799] {spark_submit.py:490} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
[2023-05-28 14:17:41,800] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-05-28 14:17:41,800] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-05-28 14:17:41,801] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-05-28 14:17:41,801] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
[2023-05-28 14:17:41,801] {spark_submit.py:490} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
[2023-05-28 14:17:41,801] {spark_submit.py:490} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
[2023-05-28 14:17:41,802] {spark_submit.py:490} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
[2023-05-28 14:17:41,802] {spark_submit.py:490} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
[2023-05-28 14:17:41,802] {spark_submit.py:490} INFO - at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789)
[2023-05-28 14:17:41,803] {spark_submit.py:490} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-05-28 14:17:41,803] {spark_submit.py:490} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-05-28 14:17:41,804] {spark_submit.py:490} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-05-28 14:17:41,804] {spark_submit.py:490} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-05-28 14:17:41,804] {spark_submit.py:490} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-05-28 14:17:41,805] {spark_submit.py:490} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2023-05-28 14:17:41,805] {spark_submit.py:490} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-05-28 14:17:41,807] {spark_submit.py:490} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-05-28 14:17:41,818] {spark_submit.py:490} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-05-28 14:17:41,819] {spark_submit.py:490} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-05-28 14:17:41,820] {spark_submit.py:490} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-05-28 14:17:41,826] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-05-28 14:17:41,827] {spark_submit.py:490} INFO - 23/05/28 14:17:41 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.23.0.3:51434
[2023-05-28 14:17:41,827] {spark_submit.py:490} INFO - 23/05/28 14:17:41 WARN TaskSetManager: Lost task 0.1 in stage 4.3 (TID 95) (172.23.0.3 executor 7): FetchFailed(null, shuffleId=0, mapIndex=-1, mapId=-1, reduceId=0, message=
[2023-05-28 14:17:41,828] {spark_submit.py:490} INFO - org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 partition 0
[2023-05-28 14:17:41,828] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1718)
[2023-05-28 14:17:41,829] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:1665)
[2023-05-28 14:17:41,829] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:1664)
[2023-05-28 14:17:41,829] {spark_submit.py:490} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-05-28 14:17:41,831] {spark_submit.py:490} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-05-28 14:17:41,831] {spark_submit.py:490} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-05-28 14:17:41,832] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1664)
[2023-05-28 14:17:41,833] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1306)
[2023-05-28 14:17:41,833] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1268)
[2023-05-28 14:17:41,834] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)
[2023-05-28 14:17:41,834] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)
[2023-05-28 14:17:41,834] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)
[2023-05-28 14:17:41,835] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)
[2023-05-28 14:17:41,835] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:200)
[2023-05-28 14:17:41,836] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:41,836] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:41,836] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:41,837] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:41,837] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:41,837] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:41,838] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:41,838] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:41,839] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:41,840] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:41,841] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:41,842] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:41,842] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:41,842] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:41,842] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:41,843] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:41,843] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:41,843] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:41,843] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:41,844] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:41,844] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
[2023-05-28 14:17:41,845] {spark_submit.py:490} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
[2023-05-28 14:17:41,845] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:139)
[2023-05-28 14:17:41,845] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
[2023-05-28 14:17:41,846] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
[2023-05-28 14:17:41,846] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
[2023-05-28 14:17:41,846] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2023-05-28 14:17:41,852] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2023-05-28 14:17:41,857] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:833)
[2023-05-28 14:17:41,862] {spark_submit.py:490} INFO - 
[2023-05-28 14:17:41,867] {spark_submit.py:490} INFO - )
[2023-05-28 14:17:41,868] {spark_submit.py:490} INFO - 23/05/28 14:17:41 INFO TaskSetManager: task 0.1 in stage 4.3 (TID 95) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
[2023-05-28 14:17:41,868] {spark_submit.py:490} INFO - 23/05/28 14:17:41 INFO TaskSchedulerImpl: Removed TaskSet 4.3, whose tasks have all completed, from pool
[2023-05-28 14:17:42,281] {spark_submit.py:490} INFO - Traceback (most recent call last):
[2023-05-28 14:17:42,282] {spark_submit.py:490} INFO - File "/usr/local/spark/app/feature_engineering_processing.py", line 29, in <module>
[2023-05-28 14:17:42,284] {spark_submit.py:490} INFO - df_with_rolling_avg_and_median.write.mode('overwrite').option('compression', 'lz4').parquet(output_path+type+".parquet")
[2023-05-28 14:17:42,285] {spark_submit.py:490} INFO - File "/home/***/.local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1656, in parquet
[2023-05-28 14:17:42,287] {spark_submit.py:490} INFO - File "/home/***/.local/lib/python3.7/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1323, in __call__
[2023-05-28 14:17:42,288] {spark_submit.py:490} INFO - File "/home/***/.local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
[2023-05-28 14:17:42,288] {spark_submit.py:490} INFO - File "/home/***/.local/lib/python3.7/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 328, in get_return_value
[2023-05-28 14:17:42,332] {spark_submit.py:490} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling o46.parquet.
[2023-05-28 14:17:42,333] {spark_submit.py:490} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0) has failed the maximum allowable number of times: 4. Most recent failure reason:
[2023-05-28 14:17:42,333] {spark_submit.py:490} INFO - org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 partition 43
[2023-05-28 14:17:42,334] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1718)
[2023-05-28 14:17:42,334] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:1665)
[2023-05-28 14:17:42,334] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:1664)
[2023-05-28 14:17:42,335] {spark_submit.py:490} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-05-28 14:17:42,335] {spark_submit.py:490} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-05-28 14:17:42,336] {spark_submit.py:490} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-05-28 14:17:42,336] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1664)
[2023-05-28 14:17:42,337] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1306)
[2023-05-28 14:17:42,337] {spark_submit.py:490} INFO - at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1268)
[2023-05-28 14:17:42,338] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)
[2023-05-28 14:17:42,338] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)
[2023-05-28 14:17:42,338] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)
[2023-05-28 14:17:42,339] {spark_submit.py:490} INFO - at org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)
[2023-05-28 14:17:42,339] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:200)
[2023-05-28 14:17:42,339] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:42,340] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:42,340] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:42,341] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:42,341] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:42,342] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:42,342] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:42,342] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:42,343] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:42,343] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:42,345] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:42,346] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:42,346] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:42,347] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:42,347] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:42,348] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:42,348] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:42,348] {spark_submit.py:490} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-05-28 14:17:42,349] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-05-28 14:17:42,349] {spark_submit.py:490} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-05-28 14:17:42,349] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
[2023-05-28 14:17:42,349] {spark_submit.py:490} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
[2023-05-28 14:17:42,350] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:139)
[2023-05-28 14:17:42,350] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
[2023-05-28 14:17:42,350] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
[2023-05-28 14:17:42,351] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
[2023-05-28 14:17:42,351] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2023-05-28 14:17:42,351] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2023-05-28 14:17:42,351] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:833)
[2023-05-28 14:17:42,352] {spark_submit.py:490} INFO - 
[2023-05-28 14:17:42,352] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
[2023-05-28 14:17:42,352] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
[2023-05-28 14:17:42,353] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
[2023-05-28 14:17:42,353] {spark_submit.py:490} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-05-28 14:17:42,354] {spark_submit.py:490} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-05-28 14:17:42,354] {spark_submit.py:490} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-05-28 14:17:42,355] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
[2023-05-28 14:17:42,355] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1961)
[2023-05-28 14:17:42,355] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2978)
[2023-05-28 14:17:42,356] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
[2023-05-28 14:17:42,356] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
[2023-05-28 14:17:42,357] {spark_submit.py:490} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-05-28 14:17:42,357] {spark_submit.py:490} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
[2023-05-28 14:17:42,357] {spark_submit.py:490} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
[2023-05-28 14:17:42,358] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)
[2023-05-28 14:17:42,358] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)
[2023-05-28 14:17:42,359] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
[2023-05-28 14:17:42,359] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
[2023-05-28 14:17:42,359] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
[2023-05-28 14:17:42,360] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
[2023-05-28 14:17:42,360] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
[2023-05-28 14:17:42,361] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
[2023-05-28 14:17:42,362] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:354)
[2023-05-28 14:17:42,362] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:382)
[2023-05-28 14:17:42,362] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)
[2023-05-28 14:17:42,363] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-05-28 14:17:42,363] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
[2023-05-28 14:17:42,364] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
[2023-05-28 14:17:42,364] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
[2023-05-28 14:17:42,365] {spark_submit.py:490} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
[2023-05-28 14:17:42,365] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
[2023-05-28 14:17:42,365] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-05-28 14:17:42,366] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-05-28 14:17:42,366] {spark_submit.py:490} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
[2023-05-28 14:17:42,367] {spark_submit.py:490} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
[2023-05-28 14:17:42,367] {spark_submit.py:490} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
[2023-05-28 14:17:42,368] {spark_submit.py:490} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
[2023-05-28 14:17:42,368] {spark_submit.py:490} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-05-28 14:17:42,369] {spark_submit.py:490} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-05-28 14:17:42,372] {spark_submit.py:490} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
[2023-05-28 14:17:42,372] {spark_submit.py:490} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
[2023-05-28 14:17:42,373] {spark_submit.py:490} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
[2023-05-28 14:17:42,373] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-05-28 14:17:42,374] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-05-28 14:17:42,374] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-05-28 14:17:42,375] {spark_submit.py:490} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
[2023-05-28 14:17:42,375] {spark_submit.py:490} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
[2023-05-28 14:17:42,376] {spark_submit.py:490} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
[2023-05-28 14:17:42,376] {spark_submit.py:490} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
[2023-05-28 14:17:42,376] {spark_submit.py:490} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
[2023-05-28 14:17:42,377] {spark_submit.py:490} INFO - at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789)
[2023-05-28 14:17:42,377] {spark_submit.py:490} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-05-28 14:17:42,377] {spark_submit.py:490} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-05-28 14:17:42,378] {spark_submit.py:490} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-05-28 14:17:42,378] {spark_submit.py:490} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-05-28 14:17:42,379] {spark_submit.py:490} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-05-28 14:17:42,379] {spark_submit.py:490} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2023-05-28 14:17:42,380] {spark_submit.py:490} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-05-28 14:17:42,380] {spark_submit.py:490} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-05-28 14:17:42,380] {spark_submit.py:490} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-05-28 14:17:42,381] {spark_submit.py:490} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-05-28 14:17:42,381] {spark_submit.py:490} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-05-28 14:17:42,382] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-05-28 14:17:42,382] {spark_submit.py:490} INFO - 
[2023-05-28 14:17:42,454] {spark_submit.py:490} INFO - 23/05/28 14:17:42 INFO SparkContext: Invoking stop() from shutdown hook
[2023-05-28 14:17:42,454] {spark_submit.py:490} INFO - 23/05/28 14:17:42 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2023-05-28 14:17:42,478] {spark_submit.py:490} INFO - 23/05/28 14:17:42 INFO SparkUI: Stopped Spark web UI at http://58e417c56c9d:4040
[2023-05-28 14:17:42,498] {spark_submit.py:490} INFO - 23/05/28 14:17:42 INFO StandaloneSchedulerBackend: Shutting down all executors
[2023-05-28 14:17:42,499] {spark_submit.py:490} INFO - 23/05/28 14:17:42 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
[2023-05-28 14:17:42,657] {spark_submit.py:490} INFO - 23/05/28 14:17:42 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-05-28 14:17:42,811] {spark_submit.py:490} INFO - 23/05/28 14:17:42 INFO MemoryStore: MemoryStore cleared
[2023-05-28 14:17:42,812] {spark_submit.py:490} INFO - 23/05/28 14:17:42 INFO BlockManager: BlockManager stopped
[2023-05-28 14:17:42,818] {spark_submit.py:490} INFO - 23/05/28 14:17:42 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-05-28 14:17:42,835] {spark_submit.py:490} INFO - 23/05/28 14:17:42 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-05-28 14:17:42,884] {spark_submit.py:490} INFO - 23/05/28 14:17:42 INFO SparkContext: Successfully stopped SparkContext
[2023-05-28 14:17:42,885] {spark_submit.py:490} INFO - 23/05/28 14:17:42 INFO ShutdownHookManager: Shutdown hook called
[2023-05-28 14:17:42,891] {spark_submit.py:490} INFO - 23/05/28 14:17:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-e3c8ebd8-c49a-4238-9e25-6cf0b42e1c19
[2023-05-28 14:17:42,891] {spark_submit.py:490} INFO - 23/05/28 14:17:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-e3c8ebd8-c49a-4238-9e25-6cf0b42e1c19/pyspark-fa283141-359d-4467-b8f0-cbf54189c684
[2023-05-28 14:17:42,896] {spark_submit.py:490} INFO - 23/05/28 14:17:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-acea3878-616b-4801-b4b8-02bcb3e3c364
[2023-05-28 14:17:43,409] {taskinstance.py:1889} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 422, in submit
    f"Cannot execute: {self._mask_cmd(spark_submit_cmd)}. Error code is: {returncode}."
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://spark:7077 --name arrow-spark /usr/local/spark/app/feature_engineering_processing.py etfs. Error code is: 1.
[2023-05-28 14:17:43,423] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=stock_spark_***, task_id=feature_engineering.etf_feature_processing, execution_date=20230528T140612, start_date=20230528T141616, end_date=20230528T141743
[2023-05-28 14:17:43,459] {standard_task_runner.py:97} ERROR - Failed to execute job 346 for task feature_engineering.etf_feature_processing (Cannot execute: spark-submit --master spark://spark:7077 --name arrow-spark /usr/local/spark/app/feature_engineering_processing.py etfs. Error code is: 1.; 1998)
[2023-05-28 14:17:43,550] {local_task_job.py:156} INFO - Task exited with return code 1
[2023-05-28 14:17:43,606] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
