{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27f7baac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyspark \n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b59fa3b-ab81-4313-994b-3454f2851ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "# conf = SparkConf().set(\"spark.speculation\",\"false\")\n",
    "# spark = SparkSession.builder.appName(\"random_reg\").getOrCreate()\n",
    "spark = SparkSession.builder.appName(\"random_reg\").config(\"spark.sql.parquet.compression.codec\", \"lz4\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f33f74a8-ebe2-4743-81b9-89196c9a4915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------------------------+----------+-------------------+------------------+-------------------+------------------+-------------------+-----+-------+--------------------+\n",
      "|ACSI|American Customer Satisfaction ETF|2016-11-07|24.8899993896484383|25.010000228881836|24.8899993896484385|24.989999771118164|23.8709430694580087|62300|62300.0|23.87094306945800810|\n",
      "+----+----------------------------------+----------+-------------------+------------------+-------------------+------------------+-------------------+-----+-------+--------------------+\n",
      "|ACSI|              American Customer...|2016-11-08| 27.010000228881836|27.010000228881836| 25.010000228881836|25.200000762939453|   24.0715389251709|20300|41300.0|  23.870943069458008|\n",
      "+----+----------------------------------+----------+-------------------+------------------+-------------------+------------------+-------------------+-----+-------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "today = datetime.today().strftime('%Y%m%d')\n",
    "feature_engineering_path = \"/usr/local/spark/staging/\" + today + \"/feature_engineering/\"\n",
    "\n",
    "full_path = os.path.join(feature_engineering_path, '*', '*.csv')\n",
    "stock_df = spark.read.format(\"csv\").load(full_path,header=True)\n",
    "stock_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8d117038-d12d-4c53-8c75-13cfe5273022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Symbol: string (nullable = true)\n",
      " |-- Security Name: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      " |-- Volume: long (nullable = true)\n",
      " |-- vol_moving_avg: double (nullable = true)\n",
      " |-- adj_close_rolling_med: double (nullable = true)\n",
      " |-- __index_level_0__: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stock_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "878345df-d784-4ef6-b31b-3b94f4f43b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df = stock_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e5925ab2-bc14-4f85-80ed-2a8a0fd1a4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            features|Volume|\n",
      "+--------------------+------+\n",
      "|[206633.333333333...|166500|\n",
      "|[206570.0,11.9849...| 25400|\n",
      "|[193106.666666666...| 24500|\n",
      "+--------------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vectorAssembler = VectorAssembler(inputCols = ['vol_moving_avg', 'adj_close_rolling_med'], outputCol = 'features')\n",
    "vstock_df = vectorAssembler.transform(stock_df)\n",
    "vstock_df = vstock_df.select('features', 'Volume')\n",
    "vstock_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6cfa05e5-8dbc-4921-8837-69c4b71eeebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = vstock_df.randomSplit([0.7, 0.3])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e64d782b-dc73-4529-8252-616a3ad783ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3e773417-f0c7-40d5-9e38-2b0944cfee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = RandomForestRegressor(featuresCol = 'features', labelCol='Volume')\n",
    "lr_model = lr.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9ce7f996-7e81-4738-830d-453f0accf1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+--------------------+\n",
      "|        prediction|Volume|            features|\n",
      "+------------------+------+--------------------+\n",
      "|15725.513496688707| 30300|[14480.0,12.90999...|\n",
      "|15128.879592409294| 10900|[14883.3333333333...|\n",
      "|15725.513496688707| 13600|[15090.0,12.92999...|\n",
      "|14795.567904097601| 14300|[15156.6666666666...|\n",
      "|15725.513496688707| 13100|[15193.3333333333...|\n",
      "+------------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_predictions = lr_model.transform(test_df)\n",
    "lr_predictions.select(\"prediction\",\"Volume\",\"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7c340610-eca5-4213-a459-e715546db21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE on test data = 46352\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"Volume\")\n",
    "print(\"MAE on test data = %g\" % lr_evaluator.evaluate(lr_predictions, {lr_evaluator.metricName: \"mae\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eb8584a-2fb2-431f-af11-48e4ccb14690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+----------+-------+--------+--------+-------+-----------------+-------+--------------+---------------------+-----------------+\n",
      "|Symbol|       Security Name|      Date|   Open|    High|     Low|  Close|        Adj Close| Volume|vol_moving_avg|adj_close_rolling_med|__index_level_0__|\n",
      "+------+--------------------+----------+-------+--------+--------+-------+-----------------+-------+--------------+---------------------+-----------------+\n",
      "|   DIA|SPDR Dow Jones In...|1998-01-20|77.8125|78.84375|77.40625|78.8125|48.57304763793945|1744600|          null|                 null|                0|\n",
      "+------+--------------------+----------+-------+--------+--------+-------+-----------------+-------+--------------+---------------------+-----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fullPath = '/usr/local/spark/staging/20230513/feature_engineering/etfs/*.parquet'\n",
    "df = spark.read.format(\"parquet\") \\\n",
    "   .load(fullPath)\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ba2a742-63f6-4c74-89c3-22101a7f94dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1809784"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "e497f66c-092e-473c-975f-cbc73737048c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|Symbol|       Security Name|\n",
      "+------+--------------------+\n",
      "|     A|Agilent Technolog...|\n",
      "|    AA|Alcoa Corporation...|\n",
      "+------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql.functions import element_at, split, col\n",
    "\n",
    "type = \"etfs\"\n",
    "input_path = \"/usr/local/spark/data\"\n",
    "stage_path = \"/usr/local/spark/staging\"\n",
    "today = datetime.today().strftime('%Y%m%d')\n",
    "\n",
    "# Set the input paths\n",
    "output_path = stage_path + \"/\" + today + \"/raw_data_processing/\" + type\n",
    "\n",
    "# Read the CSV files into Pandas dataframes\n",
    "symbols_valid_meta = spark.read.csv(os.path.join(input_path, \"symbols_valid_meta.csv\"), header='true')\n",
    "symbols_valid_meta = symbols_valid_meta.select(['Symbol', 'Security Name'])\n",
    "symbols_valid_meta.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "4cdbff51-f290-432f-8900-b61709724129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+-----------------+------------------+------------------+------------------+------+-------+\n",
      "|      Date|             Open|             High|               Low|             Close|         Adj Close|Volume|Symbol_|\n",
      "+----------+-----------------+-----------------+------------------+------------------+------------------+------+-------+\n",
      "|2018-08-15|11.84000015258789|11.84000015258789|11.739999771118164|11.739999771118164|11.739999771118164| 27300|   AAAU|\n",
      "+----------+-----------------+-----------------+------------------+------------------+------------------+------+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = 'AAAU.csv'\n",
    "stock_df = spark.read.csv(os.path.join(input_path, type, file), header='true')\n",
    "symbol = file.replace('.csv', '')\n",
    "stock_df = stock_df.withColumn(\"Symbol_\", lit(symbol))\n",
    "stock_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "c58106c3-4cdd-4ba6-a122-69ab40214192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+-----+-----+-----------------+------+\n",
      "|      Date|Open|High|  Low|Close|        Adj Close|Volume|\n",
      "+----------+----+----+-----+-----+-----------------+------+\n",
      "|1986-04-03| 0.0|4.75|4.625|4.625|4.449552059173584| 15300|\n",
      "+----------+----+----+-----+-----+-----------------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_path = os.path.join(input_path, type, '*.csv')\n",
    "stock_df = spark.read.format(\"csv\").load(full_path, header='true')\n",
    "# symbol = file.replace('.csv', '')\n",
    "# stock_df = stock_df.withColumn(\"Symbol_\", lit(symbol))\n",
    "stock_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "b44ca716-b2cd-4682-9e93-9a193607c6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+------+-----+-----------------+------+---------------------------------------+\n",
      "|Date      |Open|High|Low   |Close|Adj Close        |Volume|filename                               |\n",
      "+----------+----+----+------+-----+-----------------+------+---------------------------------------+\n",
      "|1986-04-03|0.0 |4.75|4.625 |4.625|4.449552059173584|15300 |file:/usr/local/spark/data/etfs/CEF.csv|\n",
      "|1986-04-04|0.0 |4.75|4.6875|4.75 |4.56981086730957 |12000 |file:/usr/local/spark/data/etfs/CEF.csv|\n",
      "+----------+----+----+------+-----+-----------------+------+---------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stock_df = stock_df.withColumn(\"filename\", input_file_name())\n",
    "stock_df.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "6569eea7-5bd9-45d5-9551-d7211e719baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+------+-----------------+------+-------+\n",
      "|Date      |Open|High  |Low   |Close |Adj Close        |Volume|Symbol_|\n",
      "+----------+----+------+------+------+-----------------+------+-------+\n",
      "|1986-04-03|0.0 |4.75  |4.625 |4.625 |4.449552059173584|15300 |CEF    |\n",
      "|1986-04-04|0.0 |4.75  |4.6875|4.75  |4.56981086730957 |12000 |CEF    |\n",
      "|1986-04-07|0.0 |4.875 |4.75  |4.75  |4.56981086730957 |11500 |CEF    |\n",
      "|1986-04-08|0.0 |4.8125|4.6875|4.75  |4.56981086730957 |21000 |CEF    |\n",
      "|1986-04-09|0.0 |4.8125|4.625 |4.6875|4.50968074798584 |22800 |CEF    |\n",
      "+----------+----+------+------+------+-----------------+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stock_df = stock_df.withColumn('filename', split(stock_df.filename, '/'))\n",
    "stock_df = stock_df.withColumn('filename', element_at(col('filename'), -1) )\n",
    "stock_df = stock_df.withColumn('Symbol_', split(stock_df.filename, '\\.')[0])\n",
    "stock_df = stock_df.drop('filename')\n",
    "stock_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "b43b2edc-be97-4983-a011-cb2baa4925e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------------------------------+----------+----+----+-----+-----+-----------------+------+\n",
      "|Symbol|Security Name                              |Date      |Open|High|Low  |Close|Adj Close        |Volume|\n",
      "+------+-------------------------------------------+----------+----+----+-----+-----+-----------------+------+\n",
      "|CEF   |Sprott Physical Gold and Silver Trust Units|1986-04-03|0.0 |4.75|4.625|4.625|4.449552059173584|15300 |\n",
      "+------+-------------------------------------------+----------+----+----+-----+-----+-----------------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = ['Symbol', 'Security Name', 'Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n",
    "stock_df = stock_df.join(symbols_valid_meta, stock_df.Symbol_ ==  symbols_valid_meta.Symbol,\"left\").drop('Symbol_')\n",
    "stock_df = stock_df.select(columns)\n",
    "stock_df.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "5bf0fa1b-fd55-46c9-8816-56850ff1682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df.write.parquet(output_path + \".parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eadfda9-663a-4a0b-a211-0b72347fa921",
   "metadata": {},
   "source": [
    "<H1>FEATURE ENGINEERING<H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e59affa-654a-4eab-9112-c1b908ebcece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql.functions import element_at, split, col\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"random_reg\").getOrCreate()\n",
    "\n",
    "type = \"etfs\"\n",
    "today = datetime.today().strftime('%Y%m%d')\n",
    "raw_data_processing_path = \"/usr/local/spark/staging/\" + today + \"/raw_data_processing/\"\n",
    "output_path = \"/usr/local/spark/staging/\" + today + \"/feature_engineering/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "770e56ef-0966-4214-a68b-4517eb184e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------------------------------+----------+----+------+------+------+-----------------+------+\n",
      "|Symbol|Security Name                              |Date      |Open|High  |Low   |Close |Adj Close        |Volume|\n",
      "+------+-------------------------------------------+----------+----+------+------+------+-----------------+------+\n",
      "|CEF   |Sprott Physical Gold and Silver Trust Units|1986-04-03|0.0 |4.75  |4.625 |4.625 |4.449552059173584|15300 |\n",
      "|CEF   |Sprott Physical Gold and Silver Trust Units|1986-04-04|0.0 |4.75  |4.6875|4.75  |4.56981086730957 |12000 |\n",
      "|CEF   |Sprott Physical Gold and Silver Trust Units|1986-04-07|0.0 |4.875 |4.75  |4.75  |4.56981086730957 |11500 |\n",
      "|CEF   |Sprott Physical Gold and Silver Trust Units|1986-04-08|0.0 |4.8125|4.6875|4.75  |4.56981086730957 |21000 |\n",
      "|CEF   |Sprott Physical Gold and Silver Trust Units|1986-04-09|0.0 |4.8125|4.625 |4.6875|4.50968074798584 |22800 |\n",
      "|CEF   |Sprott Physical Gold and Silver Trust Units|1986-04-10|0.0 |4.6875|4.625 |4.625 |4.449552059173584|6200  |\n",
      "|CEF   |Sprott Physical Gold and Silver Trust Units|1986-04-11|0.0 |4.6875|4.5625|4.625 |4.449552059173584|37100 |\n",
      "|CEF   |Sprott Physical Gold and Silver Trust Units|1986-04-14|0.0 |4.625 |4.5   |4.5625|4.389422416687012|28200 |\n",
      "|CEF   |Sprott Physical Gold and Silver Trust Units|1986-04-15|0.0 |4.6875|4.5625|4.625 |4.449552059173584|14200 |\n",
      "|CEF   |Sprott Physical Gold and Silver Trust Units|1986-04-16|0.0 |4.5625|4.5   |4.5625|4.389422416687012|14300 |\n",
      "|CEF   |Sprott Physical Gold and Silver Trust Units|1986-04-17|0.0 |4.625 |4.4375|4.5   |4.329293727874756|23400 |\n",
      "|CEF   |Sprott Physical Gold and Silver Trust Units|1986-04-18|0.0 |4.5   |4.4375|4.4375|4.269164562225342|13800 |\n",
      "|CEF   |Sprott Physical Gold and Silver Trust Units|1986-04-21|0.0 |4.375 |4.25  |4.375 |4.209035873413086|28200 |\n",
      "|CEF   |Sprott Physical Gold and Silver Trust Units|1986-04-22|0.0 |4.375 |4.3125|4.375 |4.209035873413086|8800  |\n",
      "|CEF   |Sprott Physical Gold and Silver Trust Units|1986-04-23|0.0 |4.5   |4.375 |4.5   |4.329293727874756|11800 |\n",
      "|CEF   |Sprott Physical Gold and Silver Trust Units|1986-04-24|0.0 |4.5625|4.5   |4.5625|4.389422416687012|4300  |\n",
      "|CEF   |Sprott Physical Gold and Silver Trust Units|1986-04-25|0.0 |4.5625|4.5   |4.5625|4.389422416687012|12400 |\n",
      "|CEF   |Sprott Physical Gold and Silver Trust Units|1986-04-28|0.0 |4.5625|4.5   |4.5   |4.329293727874756|4200  |\n",
      "|CEF   |Sprott Physical Gold and Silver Trust Units|1986-04-29|0.0 |4.5   |4.4375|4.4375|4.269164562225342|5600  |\n",
      "|CEF   |Sprott Physical Gold and Silver Trust Units|1986-04-30|0.0 |4.5   |4.375 |4.5   |4.329293727874756|39400 |\n",
      "+------+-------------------------------------------+----------+----+------+------+------+-----------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_path = os.path.join(raw_data_processing_path, type, '*.csv')\n",
    "stock_df = spark.read.format(\"csv\").load(full_path,header=True)\n",
    "stock_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6be4f609-b1a9-4e78-9272-e592769af257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+----------+------------------+------------------+------------------+------------------+------------------+------+------------------+---------------------+\n",
      "|Symbol|       Security Name|      Date|              Open|              High|               Low|             Close|         Adj Close|Volume|    vol_moving_avg|adj_close_rolling_med|\n",
      "+------+--------------------+----------+------------------+------------------+------------------+------------------+------------------+------+------------------+---------------------+\n",
      "|  ACSI|American Customer...|2016-11-07|24.889999389648438|25.010000228881836|24.889999389648438|24.989999771118164|23.870943069458008| 62300|           62300.0|   23.870943069458008|\n",
      "|  ACSI|American Customer...|2016-11-08|27.010000228881836|27.010000228881836|25.010000228881836|25.200000762939453|  24.0715389251709| 20300|           41300.0|   23.870943069458008|\n",
      "|  ACSI|American Customer...|2016-11-09|  25.1200008392334| 25.43000030517578|  25.1200008392334| 25.43000030517578| 24.29123878479004| 18600|33733.333333333336|     24.0715389251709|\n",
      "|  ACSI|American Customer...|2016-11-10| 25.59000015258789| 25.59000015258789|25.479999542236328|25.549999237060547|24.405864715576172| 19000|           30050.0|     24.0715389251709|\n",
      "|  ACSI|American Customer...|2016-11-11|25.540000915527344|25.700000762939453|25.540000915527344|25.700000762939453|24.549152374267578| 18400|           27720.0|    24.29123878479004|\n",
      "|  ACSI|American Customer...|2016-11-14|25.920000076293945|25.940000534057617|25.836000442504883|25.940000534057617| 24.77840232849121| 20100|           26450.0|    24.29123878479004|\n",
      "|  ACSI|American Customer...|2016-11-15|25.959999084472656|  26.1200008392334|25.959999084472656|  26.1200008392334|24.950342178344727| 20400|25585.714285714286|   24.405864715576172|\n",
      "|  ACSI|American Customer...|2016-11-16|26.059999465942383|26.079999923706055|26.010000228881836|26.010000228881836| 24.84526824951172| 18400|           24687.5|   24.405864715576172|\n",
      "|  ACSI|American Customer...|2016-11-17|26.131000518798828|26.229999542236328|26.110000610351562|26.229999542236328|25.055416107177734| 12400|23322.222222222223|   24.549152374267578|\n",
      "|  ACSI|American Customer...|2016-11-18|26.280000686645508|26.280000686645508|26.219999313354492|26.239999771118164| 25.06496810913086| 12500|           22240.0|   24.549152374267578|\n",
      "|  ACSI|American Customer...|2016-11-21|26.309999465942383|26.360000610351562| 26.28499984741211| 26.34000015258789|25.160490036010742| 14400|21527.272727272728|    24.77840232849121|\n",
      "|  ACSI|American Customer...|2016-11-22|26.559999465942383| 26.59000015258789|26.520000457763672| 26.59000015258789|25.399293899536133| 13100|           20825.0|    24.77840232849121|\n",
      "|  ACSI|American Customer...|2016-11-23| 26.59000015258789|  26.6299991607666| 26.55900001525879| 26.59000015258789|25.399293899536133| 15300|           20400.0|    24.84526824951172|\n",
      "|  ACSI|American Customer...|2016-11-25|             26.75|             26.75|             26.75|             26.75|  25.5521297454834| 10000| 19657.14285714286|    24.84526824951172|\n",
      "|  ACSI|American Customer...|2016-11-28|26.690000534057617|26.690000534057617| 26.59000015258789|  26.6200008392334| 25.42795181274414| 13500|19246.666666666668|   24.950342178344727|\n",
      "|  ACSI|American Customer...|2016-11-29|  26.6299991607666|26.729999542236328|26.618999481201172|26.700000762939453|25.504369735717773| 12800|          18843.75|   24.950342178344727|\n",
      "|  ACSI|American Customer...|2016-11-30|28.030000686645508|28.030000686645508|              26.5|              26.5|25.313325881958008|  9800|18311.764705882353|   25.055416107177734|\n",
      "|  ACSI|American Customer...|2016-12-01|26.559999465942383|26.577999114990234|26.479999542236328|26.507999420166016|25.320966720581055|  8500|17766.666666666668|   25.055416107177734|\n",
      "|  ACSI|American Customer...|2016-12-02|26.649999618530273|26.649999618530273|26.530000686645508|26.559999465942383|25.370637893676758|  8600| 17284.21052631579|    25.06496810913086|\n",
      "|  ACSI|American Customer...|2016-12-05| 26.68000030517578|26.690000534057617|  26.6200008392334| 26.68000030517578|25.485265731811523|  8700|           16855.0|    25.06496810913086|\n",
      "+------+--------------------+----------+------------------+------------------+------------------+------------------+------------------+------+------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, avg, expr\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define the window specification partitioned by the stock symbol and ordered by the date\n",
    "window_spec = Window.partitionBy(\"Symbol\").orderBy(\"Date\")\n",
    "\n",
    "# Calculate the rolling average of the trading volume (Volume)\n",
    "# df_with_rolling_avg = stock_df.withColumn(\"vol_moving_avg\", col(\"Volume\").rolling(30).mean().over(window_spec))\n",
    "df_with_rolling_avg = stock_df.withColumn(\"vol_moving_avg\", avg(col(\"Volume\")).over(window_spec))\n",
    "\n",
    "# Calculate the rolling median of the Adjusted Close (Adj Close)\n",
    "# df_with_rolling_avg_and_median = df_with_rolling_avg.withColumn(\"adj_close_rolling_med\", col(\"Adj Close\").rolling(30).median().over(window_spec))\n",
    "df_with_rolling_avg_and_median = df_with_rolling_avg.withColumn(\"adj_close_rolling_med\", expr(\"percentile_approx(`Adj Close`, 0.5)\").over(window_spec))\n",
    "df_with_rolling_avg_and_median.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "195679ea-5f2e-4108-b3df-83af9e554aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/staging/20230524/feature_engineering/etfs.parquet\n"
     ]
    }
   ],
   "source": [
    "print(output_path+type+\".parquet\")\n",
    "df_with_rolling_avg_and_median.write.csv(output_path+type+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac0f837-865b-40fd-821e-03284f869219",
   "metadata": {},
   "source": [
    "<h1> ML train </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "827d367e-c41f-4ec1-ab1b-73e6de44df48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor, RandomForestRegressionModel\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"random_reg\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c97739f-333f-4085-8e05-0f5a2c5d2f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the second argument passed to spark-submit (the first is the python app)\n",
    "# type = \"etfs\"\n",
    "#type = sys.argv[1]\n",
    "today = datetime.today().strftime('%Y%m%d')\n",
    "feature_engineering_path = \"/usr/local/spark/staging/20230527/feature_engineering/\"\n",
    "\n",
    "full_path = os.path.join(feature_engineering_path, '*', '*.csv')\n",
    "stock_df = spark.read.format(\"csv\").load(full_path,header=True)\n",
    "\n",
    "stock_df = stock_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87d50dbb-3b6a-41c2-8cd6-2028f4e91f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+----------+------------------+------------------+------------------+------------------+------------------+------+--------------+---------------------+\n",
      "|Symbol|       Security Name|      Date|              Open|              High|               Low|             Close|         Adj Close|Volume|vol_moving_avg|adj_close_rolling_med|\n",
      "+------+--------------------+----------+------------------+------------------+------------------+------------------+------------------+------+--------------+---------------------+\n",
      "|  ACSI|American Customer...|2016-11-07|24.889999389648438|25.010000228881836|24.889999389648438|24.989999771118164|23.870943069458008| 62300|       62300.0|   23.870943069458008|\n",
      "|  ACSI|American Customer...|2016-11-08|27.010000228881836|27.010000228881836|25.010000228881836|25.200000762939453|  24.0715389251709| 20300|       41300.0|   23.870943069458008|\n",
      "+------+--------------------+----------+------------------+------------------+------------------+------------------+------------------+------+--------------+---------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stock_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef94e788-a036-4ac3-845e-49efdd688e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = stock_df.selectExpr(\"cast(vol_moving_avg as float) vol_moving_avg\", \n",
    "                    \"cast(adj_close_rolling_med as float) adj_close_rolling_med\", \n",
    "                    \"cast(Volume as float) Volume\")\n",
    "vectorAssembler = VectorAssembler(inputCols = ['vol_moving_avg', 'adj_close_rolling_med'], outputCol = 'features')\n",
    "vstock_df = vectorAssembler.transform(feature_df)\n",
    "vstock_df = vstock_df.select('features', 'Volume')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "782882c0-63db-4b49-8a8f-649df89075e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|            features| Volume|\n",
      "+--------------------+-------+\n",
      "|[62300.0,23.87094...|62300.0|\n",
      "|[41300.0,23.87094...|20300.0|\n",
      "|[33733.33203125,2...|18600.0|\n",
      "|[30050.0,24.07153...|19000.0|\n",
      "|[27720.0,24.29123...|18400.0|\n",
      "|[26450.0,24.29123...|20100.0|\n",
      "|[25585.71484375,2...|20400.0|\n",
      "|[24687.5,24.40586...|18400.0|\n",
      "|[23322.22265625,2...|12400.0|\n",
      "|[22240.0,24.54915...|12500.0|\n",
      "|[21527.2734375,24...|14400.0|\n",
      "|[20825.0,24.77840...|13100.0|\n",
      "|[20400.0,24.84526...|15300.0|\n",
      "|[19657.142578125,...|10000.0|\n",
      "|[19246.666015625,...|13500.0|\n",
      "|[18843.75,24.9503...|12800.0|\n",
      "|[18311.765625,25....| 9800.0|\n",
      "|[17766.666015625,...| 8500.0|\n",
      "|[17284.2109375,25...| 8600.0|\n",
      "|[16855.0,25.06496...| 8700.0|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# stock_df.withColumn(\"vol_moving_avg1\", stock_df(\"vol_moving_avg\").cast(DoubleType))\n",
    "vstock_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "980a7ed5-889b-44b1-8eff-69df4ae2bdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = vstock_df.randomSplit([0.7, 0.3])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30388eaa-6ddf-4297-8f70-8f0375a1cbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE on test data = 703425\n"
     ]
    }
   ],
   "source": [
    "lr = RandomForestRegressor(featuresCol = 'features', labelCol='Volume')\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "lr_predictions = lr_model.transform(test_df)\n",
    "#lr_predictions.select(\"prediction\",\"Volume\",\"features\").show(5)\n",
    "\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"Volume\")\n",
    "print(\"MAE on test data = %g\" % lr_evaluator.evaluate(lr_predictions, {lr_evaluator.metricName: \"mae\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e08931e-eb88-4c7b-a7e9-ffe613e49f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.write().overwrite().save(\"/usr/local/spark/staging/20230527/feature_engineering/regressor.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "270364ca-a074-4098-92b0-211a363a1344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+--------------------+\n",
      "|        prediction|Volume|            features|\n",
      "+------------------+------+--------------------+\n",
      "| 340561.9005693862|   0.0|[0.0,-2038.119262...|\n",
      "| 340561.9005693862|   0.0|[0.0,-2038.119262...|\n",
      "|193986.56398719936|   0.0|[0.0,11.059322357...|\n",
      "|193986.56398719936|   0.0|[0.0,11.545310974...|\n",
      "|193986.56398719936|   0.0|[0.0,14.760999679...|\n",
      "+------------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressionModel.load(\"/usr/local/spark/staging/20230527/regressor.model\")\n",
    "lr_predictions = model.transform(test_df)\n",
    "lr_predictions.select(\"prediction\",\"Volume\",\"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04d0017c-fe44-4dff-871d-cf047eaa7842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------+\n",
      "|features                 |Volume|\n",
      "+-------------------------+------+\n",
      "|[0.0,-2038.1192626953125]|0.0   |\n",
      "+-------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vol_moving_avg = 0.0\n",
    "adj_close_rolling_med = -2038.1192626953125\n",
    "new_feature = spark.createDataFrame([(vol_moving_avg, adj_close_rolling_med, 0.0)],[\"vol_moving_avg\", \"adj_close_rolling_med\", \"Volume\"])\n",
    "vectorAssembler = VectorAssembler(inputCols = ['vol_moving_avg', 'adj_close_rolling_med'], outputCol = 'features')\n",
    "vstock_df = vectorAssembler.transform(new_feature)\n",
    "vstock_df = vstock_df.select('features', 'Volume')\n",
    "vstock_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ecd39bdc-8c44-4e84-b5d6-55b0dba9d9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------+-----------------+\n",
      "|features                 |Volume|prediction       |\n",
      "+-------------------------+------+-----------------+\n",
      "|[0.0,-2038.1192626953125]|0.0   |340561.9005693862|\n",
      "+-------------------------+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(vstock_df).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ac0c9870-32b7-4204-8184-56a16498e58c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340561.9005693862"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = model.transform(vstock_df)\n",
    "predict.select('prediction').collect()[0]['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b4042cfa-0430-4b93-8d5c-ea23bcbf63d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.ml.python.MLSerDe.loads.\n: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype). This happens when an unsupported/unregistered class is being unpickled that requires construction arguments. Fix it by registering a custom IObjectConstructor for this class.\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:759)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:199)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:109)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:122)\n\tat org.apache.spark.mllib.api.python.SerDeBase.loads(PythonMLLibAPI.scala:1326)\n\tat org.apache.spark.ml.python.MLSerDe.loads(MLSerDe.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [53], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvol_moving_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj_close_rolling_med\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:463\u001b[0m, in \u001b[0;36mJavaPredictionModel.predict\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, value: T) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;124;03m    Predict label for the given features.\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_java\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpredict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:71\u001b[0m, in \u001b[0;36mJavaWrapper._call_java\u001b[0;34m(self, name, *args)\u001b[0m\n\u001b[1;32m     68\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m java_args \u001b[38;5;241m=\u001b[39m [_py2java(sc, arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _java2py(sc, m(\u001b[38;5;241m*\u001b[39mjava_args))\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:71\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     68\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m java_args \u001b[38;5;241m=\u001b[39m [\u001b[43m_py2java\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _java2py(sc, m(\u001b[38;5;241m*\u001b[39mjava_args))\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/common.py:90\u001b[0m, in \u001b[0;36m_py2java\u001b[0;34m(sc, obj)\u001b[0m\n\u001b[1;32m     88\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(CPickleSerializer()\u001b[38;5;241m.\u001b[39mdumps(obj))\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMLSerDe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.ml.python.MLSerDe.loads.\n: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype). This happens when an unsupported/unregistered class is being unpickled that requires construction arguments. Fix it by registering a custom IObjectConstructor for this class.\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:759)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:199)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:109)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:122)\n\tat org.apache.spark.mllib.api.python.SerDeBase.loads(PythonMLLibAPI.scala:1326)\n\tat org.apache.spark.ml.python.MLSerDe.loads(MLSerDe.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "model.predict(array([vol_moving_avg, adj_close_rolling_med]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "28d91a2d-aae5-4eff-832c-12bac62dd787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0.       , -2038.1192627])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array([vol_moving_avg, adj_close_rolling_med])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34463b67-069f-4297-9315-5d0393e0ddd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
